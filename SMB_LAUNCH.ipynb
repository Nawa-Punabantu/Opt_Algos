{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuHJyHSut6LcZOsvdP91DT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nawa-Punabantu/Opt_Algos/blob/main/SMB_LAUNCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z_h_5CHkVxMC"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# # %%\n",
        "\n",
        "\n",
        "#%%\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# -*- coding: utf-8 -*-\n",
        "# # %%\n",
        "\n",
        "\n",
        "#%%\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy import integrate\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, Matern\n",
        "from scipy.optimize import differential_evolution\n",
        "from scipy.optimize import minimize, NonlinearConstraint\n",
        "import json\n",
        "from scipy.stats import norm\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy import integrate\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------- Functions\n",
        "\n",
        "# ----- smb\n",
        "\n",
        "# UNITS:\n",
        "# All units must conform to:\n",
        "# Time - s\n",
        "# Lengths - cm^2\n",
        "# Volumes - cm^3\n",
        "# Masses - g\n",
        "# Concentrations - g\n",
        "# Volumetric flowrates - cm^3/s\n",
        "\n",
        "\n",
        "def SMB(SMB_inputs):\n",
        "    iso_type, Names, color, num_comp, nx_per_col, e, D_all, Bm, zone_config, L, d_col, d_in, t_index_min, n_num_cycles, Q_internal, parameter_sets,  cusotom_isotherm_func, cusotom_isotherm_params_all = SMB_inputs[0:]\n",
        "\n",
        "    ###################### (CALCUALTED) SECONDARY INPUTS #########################\n",
        "\n",
        "    # Column Dimensions:\n",
        "    ################################################################\n",
        "    F = (1-e)/e     # Phase ratio\n",
        "    t=0\n",
        "    t_sets = 0\n",
        "    Ncol_num = np.sum(zone_config) # Total number of columns\n",
        "    L_total = L*Ncol_num # Total Lenght of all columns\n",
        "    A_col = np.pi*0.25*d_col**2 # cm^2\n",
        "    V_col = A_col * L # cm^3\n",
        "    V_col_total = Ncol_num * V_col # cm^3\n",
        "    A_in = np.pi * (d_in/2)**2 # cm^2\n",
        "    alpha = A_in / A_col\n",
        "\n",
        "\n",
        "\n",
        "    # Time Specs:\n",
        "    ################################################################\n",
        "\n",
        "    t_index = t_index_min*60 # s #\n",
        "\n",
        "    # Notes:\n",
        "    # - Cyclic Steady state typically happens only after 10 cycles (ref: https://doi.org/10.1205/026387603765444500)\n",
        "    # - The system is not currently designed to account for periods of no external flow\n",
        "\n",
        "    n_1_cycle = t_index * Ncol_num  # s How long a single cycle takes\n",
        "\n",
        "    total_cycle_time = n_1_cycle*n_num_cycles # s\n",
        "\n",
        "    tend = total_cycle_time # s # Final time point in ODE solver\n",
        "\n",
        "    tend_min = tend/60\n",
        "\n",
        "    t_span = (0, tend) # +dt)  # from t=0 to t=n\n",
        "\n",
        "    num_of_injections = int(np.round(tend/t_index)) # number of switching periods\n",
        "\n",
        "    # 't_start_inject_all' is a vecoter containing the times when port swithes occur for each port\n",
        "    # Rows --> Different Ports\n",
        "    # Cols --> Different time points\n",
        "    t_start_inject_all = [[] for _ in range(Ncol_num)]  # One list for each node (including the main list)\n",
        "\n",
        "    # Calculate start times for injections\n",
        "    for k in range(num_of_injections):\n",
        "        t_start_inject = k * t_index\n",
        "        t_start_inject_all[0].append(t_start_inject)  # Main list\n",
        "        for node in range(1, Ncol_num):\n",
        "            t_start_inject_all[node].append(t_start_inject + node * 0)  # all rows in t_start_inject_all are identical\n",
        "\n",
        "    t_schedule = t_start_inject_all[0]\n",
        "\n",
        "    # REQUIRED FUNCTIONS:\n",
        "    ################################################################\n",
        "\n",
        "    # 1.\n",
        "    # Func to Generate Indices for the columns\n",
        "    # def generate_repeated_numbers(n, m):\n",
        "    #     result = []\n",
        "    #     n = int(n)\n",
        "    #     m = int(m)\n",
        "    #     for i in range(m):\n",
        "    #         result.extend([i] * n)\n",
        "    #     return result\n",
        "\n",
        "    # 3.\n",
        "    # Func to divide the column into nodes\n",
        "\n",
        "    # DOES NOT INCLUDE THE C0 NODE (BY DEFAULT)\n",
        "    def set_x(L, Ncol_num,nx_col,dx):\n",
        "        if nx_col == None:\n",
        "            x = np.arange(0, L+dx, dx)\n",
        "            nnx = len(x)\n",
        "            nnx_col = int(np.round(nnx/Ncol_num))\n",
        "            nx_BC = Ncol_num - 1 # Number of Nodes (mixing points/boundary conditions) in between columns\n",
        "\n",
        "            # Indecies belonging to the mixing points between columns are stored in 'start'\n",
        "            # These can be thought of as the locations of the nx_BC nodes.\n",
        "            return x, dx, nnx_col,  nnx, nx_BC\n",
        "\n",
        "        elif dx == None:\n",
        "            nx = Ncol_num * nx_col\n",
        "            nx_BC = Ncol_num - 1 # Number of Nodes in between columns\n",
        "            x = np.linspace(0,L_total,nx)\n",
        "            ddx = x[1] - x[0]\n",
        "\n",
        "            # Indecies belonging to the mixing points between columns are stored in 'start'\n",
        "            # These can be thought of as the locations of the nx_BC nodes.\n",
        "\n",
        "            return x, ddx, nx_col, nx, nx_BC\n",
        "\n",
        "    # 4. A func that:\n",
        "    # (i) Calcualtes the internal flowrates given the external OR (ii) Visa-versa\n",
        "    def set_flowrate_values(set_Q_int, set_Q_ext, Q_rec):\n",
        "        if set_Q_ext is None and Q_rec is None:  # Chosen to specify internal/zone flowrates\n",
        "            Q_I = set_Q_int[0]\n",
        "            Q_II = set_Q_int[1]\n",
        "            Q_III = set_Q_int[2]\n",
        "            Q_IV = set_Q_int[3]\n",
        "\n",
        "            QX = -(Q_I - Q_II)\n",
        "            QF = Q_III - Q_II\n",
        "            QR = -(Q_III - Q_IV)\n",
        "            QD = -(QF + QX + QR) # OR: Q_I - Q_IV\n",
        "\n",
        "            Q_ext = np.array([QF, QR, QD, QX]) # cm^3/s\n",
        "\n",
        "            return Q_ext\n",
        "\n",
        "        elif set_Q_int is None and Q_rec is not None:  # Chosen to specify external flowrates\n",
        "            QF = set_Q_ext[0]\n",
        "            QR = set_Q_ext[1]\n",
        "            QD = set_Q_ext[2]\n",
        "            QX = set_Q_ext[3]\n",
        "\n",
        "            Q_I = Q_rec  # m^3/s\n",
        "            Q_III = (QX + QF) + Q_I\n",
        "            Q_IV = (QD - QX) + Q_I  # Fixed Q_II to Q_I as the variable was not defined yet\n",
        "            Q_II = (QR - QX) + Q_IV\n",
        "            Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV])\n",
        "            return Q_internal\n",
        "\n",
        "\n",
        "    # 5. Function to Build Port Schedules:\n",
        "\n",
        "    # This is done in two functions: (i) repeat_array and (ii) build_matrix_from_vector\n",
        "    # (i) repeat_array\n",
        "    # Summary: Creates the schedule for the 1st port, port 0, only. This is the port boadering Z2 & Z3 and always starts as a Feed port at t=0\n",
        "    # (i) build_matrix_from_vector\n",
        "    # Summary: Takes the output from \"repeat_array\" and creates schedules for all other ports.\n",
        "    # The \"trick\" is that the states of each of the, n, ports at t=0, is equal to the first, n, states of port 0.\n",
        "    # Once we know the states for each port at t=0, we form a loop that adds the next state.\n",
        "\n",
        "    # 5.1\n",
        "    def position_maker(schedule_quantity_name, F, R, D, X, Z_config):\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        Function that initializes the starting schedueles for a given quantitiy at all positions\n",
        "\n",
        "        F, R, D and X are the values of the quantiity at the respective feed ports\n",
        "\n",
        "        \"\"\"\n",
        "        # Initialize:\n",
        "        X_j = np.zeros(Ncol_num)\n",
        "\n",
        "\n",
        "        # We set each port in the appropriate position, depending on the nuber of col b/n Zones:\n",
        "        # By default, Position i = 0 (entrance to col,0) is reserved for the feed node.\n",
        "\n",
        "        # Initialize Positions:\n",
        "        # Q_position is a vector whose len is = number of mixing points (ports) b/n columns\n",
        "\n",
        "        X_j[0] = F        # FEED\n",
        "        X_j[Z_config[2]] = R     # RAFFINATE\n",
        "        X_j[Z_config[2] + Z_config[3]] = D    # DESORBENT\n",
        "        X_j[Z_config[2] + Z_config[3]+  Z_config[0]] = X   # EXTRACT\n",
        "\n",
        "        return X_j\n",
        "\n",
        "    # 5.2\n",
        "    def repeat_array(vector, start_time_num):\n",
        "        # vector = the states of all ports at t=0, vector[0] = is always the Feed port\n",
        "        # start_time_num = The number of times the state changes == num of port switches == num_injections\n",
        "        repeated_array = np.tile(vector, (start_time_num // len(vector) + 1))\n",
        "        return repeated_array[:start_time_num]\n",
        "\n",
        "    def initial_u_col(Zconfig, Qint):\n",
        "        \"\"\"\n",
        "        Fun that returns the the inital state at t=0 of the volumetric\n",
        "        flows in all the columns.\n",
        "\n",
        "        \"\"\"\n",
        "        # First row is for col0, which is the feed to zone 3\n",
        "        Zconfig_roll = np.roll(Zconfig, -2)\n",
        "        Qint_roll = np.roll(Qint, -2)\n",
        "\n",
        "        # print(Qint)\n",
        "        X = np.array([])\n",
        "\n",
        "        for i in range(len(Qint_roll)):\n",
        "            X_add = np.ones(Zconfig_roll[i])*Qint_roll[i]\n",
        "            # print('X_add:\\n', X_add)\n",
        "\n",
        "            X = np.append(X, X_add)\n",
        "        # X = np.concatenate(X)\n",
        "        # print('X:\\n', X)\n",
        "        return X\n",
        "\n",
        "\n",
        "    def build_matrix_from_vector(vector, t_schedule):\n",
        "        \"\"\"\n",
        "        Fun that returns the schedeule given the inital state at t=0\n",
        "        vector: inital state of given quantity at t=0 at all nodes\n",
        "        t_schedule: times at which port changes happen\n",
        "\n",
        "        \"\"\"\n",
        "        # vector = the states of all ports at t=0, vector[0] = is always the Feed port\n",
        "        start_time_num = int(len(t_schedule))\n",
        "        vector = np.array(vector)  # Convert the vector to a NumPy array\n",
        "        n = len(vector) # number of ports/columns\n",
        "\n",
        "        # Initialize the matrix for repeated elements, ''ALL''\n",
        "        ALL = np.zeros((n, start_time_num), dtype=vector.dtype)  # Shape is (n, start_time_num)\n",
        "\n",
        "        for i in range(start_time_num):\n",
        "            # print('i:',i)\n",
        "            ALL[:, i] = np.roll(vector, i)\n",
        "        return ALL\n",
        "\n",
        "\n",
        "\n",
        "    # # Uncomment as necessary depending on specification of either:\n",
        "    # # (1) Internal OR (2) External flowrates :\n",
        "    # # (1)\n",
        "    # Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV])\n",
        "    Q_external = set_flowrate_values(Q_internal, None, None) # Order: QF, QR, QD, QX\n",
        "    QF, QR, QD, QX = Q_external[0], Q_external[1], Q_external[2], Q_external[3]\n",
        "    # print('Q_external:', Q_external)\n",
        "\n",
        "    # (2)\n",
        "    # QX, QF, QR = -0.277, 0.315, -0.231  # cm^3/s\n",
        "    # QD = - (QF + QX + QR)\n",
        "    # Q_external = np.array([QF, QR, QD, QX])\n",
        "    # Q_rec = 33.69 # cm^3/s\n",
        "    # Q_internal = set_flowrate_values(None, Q_external, Q_rec) # Order: QF, QR, QD, Q\n",
        "\n",
        "    ################################################################################################\n",
        "\n",
        "\n",
        "    # Make concentration schedules for each component\n",
        "\n",
        "    Cj_pulse_all = [[] for _ in range(num_comp)]\n",
        "    for i in range(num_comp):\n",
        "        Cj_position = []\n",
        "        Cj_position = position_maker('Feed Conc Schedule:', parameter_sets[i]['C_feed'], 0, 0, 0, zone_config)\n",
        "        Cj_pulse = build_matrix_from_vector(Cj_position,  t_schedule)\n",
        "        Cj_pulse_all[i] = Cj_pulse\n",
        "\n",
        "\n",
        "    Q_position = position_maker('Vol Flow Schedule:', Q_external[0], Q_external[1], Q_external[2], Q_external[3], zone_config)\n",
        "    Q_pulse_all = build_matrix_from_vector(Q_position,  t_schedule)\n",
        "\n",
        "    # Spacial Discretization:\n",
        "    # Info:\n",
        "    # nx --> Total Number of Nodes (EXCLUDING mixing points b/n nodes)\n",
        "    # nx_col --> Number of nodes in 1 column\n",
        "    # nx_BC --> Number of mixing points b/n nodes\n",
        "    x, dx, nx_col, nx, nx_BC = set_x(L=L_total, Ncol_num = Ncol_num, nx_col = nx_per_col, dx = None)\n",
        "    start = [i*nx_col for i in range(0,Ncol_num)] # Locations of the BC indecies\n",
        "    u_col_at_t0 = initial_u_col(zone_config, Q_internal)\n",
        "    Q_col_all = build_matrix_from_vector(u_col_at_t0, t_schedule)\n",
        "\n",
        "\n",
        "    # DISPLAYING INPUT INFORMATION:\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('Number of Components:', num_comp)\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nTime Specs:\\n')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('Number of Cycles:', n_num_cycles)\n",
        "    # print('Time Per Cycle:', n_1_cycle/60, \"min\")\n",
        "    # print('Simulation Time:', tend_min, 'min')\n",
        "    # print('Index Time:', t_index, 's OR', t_index/60, 'min' )\n",
        "    # print('Number of Port Switches:', num_of_injections)\n",
        "    # print('Injections happen at t(s) = :', t_schedule, 'seconds')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nColumn Specs:\\n')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('Configuration:', zone_config, '[Z1,Z2,Z3,Z4]')\n",
        "    # print(f\"Number of Columns: {Ncol_num}\")\n",
        "    # print('Column Length:', L, 'cm')\n",
        "    # print('Column Diameter:', d_col, 'cm')\n",
        "    # print('Column Volume:', V_col, 'cm^3')\n",
        "    # print(\"alpha:\", alpha, '(alpha = A_in / A_col)')\n",
        "    # print(\"Nodes per Column:\",nx_col)\n",
        "    # print(\"Boundary Nodes locations,x[i], i =\", start)\n",
        "    # print(\"Total Number of Nodes (nx):\",nx)\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nFlowrate Specs:\\n')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print(\"External Flowrates =\", Q_external, '[F,R,D,X] ml/min')\n",
        "    # print(\"Ineternal Flowrates =\", Q_internal, 'ml/min')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nPort Schedules:')\n",
        "    # for i in range(num_comp):\n",
        "    #     print(f\"Concentration Schedule:\\nShape:\\n {Names[i]}:\\n\",np.shape(Cj_pulse_all[i]),'\\n', Cj_pulse_all[i], \"\\n\")\n",
        "    # print(\"Injection Flowrate Schedule:\\nShape:\",np.shape(Q_pulse_all),'\\n', Q_pulse_all, \"\\n\")\n",
        "    # print(\"Respective Column Flowrate Schedule:\\nShape:\",np.shape(Q_col_all),'\\n', Q_col_all, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    # ###########################################################################################\n",
        "\n",
        "    # Isotherm Models:\n",
        "\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "    # 1. LINEAR\n",
        "    def iso_lin(theta_lin, c):\n",
        "        # params: [HA, HB]\n",
        "        H = theta_lin\n",
        "        q_star = H*c\n",
        "\n",
        "        return q_star # [qA, qB, ...]\n",
        "\n",
        "    # 2.  LANGMUIR\n",
        "\n",
        "    # 2.1 Independent Langmuir\n",
        "    def iso_langmuir(theta_lang, c, comp_idx): # already for specific comp\n",
        "        H = theta_lang\n",
        "        q_star = H*c/(1 + H*c)\n",
        "        #q_star = H[comp_idx]*c/(1 + K[0]*c + K[1]*c)\n",
        "        # q_star = theta_lang[0]*c/(1 + theta_lang[1]*c + theta_lang[2]*c) +\\\n",
        "        #     theta_lang[3]*c/(1 + theta_lang[4]*c + theta_lang[5]*c)\n",
        "        return q_star\n",
        "\n",
        "    # 2.3 Coupled Langmuir\n",
        "    def iso_cup_langmuir(theta_cuplang, c, IDX, comp_idx): # already for specific comp\n",
        "        H = theta_cuplang[:2] # [HA, HB]\n",
        "        K = theta_cuplang[2:] # [KA, KB]\n",
        "        cA = c[IDX[0] + 0: IDX[0] + nx ]\n",
        "        cB = c[IDX[1] + 0: IDX[1] + nx ]\n",
        "        c_i = [cA, cB]\n",
        "        q_star = H[comp_idx]*c_i[comp_idx]/(1 + K[0]*cA + K[1]*cB)\n",
        "        return q_star\n",
        "\n",
        "    # 2.3 Bi-Langmuir\n",
        "    def iso_bi_langmuir(theta_bl, c, IDX, comp_idx): # already for specific comp\n",
        "        cA = c[IDX[0] + 0: IDX[0] + nx ]\n",
        "        cB = c[IDX[1] + 0: IDX[1] + nx ]\n",
        "        c_i = [cA, cB]\n",
        "\n",
        "        q_star = theta_bl[0]*c_i[comp_idx]/(1 + theta_bl[1]*cA + theta_bl[2]*cB) +\\\n",
        "                theta_bl[3]*c_i[comp_idx]/(1 + theta_bl[4]*cA + theta_bl[5]*cB)\n",
        "\n",
        "        return q_star\n",
        "\n",
        "\n",
        "    # 3. FREUDLICH:\n",
        "    def iso_freundlich(theta_fre, c): # already for specific comp\n",
        "        q_star = theta_fre[0]*c**(1/theta_fre[1])\n",
        "        return q_star\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "    # Mass Transfer (MT) Models:\n",
        "\n",
        "    def mass_transfer(kav_params, q_star, q): # already for specific comp\n",
        "        # kav_params: [kA, kB]\n",
        "        kav =  kav_params\n",
        "        MT = kav * Bm/(5 + Bm) * (q_star - q)\n",
        "        # MT = kav * (q_star - q)\n",
        "        return MT\n",
        "\n",
        "    # MT PARAMETERS\n",
        "    ###########################################################################################\n",
        "    # print('np.shape(parameter_sets[:][\"kh\"]):', np.shape(parameter_sets[3]))\n",
        "    kav_params = [parameter_sets[i][\"kh\"] for i in range(num_comp)]  # [kA, kB, kC, kD, kE, kF]\n",
        "    # print('kav_params:', kav_params)\n",
        "    # print('----------------------------------------------------------------')\n",
        "    ###########################################################################################\n",
        "\n",
        "    # # FORMING THE ODES\n",
        "\n",
        "\n",
        "    # Form the remaining schedule matrices that are to be searched by the funcs\n",
        "\n",
        "    # Column velocity schedule:\n",
        "    u_col_all = -Q_col_all/A_col/e\n",
        "\n",
        "    # Column Dispersion schedule:\n",
        "    # Different matrices for each comp, because diff Pe's for each comp\n",
        "    D_col_all = []\n",
        "    for i in range(num_comp): # for each comp\n",
        "        # D_col = -(u_col_all*L)/Pe_all[i] # constant dispersion coeff\n",
        "        D_col = np.ones_like(u_col_all)*D_all[i]\n",
        "        D_col_all.append(D_col)\n",
        "\n",
        "\n",
        "    # print(f'Shape of u_col_all: {np.shape(u_col_all)}')\n",
        "    # print(f'Shape of D_col_all: {np.shape(D_col_all)}')\n",
        "    # print(f'u_col_all: {u_col_all}')\n",
        "    # print(f'\\nD_col_all: {D_col_all}')\n",
        "    # Storage Spaces:\n",
        "    coef_0 = np.zeros_like(u_col_all)\n",
        "    coef_1 = np.zeros_like(u_col_all)\n",
        "    coef_2 = np.zeros_like(u_col_all)\n",
        "\n",
        "    # coef_0, coef_1, & coef_2 correspond to the coefficents of ci-1, ci & ci+1 respectively\n",
        "    # These depend on u and so change with time, thus have a schedule\n",
        "\n",
        "    # From descritization:\n",
        "    coef_0_all = []\n",
        "    coef_1_all = []\n",
        "    coef_2_all = []\n",
        "\n",
        "    for j in range(num_comp): # for each comp\n",
        "        for i  in range(Ncol_num): # coefficients for each col\n",
        "            coef_0[i,:] = ( D_col_all[j][i,:]/dx**2 ) - ( u_col_all[i,:]/dx ) # coefficeint of i-1\n",
        "            coef_1[i,:] = ( u_col_all[i,:]/dx ) - (2*D_col_all[j][i,:]/(dx**2))# coefficeint of i\n",
        "            coef_2[i,:] = (D_col_all[j][i,:]/(dx**2))    # coefficeint of i+1\n",
        "        coef_0_all.append(coef_0)\n",
        "        coef_1_all.append(coef_1)\n",
        "        coef_2_all.append(coef_2)\n",
        "\n",
        "    # All shedules:\n",
        "    # For each shceudle, rows => col idx, columns => Time idx\n",
        "    # :\n",
        "    # - Q_pulse_all: Injection flowrates\n",
        "    # - C_pulse_all: Injection concentrations for each component\n",
        "    # - Q_col_all:  Flowrates WITHIN each col\n",
        "    # - u_col_all: Linear velocities WITHIN each col\n",
        "    # - D_col_all: Dispersion coefficeints WITHIN each col\n",
        "    # - coef_0, 1 and 2: ci, ci-1 & ci+1 ceofficients\n",
        "\n",
        "    # print('coef_0:\\n',coef_0)\n",
        "    # print('coef_1:\\n',coef_1)\n",
        "    # print('coef_2:\\n',coef_2)\n",
        "    # print('\\nD_col_all:\\n',D_col_all)\n",
        "    # print('Q_col_all:\\n',Q_col_all)\n",
        "    # print('A_col:\\n',A_col)\n",
        "    # print('u_col_all:\\n',u_col_all)\n",
        "\n",
        "\n",
        "    def coeff_matrix_builder_UNC(t, Q_col_all, Q_pulse_all, dx, start, alpha, c, nx_col, comp_idx): # note that c_length must include nx_BC\n",
        "\n",
        "        # Define the functions that call the appropriate schedule matrices:\n",
        "        # Because all scheudels are of the same from, only one function is required\n",
        "        # Calling volumetric flows:\n",
        "        get_X = lambda t, X_schedule, col_idx: next((X_schedule[col_idx][j] for j in range(len(X_schedule[col_idx])) if t_start_inject_all[col_idx][j] <= t < t_start_inject_all[col_idx][j] + t_index), 1/100000000)\n",
        "        get_C = lambda t, C_schedule, col_idx, comp_idx: next((C_schedule[comp_idx][col_idx][j] for j in range(len(C_schedule[comp_idx][col_idx])) if t_start_inject_all[col_idx][j] <= t < t_start_inject_all[col_idx][j] + t_index), 1/100000000)\n",
        "\n",
        "        def small_col_matix(nx_col, col_idx):\n",
        "        # Initialize small_col_coeff ('small' = for 1 col)\n",
        "\n",
        "            small_col_coeff = np.zeros((int(nx_col),int(nx_col))) #(5,5)\n",
        "\n",
        "            # Where the 1st (0th) row and col are for c1\n",
        "            # get_C(t, coef_0_all, k, comp_idx)\n",
        "            # small_col_coeff[0,0], small_col_coeff[0,1] = get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "            small_col_coeff[0,0], small_col_coeff[0,1] = get_C(t,coef_1_all,col_idx, comp_idx), get_C(t,coef_2_all,col_idx, comp_idx)\n",
        "            # for c2:\n",
        "            # small_col_coeff[1,0], small_col_coeff[1,1], small_col_coeff[1,2] = get_X(t,coef_0,col_idx), get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "            small_col_coeff[1,0], small_col_coeff[1,1], small_col_coeff[1,2] = get_C(t,coef_0_all,col_idx, comp_idx), get_C(t, coef_1_all, col_idx, comp_idx), get_C(t, coef_2_all,col_idx, comp_idx)\n",
        "\n",
        "            for i in range(2,nx_col): # from row i=2 onwards\n",
        "                # np.roll the row entries from the previous row, for all the next rows\n",
        "                new_row = np.roll(small_col_coeff[i-1,:],1)\n",
        "                small_col_coeff[i:] = new_row\n",
        "\n",
        "            small_col_coeff[-1,0] = 0\n",
        "            small_col_coeff[-1,-1] = small_col_coeff[-1,-1] +  get_C(t,coef_2_all,col_idx, comp_idx) # coef_1 + coef_2 account for rolling boundary\n",
        "\n",
        "\n",
        "            return small_col_coeff\n",
        "\n",
        "\n",
        "        larger_coeff_matrix = np.zeros((nx,nx)) # ''large'' = for all cols # (20, 20)\n",
        "\n",
        "        # Add the cols\n",
        "        for col_idx in range(Ncol_num):\n",
        "            larger_coeff_matrix[col_idx*nx_col:(col_idx+1)*nx_col, col_idx*nx_col:(col_idx+1)*nx_col] = small_col_matix(nx_col,col_idx)\n",
        "        # print('np.shape(larger_coeff_matrix)\\n',np.shape(larger_coeff_matrix))\n",
        "\n",
        "        # vector_add: vector that applies the boundary conditions to each boundary node\n",
        "        def vector_add(nx, c, start, comp_idx):\n",
        "            vec_add = np.zeros(nx)\n",
        "            c_BC = np.zeros(Ncol_num)\n",
        "            # Indeceis for the boundary nodes are stored in \"start\"\n",
        "            # Each boundary node is affected by the form:\n",
        "            # c_BC = V1 * C_IN - V2 * c[i] + V3 * c[i+1]\n",
        "\n",
        "            # R1 = ((beta * alpha) / gamma)\n",
        "            # R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "            # R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "\n",
        "            # Where:\n",
        "            # C_IN is the weighted conc exiting the port facing the column entrance.\n",
        "            # alpha , bata and gamma depend on the column vecolity and are thus time dependant\n",
        "            # Instead of forming schedules for alpha , bata and gamma, we calculate them in-line\n",
        "\n",
        "            for i in range(len(start)):\n",
        "                #  start[i] => the node at the entrance to the ith col\n",
        "                # So start[3] is the node representing the 1st node in col 3\n",
        "\n",
        "                Q_1 = get_X(t, Q_col_all, i-1) # Vol_flow from previous column (which for column 0, is the last column in the chain)\n",
        "                Q_2 = get_X(t, Q_pulse_all, i) # Vol_flow injected IN port i\n",
        "\n",
        "                Q_out_port = get_X(t, Q_col_all, i) # Vol_flow OUT of port 0 (Also could have used Q_1 + Q_2)\n",
        "\n",
        "\n",
        "                W1 = Q_1/Q_out_port # Weighted flowrate to column i\n",
        "                W2 = Q_2/Q_out_port # Weighted flowrate to column i\n",
        "\n",
        "                # Calcualte Weighted Concentration:\n",
        "\n",
        "                c_injection = get_C(t, Cj_pulse_all, i, comp_idx)\n",
        "\n",
        "                if Q_2 > 0: # Concentration in the next column is only affected for injection flows IN\n",
        "                    C_IN = W1 * c[i*nx_col-1] + W2 * c_injection\n",
        "                else:\n",
        "                    # C_IN = c[i*nx_col-1] # no change in conc during product collection\n",
        "                    C_IN = c[start[i]-1] # no change in conc during product collection\n",
        "\n",
        "                # Calcualte alpha, bata and gamma:\n",
        "                # Da = get_X(t, D_col_all, i)\n",
        "                Da = get_C(t, D_col_all, i, comp_idx)\n",
        "                u =  get_X(t, u_col_all, i)\n",
        "                beta = 1 / alpha\n",
        "                gamma = 1 - 3 * Da / (2 * u * dx)\n",
        "\n",
        "                ##\n",
        "                R1 = ((beta * alpha) / gamma)\n",
        "                R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "                R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "                ##\n",
        "\n",
        "                # Calcualte the BC effects:\n",
        "                j = start[i]\n",
        "                c_BC[i] = R1 * C_IN - R2 * c[j] + R3 * c[j+1] # the boundary concentration for that node\n",
        "\n",
        "            # print('c_BC:\\n', c_BC)\n",
        "\n",
        "            for k in range(len(c_BC)):\n",
        "                # vec_add[start[k]]  = get_X(t,coef_0,k)*c_BC[k]\n",
        "                vec_add[start[k]]  = get_C(t, coef_0_all, k, comp_idx)*c_BC[k]\n",
        "\n",
        "            return vec_add\n",
        "            # print('np.shape(vect_add)\\n',np.shape(vec_add(nx, c, start)))\n",
        "        return larger_coeff_matrix, vector_add(nx, c, start, comp_idx)\n",
        "\n",
        "    def coeff_matrix_builder_CUP(t, Q_col_all, Q_pulse_all, dx, start_CUP, alpha, c, nx_col,IDX): # note that c_length must include nx_BC\n",
        "\n",
        "        # Define the functions that call the appropriate schedule matrices:\n",
        "        # Because all scheudels are of the same from, only one function is required\n",
        "        # Calling volumetric flows:\n",
        "        get_X = lambda t, X_schedule, col_idx: next((X_schedule[col_idx][j] for j in range(len(X_schedule[col_idx])) if t_start_inject_all[col_idx][j] <= t < t_start_inject_all[col_idx][j] + t_index), 1/100000000)\n",
        "\n",
        "\n",
        "        # 1. From coefficent \"small\" matrix for movement of single comp through single col\n",
        "        # 2. Form  \"large\" coefficent matrix for movement through one all cols\n",
        "        # 3. The large  coefficent matrix for each comp will then be combined into Final Matrix\n",
        "\n",
        "        # 1.\n",
        "        def small_col_matrix(nx_col, col_idx):\n",
        "\n",
        "        # Initialize small_col_coeff ('small' = for 1 col)\n",
        "\n",
        "            small_col_coeff = np.zeros((int(nx_col),int(nx_col))) #(5,5)\n",
        "\n",
        "            # Where the 1st (0th) row and col are for c1\n",
        "            #\n",
        "            small_col_coeff[0,0], small_col_coeff[0,1] = get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "            # for c2:\n",
        "            small_col_coeff[1,0], small_col_coeff[1,1], small_col_coeff[1,2] = get_X(t,coef_0,col_idx), get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "\n",
        "            for i in range(2,nx_col): # from row i=2 onwards\n",
        "                # np.roll the row entries from the previous row, for all the next rows\n",
        "                new_row = np.roll(small_col_coeff[i-1,:],1)\n",
        "                small_col_coeff[i:] = new_row\n",
        "\n",
        "            small_col_coeff[-1,0] = 0\n",
        "            small_col_coeff[-1,-1] = small_col_coeff[-1,-1] +  get_X(t,coef_2,col_idx) # coef_1 + coef_2 account for rolling boundary\n",
        "\n",
        "            return small_col_coeff\n",
        "\n",
        "        # 2. Func to Build Large Matrix\n",
        "\n",
        "        def matrix_builder(M, M0):\n",
        "            # M = Matrix to add (small)\n",
        "            # M0 = Initial state of the larger matrix to be added to\n",
        "            nx_col = M.shape[0]\n",
        "            repeat = int(np.round(M0.shape[0]/M.shape[0]))# numbner of times the small is added to the larger matrix\n",
        "            for col_idx in range(repeat):\n",
        "                        M0[col_idx*nx_col:(col_idx+1)*nx_col, col_idx*nx_col:(col_idx+1)*nx_col] = M\n",
        "            return M0\n",
        "\n",
        "\n",
        "        # 3. Generate and Store the Large Matrices\n",
        "        # Storage Space:\n",
        "        # NOTE: Assuming all components have the same Dispersion coefficients,\n",
        "        # all components will have the same large_col_matrix\n",
        "        # Add the cols\n",
        "        larger_coeff_matrix = np.zeros((nx,nx)) # ''large'' = for all cols # (20, 20)\n",
        "\n",
        "        for col_idx in range(Ncol_num):\n",
        "            larger_coeff_matrix[col_idx*nx_col:(col_idx+1)*nx_col, col_idx*nx_col:(col_idx+1)*nx_col] = small_col_matrix(nx_col,col_idx)\n",
        "\n",
        "        # print('np.shape(larger_coeff_matrix)\\n',np.shape(larger_coeff_matrix))\n",
        "\n",
        "        # Inital final matrix:\n",
        "        n = nx*num_comp\n",
        "        final_matrix0 = np.zeros((n,n))\n",
        "\n",
        "\n",
        "        final_matrix = matrix_builder(larger_coeff_matrix, final_matrix0)\n",
        "\n",
        "            # vector_add: vector that applies the boundary conditions to each boundary node\n",
        "        def vector_add(nx, c, start):\n",
        "            vec_add = np.zeros(nx*num_comp)\n",
        "            c_BC = np.zeros(nx*num_comp)\n",
        "            # Indeceis for the boundary nodes are stored in \"start\"\n",
        "            # Each boundary node is affected by the form:\n",
        "            # c_BC = V1 * C_IN - V2 * c[i] + V3 * c[i+1]\n",
        "\n",
        "            # R1 = ((beta * alpha) / gamma)\n",
        "            # R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "            # R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "\n",
        "            # Where:\n",
        "            # C_IN is the weighted conc exiting the port facing the column entrance.\n",
        "            # alpha , bata and gamma depend on the column vecolity and are thus time dependant\n",
        "            # Instead of forming schedules for alpha , bata and gamma, we calculate them in-line\n",
        "\n",
        "            for i in range(len(start)):\n",
        "                #k = i%len(start) # Recounts columns for B\n",
        "                Q_1 = get_X(t, Q_col_all, i-1) # Vol_flow from previous column (which for column 0, is the last column in the chain)\n",
        "                Q_2 = get_X(t, Q_pulse_all, i) # Vol_flow injected IN port i\n",
        "\n",
        "                Q_out_port = get_X(t, Q_col_all, i) # Vol_flow OUT of port 0 (Also could have used Q_1 + Q_2)\n",
        "\n",
        "\n",
        "                W1 = Q_1/Q_out_port # Weighted flowrate to column i\n",
        "                W2 = Q_2/Q_out_port # Weighted flowrate to column i\n",
        "\n",
        "                # Calcualte Weighted Concentration:\n",
        "                # Identifiers:\n",
        "                A = IDX[0]\n",
        "                B = IDX[1]\n",
        "\n",
        "                # C_IN_A = W1 * c[A + i*nx_col-1] + W2 * get_X(t, C_pulse_all_A, i) # c[-1] conc out the last col\n",
        "                # C_IN_B = W1 * c[B + i*nx_col-1] + W2 * get_X(t, C_pulse_all_B, i) # c[-1] conc out the last col\n",
        "\n",
        "                C_IN_A = W1 * c[A + i*nx_col-1] + W2 * get_X(t, Cj_pulse_all[0], i) # c[-1] conc out the last col\n",
        "                C_IN_B = W1 * c[B + i*nx_col-1] + W2 * get_X(t, Cj_pulse_all[1], i) # c[-1] conc out the last col\n",
        "\n",
        "\n",
        "                # Calcualte alpha, bata and gamma:\n",
        "                Da = get_X(t, D_col_all, i)\n",
        "                u =  get_X(t, u_col_all, i)\n",
        "                beta = 1 / alpha\n",
        "                gamma = 1 - 3 * Da / (2 * u * dx)\n",
        "\n",
        "                ##\n",
        "                R1 = ((beta * alpha) / gamma)\n",
        "                R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "                R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "                ##\n",
        "\n",
        "                # Calcualte the BC effects:\n",
        "                j = start[i]\n",
        "                # print('j:', j)\n",
        "                c_BC[i] = R1 * C_IN_A - R2 * c[j] + R3 * c[j+1] # the boundary concentration for that node\n",
        "                c_BC[B + i] = R1 * C_IN_B - R2 * c[B+j] + R3 * c[B+j+1]\n",
        "            # print('c_BC:\\n', c_BC)\n",
        "            # print('c_BC.shape:\\n', c_BC.shape)\n",
        "\n",
        "            for k in range(len(start)):\n",
        "                vec_add[start[k]]  = get_X(t,coef_0,k)*c_BC[k]\n",
        "                vec_add[B + start[k]]  = get_X(t,coef_0,k)*c_BC[B+ k]\n",
        "\n",
        "            return vec_add\n",
        "            # print('np.shape(vect_add)\\n',np.shape(vec_add(nx, c, start)))\n",
        "        return final_matrix, vector_add(nx, c, start_CUP)\n",
        "\n",
        "    # ###########################################################################################\n",
        "\n",
        "    # # mod1: UNCOUPLED ISOTHERM:\n",
        "    # # Profiles for each component can be solved independently\n",
        "\n",
        "    # ###########################################################################################\n",
        "    def mod1(t, v, comp_idx, Q_pulse_all):\n",
        "        # call.append(\"call\")\n",
        "        # print(len(call))\n",
        "        c = v[:nx]\n",
        "        q = v[nx:]\n",
        "\n",
        "        # Initialize the derivatives\n",
        "        dc_dt = np.zeros_like(c)\n",
        "        dq_dt = np.zeros_like(q)\n",
        "        # print('v size\\n',np.shape(v))\n",
        "\n",
        "        # Isotherm:\n",
        "        #########################################################################\n",
        "        isotherm = cusotom_isotherm_func(cusotom_isotherm_params_all[comp_idx,:],c)\n",
        "        # isotherm = iso_lin(theta_lin[comp_idx], c)\n",
        "        #isotherm = iso_langmuir(theta_lang[comp_idx], c, comp_idx)\n",
        "        #isotherm = iso_freundlich(theta_fre, c)\n",
        "\n",
        "\n",
        "        # Mass Transfer:\n",
        "        #########################################################################\n",
        "        # print('isotherm size\\n',np.shape(isotherm))\n",
        "        MT = mass_transfer(kav_params[comp_idx], isotherm, q)\n",
        "        #print('MT:\\n', MT)\n",
        "\n",
        "        coeff_matrix, vec_add = coeff_matrix_builder_UNC(t, Q_col_all, Q_pulse_all, dx, start, alpha, c, nx_col, comp_idx)\n",
        "        # print('coeff_matrix:\\n',coeff_matrix)\n",
        "        # print('vec_add:\\n',vec_add)\n",
        "        dc_dt = coeff_matrix @ c + vec_add - F * MT\n",
        "        dq_dt = MT\n",
        "\n",
        "        return np.concatenate([dc_dt, dq_dt])\n",
        "\n",
        "    # ##################################################################################\n",
        "\n",
        "    def mod2(t, v):\n",
        "\n",
        "        # where, v = [c, q]\n",
        "        c = v[:num_comp*nx] # c = [cA, cB] | cA = c[:nx], cB = c[nx:]\n",
        "        q = v[num_comp*nx:] # q = [qA, qB]| qA = q[:nx], qB = q[nx:]\n",
        "\n",
        "        # Craate Lables so that we know the component assignement in the c vecotor:\n",
        "        A, B = 0*nx, 1*nx # Assume Binary 2*nx, 3*nx, 4*nx, 5*nx\n",
        "        IDX = [A, B]\n",
        "\n",
        "        # Thus to refer to the liquid concentration of the i = nth row of component B: c[C + n]\n",
        "        # Or the the solid concentration 10th row of component B: q[B + 10]\n",
        "        # Or to refer to all A's OR B's liquid concentrations: c[A + 0: A + nx] OR c[B + 0: B + nx]\n",
        "\n",
        "\n",
        "        # Initialize the derivatives\n",
        "        dc_dt = np.zeros_like(c)\n",
        "        dq_dt = np.zeros_like(q)\n",
        "\n",
        "\n",
        "        coeff_matrix, vec_add = coeff_matrix_builder_CUP(t, Q_col_all, Q_pulse_all, dx, start, alpha, c, nx_col, IDX)\n",
        "        # print('coeff_matrix:\\n',coeff_matrix)\n",
        "        # print('vec_add:\\n',vec_add)\n",
        "\n",
        "\n",
        "\n",
        "        ####################### Building MT Terms ####################################################################\n",
        "\n",
        "        # Initialize\n",
        "\n",
        "        MT = np.zeros(len(c)) # column vector: MT kinetcis for each comp: MT = [MT_A MT_B]\n",
        "\n",
        "        for comp_idx in range(num_comp): # for each component\n",
        "\n",
        "            ######################(i) Isotherm ####################################################################\n",
        "\n",
        "            # Comment as necessary for required isotherm:\n",
        "            # isotherm = iso_bi_langmuir(theta_blang[comp_idx], c, IDX, comp_idx)\n",
        "            isotherm = iso_cup_langmuir(theta_cup_lang, c, IDX, comp_idx)\n",
        "            # print('qstar:\\n', isotherm.shape)\n",
        "            ################### (ii) MT ##########################################################\n",
        "            MT_comp = mass_transfer(kav_params[comp_idx], isotherm, q[IDX[comp_idx] + 0: IDX[comp_idx] + nx ])\n",
        "            MT[IDX[comp_idx] + 0: IDX[comp_idx] + nx ] = MT_comp\n",
        "            # [MT_A, MT_B, . . . ] KINETICS FOR EACH COMP\n",
        "\n",
        "\n",
        "\n",
        "        dc_dt = coeff_matrix @ c + vec_add - F * MT\n",
        "        dq_dt = MT\n",
        "\n",
        "        return np.concatenate([dc_dt, dq_dt])\n",
        "\n",
        "    # ##################################################################################\n",
        "\n",
        "    # SOLVING THE ODES\n",
        "    # creat storage spaces:\n",
        "    y_matrices = []\n",
        "\n",
        "    t_sets = []\n",
        "    t_lengths = []\n",
        "\n",
        "    c_IN_values_all = []\n",
        "    F_in_values_all = []\n",
        "    call = []\n",
        "\n",
        "    # print('----------------------------------------------------------------')\n",
        "    # print(\"\\n\\nSolving the ODEs. . . .\")\n",
        "\n",
        "\n",
        "\n",
        "    if iso_type == \"UNC\": # UNCOUPLED - solve 1 comp at a time\n",
        "        for comp_idx in range(num_comp): # for each component\n",
        "            # print(f'Solving comp {comp_idx}. . . .')\n",
        "            # print('\\nSolution Size:')\n",
        "            v0 = np.zeros(Ncol_num* (nx_col + nx_col)) #  for both c and q\n",
        "            solution = solve_ivp(mod1, t_span, v0, args=(comp_idx , Q_pulse_all))\n",
        "            y_solution, t = solution.y, solution.t\n",
        "            y_matrices.append(y_solution)\n",
        "            t_sets.append(t)\n",
        "            t_lengths.append(len(t))\n",
        "            # print(f'y_matrices[{i}]', y_matrices[i].shape)\n",
        "\n",
        "\n",
        "    # Assuming only a binary coupled system\n",
        "    if iso_type == \"CUP\": # COUPLED - solve\n",
        "            # nx = nx_col*num_comp\n",
        "            v0 = np.zeros(num_comp*(nx)*2) # for c and 2, for each comp\n",
        "            solution = solve_ivp(mod2, t_span, v0)\n",
        "            y_solution, t = solution.y, solution.t\n",
        "            # Convert y_solution from: [cA, cB, qA, qB] ,  TO: [[cA, qA ], [cB, qB]]\n",
        "            # Write a function to do that\n",
        "\n",
        "            def reshape_ysol(x, nx, num_comp):\n",
        "                # Initialize a list to store the reshaped components\n",
        "                reshaped_list = []\n",
        "\n",
        "                # Iterate over the number of components\n",
        "                for i in range(num_comp):\n",
        "                    # Extract cX and qX submatrices for the i-th component\n",
        "                    cX = x[i*nx:(i+1)*nx, :]      # Extract cX submatrix\n",
        "                    qX = x[i*nx + num_comp*nx : (i+1)*nx + num_comp*nx, :]       # Extract qX submatrix\n",
        "                    concat = np.concatenate([cX, qX])\n",
        "                    # print('i:', i)\n",
        "                    # print('cX:\\n',cX)\n",
        "                    # print('qX:\\n',qX)\n",
        "                    # Append the reshaped pair [cX, qX] to the list\n",
        "                    reshaped_list.append(concat)\n",
        "\n",
        "                # Convert the list to a NumPy array\n",
        "                result = np.array(reshaped_list)\n",
        "\n",
        "                return result\n",
        "\n",
        "            y_matrices = reshape_ysol(y_solution, nx, num_comp)\n",
        "            # print('len(t_sets) = ', len(t_sets[0]))\n",
        "            # print('len(t) = ', len(t))\n",
        "\n",
        "    # print('----------------------------------------------------------------')\n",
        "    # print('\\nSolution Size:')\n",
        "    # for i in range(num_comp):\n",
        "    #     print(f'y_matrices[{i}]', y_matrices[i].shape)\n",
        "    # print('----------------------------------------------------------------')\n",
        "    # print('----------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ###########################################################################################\n",
        "\n",
        "    # VISUALIZATION\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # MASS BALANCE AND PURITY CURVES\n",
        "    ###########################################################################################\n",
        "\n",
        "    def find_indices(t_ode_times, t_schedule):\n",
        "        \"\"\"\n",
        "        t_schedule -> vector of times when (events) port switches happen e.g. at [0,5,10] seconds\n",
        "        t_ode_times -> vector of times from ODE\n",
        "\n",
        "        We want to know where in t_ode_times, t_schedule occures\n",
        "        These iwll be stored as indecies in t_idx\n",
        "        Returns:np.ndarray: An array of indices in t_ode_times corresponding to each value in t_schedule.\n",
        "        \"\"\"\n",
        "        t_idx = np.searchsorted(t_ode_times, t_schedule)\n",
        "        t_idx = np.append(t_idx, len(t_ode_times))\n",
        "\n",
        "        return t_idx\n",
        "\n",
        "    # Fucntion to find the values of scheduled quantities\n",
        "    # at all t_ode_times points\n",
        "\n",
        "    def get_all_values(X, t_ode_times, t_schedule_times, Name):\n",
        "\n",
        "        \"\"\"\n",
        "        X -> Matrix of Quantity at each schedule time. e.g:\n",
        "        At t_schedule_times = [0,5,10] seconds feed:\n",
        "        a concentraction of, X = [1,2,3] g/m^3\n",
        "\n",
        "        \"\"\"\n",
        "        # Get index times\n",
        "        t_idx = find_indices(t_ode_times, t_schedule_times)\n",
        "        # print('t_idx:\\n', t_idx)\n",
        "\n",
        "        # Initialize:\n",
        "        nrows = np.shape(X)[0]\n",
        "        # print('nrows', nrows)\n",
        "\n",
        "        values = np.zeros((nrows, len(t_ode_times))) # same num of rows, we just extend the times\n",
        "        # print('np.shape(values):\\n',np.shape(values))\n",
        "\n",
        "        # Modify:\n",
        "        k = 0\n",
        "\n",
        "        for i in range(len(t_idx)-1): # during each schedule interval\n",
        "            j = i%nrows\n",
        "\n",
        "            # # k is a counter that pushes the row index to the RHS every time it loops back up\n",
        "            # if j == 0 and i == 0:\n",
        "            #     pass\n",
        "            # elif j == 0:\n",
        "            #     k += 1\n",
        "\n",
        "            # print('j',j)\n",
        "\n",
        "            X_new = np.tile(X[:,j], (len(t_ode_times[t_idx[i]:t_idx[i+1]]), 1))\n",
        "\n",
        "            values[:, t_idx[i]:t_idx[i+1]] = X_new.T # apply appropriate quantity value at approprite time intrval\n",
        "\n",
        "        # Visualize:\n",
        "        # # Table\n",
        "        # print(Name,\" Values.shape:\\n\", np.shape(values))\n",
        "        # print(Name,\" Values:\\n\", values)\n",
        "        # # Plot\n",
        "        # plt.plot(t_ode_times, values)\n",
        "        # plt.xlabel('Time (s)')\n",
        "        # plt.ylabel('X')\n",
        "        # plt.show()\n",
        "\n",
        "        return values, t_idx\n",
        "\n",
        "\n",
        "    # Function that adds row slices from a matrix M into one vector\n",
        "    def get_X_row(M, row_start, jump, width):\n",
        "\n",
        "        \"\"\"\n",
        "        M  => Matrix whos rows are to be searched and sliced\n",
        "        row_start => Starting row - the row that the 1st slice comes from\n",
        "        jump => How far the row index jumps to caputre the next slice\n",
        "        width => the widths of each slice e.g. slice 1 is M[row, width[0]:width[1]]\n",
        "\n",
        "        \"\"\"\n",
        "        # Quick look at the inpiuts\n",
        "        # print('M.shape:\\n', M.shape)\n",
        "        # print('width:', width)\n",
        "\n",
        "        # Initialize\n",
        "        values = []\n",
        "        nrows = M.shape[0]\n",
        "\n",
        "        for i in range(len(width)-1):\n",
        "            j = i%nrows\n",
        "            # print('i', i)\n",
        "            # print('j', j)\n",
        "            t_start = int(width[i])\n",
        "            tend = int(width[i+1])\n",
        "\n",
        "            kk = (row_start+j*jump)%nrows\n",
        "\n",
        "            MM = M[kk, t_start:tend]\n",
        "\n",
        "            values.extend(MM)\n",
        "\n",
        "        return values\n",
        "\n",
        "\n",
        "\n",
        "    #  MASS INTO SYSMEM\n",
        "\n",
        "    # - Only the feed port allows material to FLOW IN\n",
        "    ###########################################################################################\n",
        "\n",
        "    # Convert the Feed concentration schedule to show feed conc for all time\n",
        "    # Do this for each component\n",
        "    # C_feed_all = [[] for _ in range(num_comp)]\n",
        "\n",
        "    row_start = 0 # iniital feed port row in schedule matrix\n",
        "\n",
        "    row_start_matrix_raff = nx_col*Z3\n",
        "    row_start_matrix_ext = (nx_col*(Z3 + Z4 + Z1))\n",
        "\n",
        "    row_start_schedule_raff = row_start+Z3\n",
        "    row_start_schedule_ext = row_start+Z3+Z4+Z1\n",
        "\n",
        "    jump_schedule = 1\n",
        "    jump_matrix = nx_col\n",
        "\n",
        "\n",
        "    def feed_profile(t_odes, Cj_pulse_all, t_schedule, row_start, jump):\n",
        "\n",
        "        \"\"\"\"\n",
        "        Function that returns :\n",
        "        (i) The total mass fed of each component\n",
        "        (ii) Vector of feed conc profiles of each component\n",
        "        \"\"\"\n",
        "\n",
        "        # Storage Locations:\n",
        "        C_feed_all = []\n",
        "        t_idx_all = []\n",
        "        m_feed = []\n",
        "\n",
        "        C_feed = [[] for _ in range(num_comp)]\n",
        "\n",
        "        for i in range(num_comp):\n",
        "\n",
        "            if iso_type == 'UNC':\n",
        "\n",
        "                C, t_idx = get_all_values(Cj_pulse_all[i], t_odes[i], t_schedule, 'Concentration')\n",
        "                t_idx_all.append(t_idx)\n",
        "\n",
        "            elif iso_type == 'CUP':\n",
        "                C, t_idx_all = get_all_values(Cj_pulse_all[i], t_odes, t_schedule, 'Concentration')\n",
        "\n",
        "            C_feed_all.append(C)\n",
        "\n",
        "            # print('t_idx_all:\\n', t_idx_all )\n",
        "\n",
        "        for i in range(num_comp):\n",
        "            if iso_type == 'UNC':\n",
        "                C_feed[i] = get_X_row( C_feed_all[i], row_start, jump, t_idx_all[i]) # g/cm^3\n",
        "            elif iso_type == 'CUP':\n",
        "                C_feed[i] = get_X_row( C_feed_all[i], row_start, jump, t_idx_all) # g/cm^3\n",
        "        # print('C_feed[0]:',C_feed[0])\n",
        "\n",
        "        for i in range(num_comp):\n",
        "            F_feed = np.array([C_feed[i]]) * QF # (g/cm^3 * cm^3/s)  =>  g/s | mass flow into col (for comp, i)\n",
        "            F_feed = np.array([F_feed]) # g/s\n",
        "\n",
        "            if iso_type == 'UNC':\n",
        "                m_feed_add = integrate.simpson(F_feed, x=t_odes[i]) # g\n",
        "            if iso_type == 'CUP':\n",
        "                m_feed_add = integrate.simpson(F_feed, x=t_odes) # g\n",
        "\n",
        "            m_feed.append(m_feed_add)\n",
        "\n",
        "        m_feed = np.concatenate(m_feed) # g\n",
        "        # print(f'm_feed: {m_feed} g')\n",
        "\n",
        "        return C_feed, m_feed, t_idx_all\n",
        "\n",
        "    if iso_type == 'UNC':\n",
        "        C_feed, m_feed, t_idx_all = feed_profile(t_sets, Cj_pulse_all, t_schedule, row_start, jump_schedule)\n",
        "    elif iso_type == 'CUP':\n",
        "        C_feed, m_feed, t_idx_all = feed_profile(t, Cj_pulse_all, t_schedule, row_start, jump_schedule)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def prod_profile(t_odes, y_odes, t_schedule, row_start_matrix, jump_matrix, t_idx_all, row_start_schedule):\n",
        "\n",
        "        \"\"\"\"\n",
        "        Function that can be used to return:\n",
        "\n",
        "        (i) The total mass exited at the Raffinate or Extract ports of each component\n",
        "        (ii) Vector of Raffinate or Extract mass flow profiles of each component\n",
        "        (iii) Vector of Raffinate or Extract vol flow profiles of each component\n",
        "\n",
        "        P = Product either raff or ext\n",
        "        \"\"\"\n",
        "        ######## Storages for the Raffinate #########\n",
        "        C_P1 = []\n",
        "        C_P2 = []\n",
        "\n",
        "        Q_all_flows = [] # Flowrates expirenced by each component\n",
        "        m_out_P = np.zeros(num_comp)\n",
        "\n",
        "        P_vflows_1 = []\n",
        "        P_mflows_1 = []\n",
        "        m_P_1 = []\n",
        "\n",
        "        P_vflows_2 = []\n",
        "        P_mflows_2 = []\n",
        "        m_P_2 = []\n",
        "        t_idx_all_Q = []\n",
        "\n",
        "        P_mprofile = []\n",
        "        P_cprofile = []\n",
        "        P_vflow = [[] for _ in range(num_comp)]\n",
        "\n",
        "\n",
        "        if iso_type == 'UNC':\n",
        "            for i in range(num_comp): # for each component\n",
        "                Q_all_flows_add, b = get_all_values(Q_col_all, t_odes[i], t_schedule, 'Column Flowrates')\n",
        "                # print('Q_all_flows_add:\\n', Q_all_flows_add)\n",
        "                Q_all_flows.append(Q_all_flows_add) # cm^3/s\n",
        "                t_idx_all_Q.append(b)\n",
        "\n",
        "        elif iso_type == 'CUP':\n",
        "            Q_all_flows, t_idx_all_Q = get_all_values(Q_col_all, t_odes, t_schedule, 'Column Flowrates')\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(num_comp):# for each component\n",
        "\n",
        "            # Search the ODE matrix\n",
        "            C_R1_add = np.array(get_X_row( y_odes[i][:nx,:], row_start_matrix-1, jump_matrix, t_idx_all[i])) # exclude q\n",
        "            C_R2_add = np.array(get_X_row( y_odes[i][:nx,:], row_start_matrix, jump_matrix, t_idx_all[i]))\n",
        "            # Search the Flowrate Schedule\n",
        "            P_vflows_1_add = np.array(get_X_row(Q_all_flows[i], row_start_schedule-1, jump_schedule, t_idx_all_Q[i]))\n",
        "            P_vflows_2_add = np.array(get_X_row(Q_all_flows[i], row_start_schedule, jump_schedule, t_idx_all_Q[i]))\n",
        "\n",
        "            # Raffinate Massflow Curves\n",
        "            # print('C_R1_add.type():\\n',type(C_R1_add))\n",
        "            # print('np.shape(C_R1_add):\\n', np.shape(C_R1_add))\n",
        "\n",
        "            # print('P_vflows_1_add.type():\\n',type(P_vflows_1_add))\n",
        "            # print('np.shape(P_vflows_1_add):\\n', np.shape(P_vflows_1_add))\n",
        "\n",
        "            # Assuming only conc change accross port when (i) adding feed or (ii) desorbent\n",
        "            C_R2_add = C_R1_add\n",
        "            # P_mflows_1_add = C_R1_add * P_vflows_1_add  # (g/cm^3 * cm^3/s)  =>  g/s\n",
        "            # P_mflows_2_add = C_R2_add * P_vflows_2_add  # g/s\n",
        "\n",
        "            if row_start_matrix == row_start_matrix_raff:\n",
        "                P_vflows_1_add = -QR*np.ones_like(C_R1_add)\n",
        "                P_mflows_1_add = C_R1_add * P_vflows_1_add  # (g/cm^3 * cm^3/s)  =>  g/s\n",
        "\n",
        "            elif row_start_matrix == row_start_matrix_ext:\n",
        "                P_vflows_1_add = -QX*np.ones_like(C_R1_add)\n",
        "                P_mflows_1_add = C_R1_add * P_vflows_1_add  # (g/cm^3 * cm^3/s)  =>  g/s\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Flow profiles:\n",
        "            # Concentration\n",
        "            P_cprofile.append(C_R1_add) # g/s\n",
        "            # Mass g/s\n",
        "            P_mprofile.append(P_mflows_1_add ) #- P_mflows_2_add) # g/s\n",
        "            # Volumetric cm^3/s\n",
        "            P_vflow[i] = P_vflows_1_add #- P_vflows_2_add # cm^3\n",
        "\n",
        "            # Integrate\n",
        "            if iso_type == 'UNC':\n",
        "                m_P_add_1 = integrate.simpson(P_mflows_1_add, x=t_odes[i]) # g\n",
        "                # m_P_add_2 = integrate.simpson(P_mflows_2_add, x=t_odes[i]) # g\n",
        "\n",
        "            if iso_type == 'CUP':\n",
        "                m_P_add_1 = integrate.simpson(P_mflows_1_add, x=t_odes) # g\n",
        "                # m_P_add_2 = integrate.simpson(P_mflows_2_add, x=t_odes) # g\n",
        "\n",
        "\n",
        "\n",
        "            # Storage\n",
        "            C_P1.append(C_R1_add)  # Concentration Profiles\n",
        "            C_P2.append(C_R2_add)\n",
        "\n",
        "            P_vflows_1.append(P_vflows_1_add)\n",
        "            P_vflows_2.append(P_vflows_2_add)\n",
        "\n",
        "            P_mflows_1.append(P_mflows_1_add)\n",
        "            # P_mflows_2.append(P_mflows_2_add)\n",
        "\n",
        "            m_P_1.append(m_P_add_1) # masses of each component\n",
        "            # m_P_2.append(m_P_add_2) # masses of each component\n",
        "\n",
        "        # Final Mass Exited\n",
        "        # Mass out from P and ext\n",
        "        for i in range(num_comp):\n",
        "            m_out_P_add = m_P_1[i] #- m_P_2[i]\n",
        "            # print(f'i:{i}')\n",
        "            # print(f'm_out_P_add = m_P_1[i] - m_P_2[i]: { m_P_1[i]} - {m_P_2[i]}')\n",
        "            m_out_P[i] = m_out_P_add # [A, B] g\n",
        "\n",
        "        return P_cprofile, P_mprofile, m_out_P, P_vflow\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluating the product flowrates\n",
        "    #######################################################\n",
        "    # raff_mprofile, m_out_raff, raff_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_R1, row_start_R2, jump_matrix, t_idx_all, row_start+Z3)\n",
        "    # ext_mprofile, m_out_ext, ext_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_X1, row_start_X2, jump_matrix, t_idx_all, row_start+Z3+Z4+Z1)\n",
        "    if iso_type == 'UNC':\n",
        "        raff_cprofile, raff_mprofile, m_out_raff, raff_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_matrix_raff, jump_matrix, t_idx_all, row_start_schedule_raff)\n",
        "        ext_cprofile, ext_mprofile, m_out_ext, ext_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_matrix_ext, jump_matrix, t_idx_all, row_start_schedule_ext)\n",
        "    elif iso_type == 'CUP':\n",
        "        raff_cprofile, raff_mprofile, m_out_raff, raff_vflow = prod_profile(t, y_matrices, t_schedule, row_start_matrix_raff, jump_matrix, t_idx_all, row_start_schedule_raff)\n",
        "        ext_cprofile, ext_mprofile, m_out_ext, ext_vflow = prod_profile(t, y_matrices, t_schedule, row_start_matrix_ext, jump_matrix, t_idx_all, row_start_schedule_ext)\n",
        "    #######################################################\n",
        "    # print(f'raff_vflow: {raff_vflow}')\n",
        "    # print(f'np.shape(raff_vflow): {np.shape(raff_vflow[0])}')\n",
        "    # print(f'ext_vflow: {ext_vflow}')\n",
        "    # print(f'np.shape(ext_vflow): {np.shape(ext_vflow[0])}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # MASS BALANCE:\n",
        "    #######################################################\n",
        "\n",
        "    # Error = Expected Accumulation - Model Accumulation\n",
        "\n",
        "    #######################################################\n",
        "\n",
        "    # Expected Accumulation = Mass In - Mass Out\n",
        "    # Model Accumulation = Integral in all col at tend (how much is left in col at end of sim)\n",
        "\n",
        "\n",
        "    # Calculate Expected Accumulation\n",
        "    #######################################################\n",
        "    m_out = np.array([m_out_raff]) + np.array([m_out_ext]) # g\n",
        "    m_out = np.concatenate(m_out)\n",
        "    m_in = np.concatenate(m_feed) # g\n",
        "    # ------------------------------------------\n",
        "    Expected_Acc = m_in - m_out # g\n",
        "    # ------------------------------------------\n",
        "\n",
        "\n",
        "    # Calculate Model Accumulation\n",
        "    #######################################################\n",
        "    def model_acc(y_ode, V_col_total, e, num_comp):\n",
        "        \"\"\"\n",
        "        Func to integrate the concentration profiles at tend and estimate the amount\n",
        "        of solute left on the solid and liquid phases\n",
        "        \"\"\"\n",
        "        mass_l = np.zeros(num_comp)\n",
        "        mass_r = np.zeros(num_comp)\n",
        "\n",
        "        for i in range(num_comp): # for each component\n",
        "\n",
        "            V_l = e * V_col_total # Liquid Volume cm^3\n",
        "            V_r = (1-e)* V_col_total # resin Volume cm^3\n",
        "\n",
        "            # conc => g/cm^3\n",
        "            # V => cm^3\n",
        "            # integrate to get => g\n",
        "\n",
        "            # # METHOD 1:\n",
        "            # V_l = np.linspace(0,V_l,nx) # cm^3\n",
        "            # V_r = np.linspace(0,V_r,nx) # cm^3\n",
        "            # mass_l[i] = integrate.simpson(y_ode[i][:nx,-1], x=x)*A_col*e # mass in liq at t=tend\n",
        "            # mass_r[i] = integrate.simpson(y_ode[i][nx:,-1], x=x)*A_col*(1-e) # mass in resin at t=tend\n",
        "\n",
        "            # METHOD 2:\n",
        "            V_l = np.linspace(0,V_l,nx) # cm^3\n",
        "            V_r = np.linspace(0,V_r,nx) # cm^3\n",
        "\n",
        "            mass_l[i] = integrate.simpson(y_ode[i][:nx,-1], x=V_l) # mass in liq at t=tend\n",
        "            mass_r[i] = integrate.simpson(y_ode[i][nx:,-1], x=V_r) # mass in resin at t=tend\n",
        "\n",
        "            # METHOD 3:\n",
        "            # c_avg[i] = np.average(y_ode[i][:nx,-1]) # Average conc at t=tend\n",
        "            # q_avg[i] = np.average(y_ode[i][:nx,-1])\n",
        "\n",
        "            # mass_l = c_avg * V_l\n",
        "            # mass_r = q_avg * V_r\n",
        "\n",
        "\n",
        "        Model_Acc = mass_l + mass_r # g\n",
        "\n",
        "        return Model_Acc\n",
        "\n",
        "    Model_Acc = model_acc(y_matrices, V_col_total, e, num_comp)\n",
        "\n",
        "    # ------------------------------------------\n",
        "    Error = Model_Acc - Expected_Acc\n",
        "\n",
        "    Error_percent = (sum(Error)/sum(Expected_Acc))*100\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # Calculate KEY PERORMANCE PARAMETERS:\n",
        "    #######################################################\n",
        "    # 1. Purity\n",
        "    # 2. Recovery\n",
        "    # 3. Productivity\n",
        "\n",
        "\n",
        "    # 1. Purity\n",
        "    #######################################################\n",
        "    # 1.1 Instantanoues:\n",
        "    # raff_in_purity = raff_mprofile/sum(raff_mprofile)\n",
        "    # ext_insant_purity = ext_mprofile/sum(ext_mprofile)\n",
        "\n",
        "    # 1.2 Integral:\n",
        "    raff_intgral_purity = m_out_raff/sum(m_out_raff)\n",
        "    ext_intgral_purity = m_out_ext/sum(m_out_ext)\n",
        "\n",
        "    # Final Attained Purity in the Stream\n",
        "    raff_stream_final_purity = np.zeros(num_comp)\n",
        "    ext_stream_final_purity = np.zeros(num_comp)\n",
        "\n",
        "    for i in range(num_comp):\n",
        "        raff_stream_final_purity[i] = raff_cprofile[i][-1]\n",
        "        ext_stream_final_purity[i] = ext_cprofile[i][-1]\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Recovery\n",
        "    #######################################################\n",
        "    # 2.1 Instantanoues:\n",
        "\n",
        "    # raff_in_recovery = raff_mprofile/sum(C_feed*QF)\n",
        "    # ext_insant_recovery = ext_mprofile/sum(C_feed*QF)\n",
        "\n",
        "    # 2.2 Integral:\n",
        "    raff_recov = m_out_raff/m_in\n",
        "    ext_recov = m_out_ext/m_in\n",
        "\n",
        "    # 3. Productivity\n",
        "    #######################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Visuliization of PERORMANCE PARAMETERS:\n",
        "    #######################################################\n",
        "\n",
        "    ############## TABLES ##################\n",
        "\n",
        "\n",
        "\n",
        "    # Define the data for the table\n",
        "    # data = {\n",
        "    #     'Metric': [\n",
        "    #         'Total Mass IN',\n",
        "    #         'Total Mass OUT',\n",
        "    #         'Total Expected Acc (IN-OUT)',\n",
        "    #         'Total Model Acc (r+l)',\n",
        "    #         'Total Error (Mod-Exp)',\n",
        "    #         'Total Error Percent (relative to Exp_Acc)',\n",
        "    #         'Final Raffinate Collected Purity [A, B,. . ]',\n",
        "    #         'Final Extract Collected Purity [A, B,. . ]',\n",
        "    #         'Final Raffinate Dimensionless Stream Concentration  [A, B,. . ]',\n",
        "    #         'Final Extract Dimensionless Stream Concentration  [A, B,. . ]',\n",
        "    #         'Final Raffinate Recovery[A, B,. . ]',\n",
        "    #         'Final Extract Recovery[A, B,. . ]'\n",
        "    #     ],\n",
        "    #     'Value': [\n",
        "    #         f\"{m_in} g\",\n",
        "    #         f\"{m_out} g\",\n",
        "    #         f'{sum(Expected_Acc)} g',\n",
        "    #         f'{sum(Model_Acc)} g',\n",
        "    #         f'{sum(Error)} g',\n",
        "    #         f'{Error_percent} %',\n",
        "\n",
        "    #         f'{raff_intgral_purity} %',\n",
        "    #         f'{ext_intgral_purity} %',\n",
        "    #         f'{raff_stream_final_purity} g/cm^3',\n",
        "    #         f'{ext_stream_final_purity}',\n",
        "    #         f'{raff_recov} %',\n",
        "    #         f'{ext_recov} %'\n",
        "    #     ]\n",
        "    # }\n",
        "\n",
        "    # # Create a DataFrame\n",
        "    # df = pd.DataFrame(data)\n",
        "\n",
        "    # # Display the DataFrame\n",
        "    # print(df)\n",
        "\n",
        "    return y_matrices, nx, t, t_sets, t_schedule, C_feed, m_in, m_out, raff_cprofile, ext_cprofile, raff_intgral_purity, raff_recov, ext_intgral_purity, ext_recov, raff_vflow, ext_vflow, Model_Acc, Expected_Acc, Error_percent\n",
        "\n",
        "# ---------------- sampling\n",
        "\n",
        "def lhq_sample_mj(m_min, m_max, n_samples, diff=0.1):\n",
        "    \"\"\"\n",
        "    Function that performs Latin Hypercube (LHS) sampling for [m1, m2, m3, m4]\n",
        "    Note that for all mjs: (m_min < m_j < m_max)\n",
        "    And that:\n",
        "          (i)   m4 < m1 - (diff*m1) and m2 < m1 - (diff*m1)\n",
        "          (ii)  m2 < m3 - (diff*m3)\n",
        "          (iii) m3 > m4 + (diff*m4)\n",
        "    Final result is an np.array of size: (n_samples, 4)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the array to store the samples\n",
        "    samples = np.zeros((n_samples, 4))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Sample m1, m2, m3, m4 within bounds and respecting constraints\n",
        "        m1 = np.random.uniform(m_min, m_max)\n",
        "        m4 = np.random.uniform(m_min, m1-diff*m1)\n",
        "\n",
        "        # Sample m2 such that it respects the constraint: m2 < m1 - (diff*m1)\n",
        "        m2 = np.random.uniform(m_min, m_max)\n",
        "        while m2 >= m1 - (diff * m1):  # Ensuring m2 < m1 - (diff*m1)\n",
        "            m2 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Sample m3 such that it respects the constraint: m3 > m2 + (diff*m2)\n",
        "        m3 = np.random.uniform(m_min, m_max)\n",
        "        while m3 <= m2 + (diff * m2):  # Ensuring m3 > m2 + (diff*m2)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Ensure the constraint: m3 > m4 + (diff * m4)\n",
        "        while m3 <= m4 + (diff * m4):  # Ensuring m3 > m4 + (diff*m4)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Store the sample in the array\n",
        "        samples[i] = [m1, m2, m3, m4]\n",
        "\n",
        "\n",
        "    return samples\n",
        "\n",
        "def fixed_feed_lhq_sample_mj(t_index_min, Q_fixed_feed, m_min, m_max, n_samples, diff=0.1):\n",
        "    \"\"\"\n",
        "    Since the feed is fixed, m3 is caluclated\n",
        "\n",
        "    Function that performs Latin Hypercube (LHS) sampling for [m1, m2, m3, m4]\n",
        "    Note that for all mjs: (m_min < m_j < m_max)\n",
        "    And that:\n",
        "          (i)   m4 < m1 - (diff*m1) and m2 < m1 - (diff*m1)\n",
        "          (ii)  m2 < m3 - (diff*m3)\n",
        "          (iii) m3 > m4 + (diff*m4)\n",
        "    Final result is an np.array of size: (n_samples, 4)\n",
        "    \"\"\"\n",
        "    # Initialize the array to store the samples\n",
        "    samples = np.zeros((n_samples, 4))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Sample m1, m2, m3, m4 within bounds and respecting constraints\n",
        "        m1 = np.random.uniform(m_min, m_max)\n",
        "        m4 = np.random.uniform(m_min, m1-diff*m1)\n",
        "\n",
        "        # Sample m2 such that it respects the constraint: m2 < m1 - (diff*m1)\n",
        "        m2 = np.random.uniform(m_min, m_max)\n",
        "        while m2 >= m1 - (diff * m1):  # Ensuring m2 < m1 - (diff*m1)\n",
        "            m2 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Sample m3 such that it respects the constraint: m3 > m2 + (diff*m2)\n",
        "        m3 = np.random.uniform(m_min, m_max)\n",
        "        while m3 <= m2 + (diff * m2):  # Ensuring m3 > m2 + (diff*m2)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Ensure the constraint: m3 > m4 + (diff * m4)\n",
        "        while m3 <= m4 + (diff * m4):  # Ensuring m3 > m4 + (diff*m4)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Store the sample in the array\n",
        "        samples[i] = [m1, m2, m3, m4]\n",
        "\n",
        "    return samples\n",
        "\n",
        "def fixed_m1_and_m4_lhq_sample_mj(m1, m4, m_min, m_max, n_samples, n_m2_div, diff=0.1):\n",
        "    \"\"\"\n",
        "    - Since the feed is fixed, m3 is caluclated AND\n",
        "    - Since the desorbant is fixed, m1 is caluclated\n",
        "\n",
        "    Function that performs Latin Hypercube (LHS) sampling for [m1, m2, m3, m4]\n",
        "    Note that for all mjs: (m_min < m_j < m_max)\n",
        "    And that:\n",
        "          (i)   m4 < m1 - (diff*m1) and m2 < m1 - (diff*m1)\n",
        "          (ii)  m2 < m3 - (diff*m3)\n",
        "          (iii) m3 > m4 + (diff*m4)\n",
        "    Final result is an np.array of size: (n_samples, 4)\n",
        "\n",
        "    [1.78902051 1.10163238 1.75875405 0.20421105], 7.438074624877448\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the array to store the samples\n",
        "    samples = np.zeros((n_samples*n_m2_div, 5))\n",
        "    samples[:,0] = np.ones(n_samples*n_m2_div)*m_max\n",
        "    samples[:,-2] = np.ones(n_samples*n_m2_div)*2\n",
        "    # print(f'samples: {samples}')\n",
        "    nn = int(np.round(n_samples/2))\n",
        "    num_of_m3_per_m2 = n_samples\n",
        "\n",
        "    m2_set = np.linspace(m_min, m_max*0.9, n_m2_div)\n",
        "    # print(f'm2_set: {m2_set}')\n",
        "\n",
        "    i = np.arange(len(m2_set))\n",
        "    k = np.repeat(i,num_of_m3_per_m2)\n",
        "\n",
        "    print(f'k:{k}')\n",
        "    #Sample from the separation triangle:\n",
        "    for i in range(len(k)): # for each vertical line\n",
        "        # print(f'k: {k[i]}')\n",
        "        m2 = m2_set[k[i]]\n",
        "\n",
        "        samples[i, 1] = m2\n",
        "\n",
        "        if i == 0:\n",
        "          m2 = 3.54\n",
        "          m3 = m2 + 0.1\n",
        "          samples[i, 1] = m2\n",
        "          samples[i, 2] = m3  # apex of trianlge\n",
        "        else:\n",
        "          m3 = np.random.uniform(m2, m_max)\n",
        "\n",
        "        samples[i, 2] = m3\n",
        "        samples[i,-1] = 0.6\n",
        "    return samples\n",
        "\n",
        "\n",
        "# ---------- Objective Function\n",
        "\n",
        "def mj_to_Qj(mj):\n",
        "  '''\n",
        "  Converts flowrate ratios to internal flowrates - flowrates within columns\n",
        "  '''\n",
        "  Qj = (mj*V_col*(1-e) + V_col*e)/(t_index_min*60) # cm^3/s\n",
        "  return Qj\n",
        "\n",
        "# Define the obj and constraint functions\n",
        "# All parameteres\n",
        "def obj_con(X):\n",
        "  \"\"\"Feasibility weighted objective; zero if not feasible.\n",
        "\n",
        "    X = [m1, m2, m3, m4, t_index];\n",
        "    Objective: WAR = Weighted Average Recovery\n",
        "    Constraint: WAP = Weighted Average Purity\n",
        "\n",
        "    Use WAP to calculate the feasibility weights. Which\n",
        "    will scale teh EI output.\n",
        "\n",
        "  \"\"\"\n",
        "  X = np.array(X)\n",
        "\n",
        "\n",
        "  # print(f'np.shape(x_new)[0]: {np.shape(X)}')\n",
        "  if X.ndim == 1:\n",
        "\n",
        "      Pur = np.zeros(2)\n",
        "      Rec = np.zeros(2)\n",
        "      # Unpack and convert to float and np.arrays from torch.tensors:\n",
        "      m1, m2, m3, m4, t_index_min = float(X[0]), float(X[1]), float(X[2]), float(X[3]), float(X[4])*t_reff\n",
        "\n",
        "      print(f'[m1, m2, m3, m4]: [{m1}, {m2}, {m3}, {m4}], t_index: {t_index_min}')\n",
        "\n",
        "      Q_I, Q_II, Q_III, Q_IV = mj_to_Qj(m1), mj_to_Qj(m2), mj_to_Qj(m3), mj_to_Qj(m4)\n",
        "      Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV]) # cm^3/s\n",
        "      Qfeed = Q_III - Q_II\n",
        "      Qraffinate = Q_III - Q_IV\n",
        "      Qdesorbent = Q_I - Q_IV\n",
        "      Qextract = Q_I - Q_II\n",
        "      Q_external = np.array([Qfeed, Qraffinate, Qdesorbent,Qextract])\n",
        "      print(f'Q_internal: {Q_internal} cm^s/s')\n",
        "      print(f'Q_internal: {Q_internal*3.6} L/h')\n",
        "\n",
        "      print(f'----------------------------------')\n",
        "      print(f'Q_external: {Q_external} cm^s/s')\n",
        "      print(f'Q_external: {Q_external*3.6} L/h')\n",
        "      # print(f'Q_internal type: {type(Q_internal)}')\n",
        "\n",
        "      SMB_inputs[12] = t_index_min  # Update t_index\n",
        "      SMB_inputs[14] = Q_internal # Update Q_internal\n",
        "\n",
        "      results = SMB(SMB_inputs)\n",
        "\n",
        "      # print(f'done solving sample {i+1}')\n",
        "\n",
        "      raff_purity = results[10]  # [Glu, Fru]\n",
        "      ext_purity = results[12]  # [Glu, Fru]\n",
        "\n",
        "      raff_recovery = results[11]  # [Glu, Fru]\n",
        "      ext_recovery = results[13]  # [Glu, Fru]\n",
        "\n",
        "      pur1 = raff_purity[0]\n",
        "      pur2 = ext_purity[1]\n",
        "\n",
        "      rec1 = raff_recovery[0]\n",
        "      rec2 = ext_recovery[1]\n",
        "\n",
        "      # Pack\n",
        "      # WAP[i] = WAP_add\n",
        "      # WAR[i] = WAR_add\n",
        "      Pur[:] = [pur1, pur2]\n",
        "      Rec[:] = [rec1, rec2]\n",
        "\n",
        "  elif X.ndim > 1:\n",
        "      Pur = np.zeros((len(X[:,0]), 2))\n",
        "      Rec = np.zeros((len(X[:,0]), 2))\n",
        "\n",
        "      for i in range(len(X[:,0])):\n",
        "\n",
        "          # Unpack and convert to float and np.arrays from torch.tensors:\n",
        "          m1, m2, m3, m4, t_index_min = float(X[i,0]), float(X[i,1]), float(X[i,2]), float(X[i,3]), float(X[i,4])*t_reff\n",
        "\n",
        "          print(f'[m1, m2, m3, m4]: [{m1}, {m2}, {m3}, {m4}], t_index: {t_index_min}')\n",
        "          Q_I, Q_II, Q_III, Q_IV = mj_to_Qj(m1), mj_to_Qj(m2), mj_to_Qj(m3), mj_to_Qj(m4)\n",
        "          Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV]) # cm^3/s\n",
        "          # Calculate and display external flowrates too\n",
        "          Qfeed = Q_III - Q_II\n",
        "          Qraffinate = Q_III - Q_IV\n",
        "          Qdesorbent = Q_I - Q_IV\n",
        "          Qextract = Q_I - Q_II\n",
        "\n",
        "          Q_external = np.array([Qfeed, Qraffinate, Qdesorbent,Qextract])\n",
        "          print(f'Q_internal: {Q_internal} cm^s/s')\n",
        "          print(f'Q_internal: {Q_internal*3.6} L/h')\n",
        "\n",
        "          print(f'----------------------------------')\n",
        "          print(f'Q_external: {Q_external} cm^s/s')\n",
        "          print(f'Q_external: {Q_external*3.6} L/h')\n",
        "\n",
        "\n",
        "          # print(f'Q_internal type: {type(Q_internal)}')\n",
        "          # Update SMB_inputs:\n",
        "          SMB_inputs[12] = t_index_min  # Update t_index\n",
        "          SMB_inputs[14] = Q_internal # Update Q_internal\n",
        "\n",
        "          results = SMB(SMB_inputs)\n",
        "\n",
        "          # print(f'done solving sample {i+1}')\n",
        "\n",
        "          raff_purity = results[10]  # [Glu, Fru]\n",
        "          ext_purity = results[12]  # [Glu, Fru]\n",
        "\n",
        "          raff_recovery = results[11]  # [Glu, Fru]\n",
        "          ext_recovery = results[13]  # [Glu, Fru]\n",
        "\n",
        "          pur1 = raff_purity[0]\n",
        "          pur2 = ext_purity[1]\n",
        "\n",
        "          rec1 = raff_recovery[0]\n",
        "          rec2 = ext_recovery[1]\n",
        "\n",
        "          # Pack\n",
        "          # WAP[i] = WAP_add\n",
        "          # WAR[i] = WAR_add\n",
        "          Pur[i,:] = [pur1, pur2]\n",
        "          Rec[i,:] = [rec1, rec2]\n",
        "          print(f'Pur: {pur1}, {pur2}')\n",
        "          print(f'Rec: {rec1}, {rec2}\\n\\n')\n",
        "\n",
        "  return  Rec, Pur, np.array([m1, m2, m3, m4, t_index_min])\n",
        "\n",
        "# Fixed index time\n",
        "def obj_con_fix_t(X):\n",
        "  \"\"\"Feasibility weighted objective; zero if not feasible.\n",
        "\n",
        "    X = [m1, m2, m3, m4];\n",
        "    Objective: WAR = Weighted Average Recovery\n",
        "    Constraint: WAP = Weighted Average Purity\n",
        "\n",
        "    Use WAP to calculate the feasibility weights. Which\n",
        "    will scale teh EI output.\n",
        "\n",
        "  \"\"\"\n",
        "  X = np.array(X)\n",
        "\n",
        "\n",
        "  # print(f'np.shape(x_new)[0]: {np.shape(X)}')\n",
        "  if X.ndim == 1:\n",
        "\n",
        "      Pur = np.zeros(2)\n",
        "      Rec = np.zeros(2)\n",
        "      # Unpack and convert to float and np.arrays from torch.tensors:\n",
        "      m1, m2, m3, m4 = float(X[0]), float(X[1]), float(X[2]), float(X[3])\n",
        "\n",
        "      print(f'[m1, m2, m3, m4]: [{m1}, {m2}, {m3}, {m4}], t_index: {t_index_min}')\n",
        "\n",
        "      Q_I, Q_II, Q_III, Q_IV = mj_to_Qj(m1), mj_to_Qj(m2), mj_to_Qj(m3), mj_to_Qj(m4)\n",
        "      Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV]) # cm^3/s\n",
        "      Qfeed = Q_III - Q_II\n",
        "      Qraffinate = Q_III - Q_IV\n",
        "      Qdesorbent = Q_I - Q_IV\n",
        "      Qextract = Q_I - Q_II\n",
        "      Q_external = np.array([Qfeed, Qraffinate, Qdesorbent,Qextract])\n",
        "      print(f'Q_internal: {Q_internal} cm^s/s')\n",
        "      print(f'Q_internal: {Q_internal*3.6} L/h')\n",
        "\n",
        "      print(f'----------------------------------')\n",
        "      print(f'Q_external: {Q_external} cm^s/s')\n",
        "      print(f'Q_external: {Q_external*3.6} L/h')\n",
        "      # print(f'Q_internal type: {type(Q_internal)}')\n",
        "\n",
        "      SMB_inputs[12] = t_index_min  # Update t_index\n",
        "      SMB_inputs[14] = Q_internal # Update Q_internal\n",
        "\n",
        "      results = SMB(SMB_inputs)\n",
        "\n",
        "      # print(f'done solving sample {i+1}')\n",
        "\n",
        "      raff_purity = results[10]  # [Glu, Fru]\n",
        "      ext_purity = results[12]  # [Glu, Fru]\n",
        "\n",
        "      raff_recovery = results[11]  # [Glu, Fru]\n",
        "      ext_recovery = results[13]  # [Glu, Fru]\n",
        "\n",
        "      pur1 = raff_purity[0]\n",
        "      pur2 = ext_purity[1]\n",
        "\n",
        "      rec1 = raff_recovery[0]\n",
        "      rec2 = ext_recovery[1]\n",
        "\n",
        "      # Pack\n",
        "      # WAP[i] = WAP_add\n",
        "      # WAR[i] = WAR_add\n",
        "      Pur[:] = [pur1, pur2]\n",
        "      Rec[:] = [rec1, rec2]\n",
        "\n",
        "  elif X.ndim > 1:\n",
        "      Pur = np.zeros((len(X[:,0]), 2))\n",
        "      Rec = np.zeros((len(X[:,0]), 2))\n",
        "\n",
        "      for i in range(len(X[:,0])):\n",
        "\n",
        "          # Unpack and convert to float and np.arrays from torch.tensors:\n",
        "          m1, m2, m3, m4, t_index_min = float(X[i,0]), float(X[i,1]), float(X[i,2]), float(X[i,3]), float(X[i,4])*t_reff\n",
        "\n",
        "          print(f'[m1, m2, m3, m4]: [{m1}, {m2}, {m3}, {m4}], t_index: {t_index_min}')\n",
        "          Q_I, Q_II, Q_III, Q_IV = mj_to_Qj(m1), mj_to_Qj(m2), mj_to_Qj(m3), mj_to_Qj(m4)\n",
        "          Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV]) # cm^3/s\n",
        "          # Calculate and display external flowrates too\n",
        "          Qfeed = Q_III - Q_II\n",
        "          Qraffinate = Q_III - Q_IV\n",
        "          Qdesorbent = Q_I - Q_IV\n",
        "          Qextract = Q_I - Q_II\n",
        "\n",
        "          Q_external = np.array([Qfeed, Qraffinate, Qdesorbent,Qextract])\n",
        "          print(f'Q_internal: {Q_internal} cm^s/s')\n",
        "          print(f'Q_internal: {Q_internal*3.6} L/h')\n",
        "\n",
        "          print(f'----------------------------------')\n",
        "          print(f'Q_external: {Q_external} cm^s/s')\n",
        "          print(f'Q_external: {Q_external*3.6} L/h')\n",
        "\n",
        "\n",
        "          # print(f'Q_internal type: {type(Q_internal)}')\n",
        "          # Update SMB_inputs:\n",
        "          SMB_inputs[12] = t_index_min  # Update t_index\n",
        "          SMB_inputs[14] = Q_internal # Update Q_internal\n",
        "\n",
        "          results = SMB(SMB_inputs)\n",
        "\n",
        "          # print(f'done solving sample {i+1}')\n",
        "\n",
        "          raff_purity = results[10]  # [Glu, Fru]\n",
        "          ext_purity = results[12]  # [Glu, Fru]\n",
        "\n",
        "          raff_recovery = results[11]  # [Glu, Fru]\n",
        "          ext_recovery = results[13]  # [Glu, Fru]\n",
        "\n",
        "          pur1 = raff_purity[0]\n",
        "          pur2 = ext_purity[1]\n",
        "\n",
        "          rec1 = raff_recovery[0]\n",
        "          rec2 = ext_recovery[1]\n",
        "\n",
        "          # Pack\n",
        "          # WAP[i] = WAP_add\n",
        "          # WAR[i] = WAR_add\n",
        "          Pur[i,:] = [pur1, pur2]\n",
        "          Rec[i,:] = [rec1, rec2]\n",
        "          print(f'Pur: {pur1}, {pur2}')\n",
        "          print(f'Rec: {rec1}, {rec2}\\n\\n')\n",
        "\n",
        "  return  Rec, Pur, np.array([m1, m2, m3, m4, t_index_min])\n",
        "\n",
        "\n",
        "# ------ Generate Initial Data\n",
        "def generate_initial_data(sampling_budget):\n",
        "\n",
        "    # generate training data\n",
        "    # print(f'Getting {sampling_budget} Samples')\n",
        "    # train_x = lhq_sample_mj(0.2, 1.7, n, diff=0.1)\n",
        "    # train_x = fixed_feed_lhq_sample_mj(t_index_min, Q_fixed_feed, 0.2, 1.7, n, diff=0.1)\n",
        "    train_all = fixed_m1_and_m4_lhq_sample_mj(m_max, m_min, m_min, m_max, sampling_budget, 1, diff=0.1)\n",
        "    # print(f'train_all: {train_all}')\n",
        "    # print(f'Done Getting {sampling_budget} Samples')\n",
        "\n",
        "    # print(f'Solving Over {sampling_budget} Samples')\n",
        "    Rec, Pur, mjs = obj_con(train_all)\n",
        "    # print(f'Rec: {Rec}, Pur: {Pur}')\n",
        "    # print(f'Done Getting {sampling_budget} Samples')\n",
        "    all_outputs = np.hstack((Rec, Pur))\n",
        "    return train_all, all_outputs\n",
        "\n",
        "# ------------------ BO FUNTIONS\n",
        "# --- Surrogate model creation ---\n",
        "def surrogate_model(X_train, y_train):\n",
        "    X_train = np.atleast_2d(X_train)\n",
        "    y_train = np.atleast_1d(y_train)\n",
        "\n",
        "    if y_train.ndim == 2 and y_train.shape[1] == 1:\n",
        "        y_train = y_train.ravel()\n",
        "\n",
        "    # kernel = C(1.0, (1e-4, 10.0)) * RBF(1.0, (1e-4, 10.0))\n",
        "    kernel = Matern(length_scale=1.0, nu=1.5)\n",
        "\n",
        "    gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-5, normalize_y=True, n_restarts_optimizer=5)\n",
        "\n",
        "    gp.fit(X_train, y_train)\n",
        "\n",
        "    return gp\n",
        "\n",
        "# --- AQ funcs:\n",
        "\n",
        "# --- AQ func: Expected Constrained Improvement ---\n",
        "def log_expected_constrained_improvement(x, surrogate_obj_gp, constraint_gps, constraint_thresholds, y_best,job, PF_weight, xi=0.01):\n",
        "    x = np.asarray(x).reshape(1, -1)\n",
        "\n",
        "    mu_obj, sigma_obj = surrogate_obj_gp.predict(x, return_std=True)\n",
        "    # print(f'mu_obj: {mu_obj}, sigma_obj: {sigma_obj} ')\n",
        "\n",
        "\n",
        "\n",
        "    with np.errstate(divide='warn'):\n",
        "\t# Note that, because we are maximizing, y_best > mu_obj - always,\n",
        "\t# So Z is always positive. And if Z is positive then its on the \"right-hand-side\" of the mean\n",
        "\t# Because norm.cdf calcualtes the integral from left to right, it will by default calculate-\n",
        "\t# the probability of begin LESS than (or equal to), Z.\n",
        "\t# Since we want to probability of being greater than or equal to Z, we have two options\n",
        "\t# (1.) Calculate Z -> and compute 1-norm.cdf(Z)\n",
        "\t# (2.) Calculate abs(Z) -> make Z negative, -abs(Z) -> integrate norm.cdf(-abs(Z))\n",
        "\n",
        "\t# The (2.) option is more robust - because by simply omitting the negative sign, it will also work for cases where we are minimizing the objective\n",
        "\t# (2.) is used below\n",
        "\n",
        "        if job == 'maximize':\n",
        "                # print(f'{job}ing')\n",
        "                # Calulate Z and enforce, -ve\n",
        "                Z = -np.abs(y_best - mu_obj) / sigma_obj\n",
        "                #  Note: ei is always positive\n",
        "\n",
        "                # 1.\n",
        "                # use logcdf() to get the log probability for cases where Z is very small\n",
        "                # Since Z < 0, logcdf() < 0 (not a probability)\n",
        "                log_cdf_term = norm.logcdf(Z)\n",
        "                # recover the actual probabitlty\n",
        "                cdf_term = np.exp(log_cdf_term)\n",
        "\n",
        "                # 2. Do the same for the pdf term\n",
        "                # Since Z < 0, logpdf() < 0 (not a likihood)\n",
        "                log_pdf_term = norm.logcdf(Z)\n",
        "                # recover the actual likihood\n",
        "                pdf_term = np.exp(log_pdf_term)\n",
        "                # ---------------------------------------------------\n",
        "                ei = (y_best - mu_obj) * cdf_term + sigma_obj * pdf_term\n",
        "\n",
        "        elif job == 'minimize':\n",
        "                # Calulate Z and enforce, -ve\n",
        "                Z = np.abs(y_best - mu_obj) / sigma_obj\n",
        "                #  Note: ei is always positive\n",
        "\n",
        "                # 1.\n",
        "                # use logcdf() to get the log probability for cases where Z is very small\n",
        "                # Since Z < 0, logcdf() < 0 (not a probability)\n",
        "                log_cdf_term = norm.logcdf(Z)\n",
        "                # recover the actual probabitlty\n",
        "                cdf_term = np.exp(log_cdf_term)\n",
        "\n",
        "                # 2. Do the same for the pdf term\n",
        "                # Since Z < 0, logpdf() < 0 (not a likihood)\n",
        "                log_pdf_term = norm.logcdf(Z)\n",
        "                # recover the actual likihood\n",
        "                pdf_term = np.exp(log_pdf_term)\n",
        "                # ---------------------------------------------------\n",
        "                ei = (mu_obj-y_best)*cdf_term + sigma_obj * pdf_term\n",
        "\n",
        "\t\t# print(f'ei: {ei}')\n",
        "\n",
        "    # print(f'ei: {ei}')\n",
        "\n",
        "\n",
        "    # Calcualte the probability of Feasibility, \"prob_feas\"\n",
        "    prob_feas = 1.0 # initialize\n",
        "\n",
        "    for gp_c, lam in zip(constraint_gps, constraint_thresholds):\n",
        "\n",
        "        mu_c, sigma_c = gp_c.predict(x, return_std=True)\n",
        "\n",
        "        # lam -> inf = 1- (-inf -> lam)\n",
        "        prob_that_LESS_than_mu = norm.cdf((lam - mu_c) / sigma_c)\n",
        "\n",
        "        prob_that_GREATER_than_mu = 1 - prob_that_LESS_than_mu\n",
        "\n",
        "        pf = prob_that_GREATER_than_mu\n",
        "\n",
        "        # pf is a vector,\n",
        "        # We just want the non-zero part\n",
        "        pf = pf[pf != 0]\n",
        "        # If theres no, non-zero part, we need to Avoid pf being an empty array:\n",
        "        if pf.size == 0 or pf < 1e-12:\n",
        "            pf = 1e-8\n",
        "\n",
        "\n",
        "        # print(f'pf: {pf}')\n",
        "\n",
        "        # if we assume that the condtions are independent,\n",
        "        # then we can \"multiply\" the weights to get the \"joint probability\" of feasility\n",
        "        prob_feas *= pf\n",
        "\n",
        "    # print(f'ei: {ei}')\n",
        "    # print(f'prob_feas: {prob_feas}')\n",
        "\n",
        "    log_eic = np.log(ei) + PF_weight*np.log(prob_feas)\n",
        "    # print(f'log_eic: {log_eic}')\n",
        "    # print(f'Convert to float')\n",
        "\n",
        "    log_eic = float(np.squeeze(log_eic))  # Convert to scalar\n",
        "    # print(f'log_eic: {log_eic}')\n",
        "    return -log_eic\n",
        "\n",
        "# 1. Expected Improvement ---\n",
        "def expected_improvement(x, surrogate_gp, y_best):\n",
        "    \"\"\"\n",
        "    Computes the Expected Improvement at a point x.\n",
        "    Scalarizes the surrogate predictions using Tchebycheff, then computes EI.\n",
        "\n",
        "    Note that the surrogate GP already has the weights applied to it\n",
        "    \"\"\"\n",
        "    x = np.array(x).reshape(1, -1)\n",
        "\n",
        "    mu, sigma = surrogate_gp.predict(x, return_std=True)\n",
        "\n",
        "    # print(f'mu: {mu}')\n",
        "    # print(f'y_best: {y_best}')\n",
        "    # Compute EI\n",
        "\n",
        "    xi = 0.2 # the greater the value of xi, the more we encourage exploration\n",
        "    with np.errstate(divide='warn'):\n",
        "        Z = ( mu - y_best - xi) / sigma\n",
        "        ei = np.abs(mu - y_best - xi) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
        "        ei[sigma == 0.0] = 0.0\n",
        "\n",
        "    return -ei[0]  # Negative for minimization\n",
        "\n",
        "    # 2. Probability of Imporovement:\n",
        "def probability_of_improvement(x, surrogate_gp, y_best, xi=0.005):\n",
        "    \"\"\"\n",
        "    Computes the Probability of Improvement (PI) acquisition function.\n",
        "\n",
        "    Parameters:\n",
        "    - mu: np.array of predicted means (shape: [n_samples])\n",
        "    - sigma: np.array of predicted std deviations (shape: [n_samples])\n",
        "    - best_f: scalar, best objective value observed so far\n",
        "    - xi: float, small value to balance exploration/exploitation (default: 0.01)\n",
        "\n",
        "    Returns:\n",
        "    - PI: np.array of probability of improvement values\n",
        "    \"\"\"\n",
        "    x = np.array(x).reshape(1, -1)\n",
        "\n",
        "    mu, sigma = surrogate_gp.predict(x, return_std=True)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if sigma == 0:\n",
        "      sigma = 1e-8\n",
        "\n",
        "    z = (y_best - mu - xi) / sigma\n",
        "\n",
        "    pi = 1 - norm.cdf(z)\n",
        "\n",
        "    return -pi\n",
        "\n",
        "# --- ParEGO Main Loop ---\n",
        "def constrained_BO(optimization_budget, bounds, initial_guess, all_initial_inputs, all_initial_ouputs, job_max_or_min, constraint_thresholds, xi):\n",
        "\n",
        "    # xi = exploration parameter (the larger it is, the more we explore)\n",
        "\n",
        "    # Initial values\n",
        "    # Unpack from: all_initial_ouputs: [GPur, FPur, GRec, FRec]\n",
        "    # Recovery Objectives\n",
        "    f1_vals = all_initial_ouputs[:,0]\n",
        "    f2_vals = all_initial_ouputs[:,1]\n",
        "    # print(f'f1_vals: {f1_vals}')\n",
        "    # print(f'f2_vals: {f2_vals}')\n",
        "    # Purity constraints\n",
        "    c1_vals  = all_initial_ouputs[:,2]\n",
        "    c2_vals  = all_initial_ouputs[:,3]\n",
        "    print(f'c1-size: {np.shape(c1_vals)}')\n",
        "    print(f'c2-size: {np.shape(c2_vals)}')\n",
        "\n",
        "    # population = np.delete(all_initial_inputs, 2, axis=1)\n",
        "    population = all_initial_inputs\n",
        "    all_inputs = all_initial_inputs # [m1, m2, m3, m4, t_index]\n",
        "    # print(f'np.shape(all_inputs):{np.shape(all_inputs)}')\n",
        "    # print(f'np.shape(all_initial_inputs):{np.shape(all_initial_inputs)}')\n",
        "\n",
        "\n",
        "    # Unpack from: all_initial_inputs\n",
        "\n",
        "    # print(f'shpae_f1_vals = {np.shape(f1_vals)}')\n",
        "\n",
        "    # Initialize where we will store solutions\n",
        "    population_all = []\n",
        "    all_constraint_1_gps = []\n",
        "    all_constraint_2_gps = []\n",
        "    ei_all = []\n",
        "\n",
        "\n",
        "    for gen in range(optimization_budget):\n",
        "        # generation = iteration\n",
        "        print(f\"\\n\\nStarting gen {gen+1}\")\n",
        "\n",
        "\n",
        "        # Generate random weights for scalarization\n",
        "        lam = np.random.rand()\n",
        "        weights = [lam, 1 - lam]\n",
        "        # print(f'weights: {weights}')\n",
        "        # Note that we generate new weights in each iteration/generation\n",
        "        # i.e. each time we update the training set\n",
        "\n",
        "        #SCALARIZE THE OBJECTIVES (BEFORE APPLYING GP)\n",
        "        phi = 0.05\n",
        "        scalarized_f_vals = np.maximum(weights[0]*f1_vals, weights[1]*f2_vals)\n",
        "\n",
        "        scalarized_f_vals += phi*(weights[0]*f1_vals + weights[1]*f2_vals)\n",
        "\n",
        "        scalarized_f_vals = weights[0]*f1_vals + weights[1]*f2_vals\n",
        "\n",
        "        # Fit GP to scalarized_surrogate_objective\n",
        "        # print(f'population { population}, \\nscalarized_f_vals {scalarized_f_vals} ')\n",
        "        scalarized_surrogate_gp = surrogate_model(population, scalarized_f_vals)\n",
        "        # Pull mean at relevant poputlation points\n",
        "        # Mean & Varriance\n",
        "        scalarized_surrogate_gp_mean, scalarized_surrogate_gp_std = scalarized_surrogate_gp.predict(population, return_std=True)\n",
        "        # The best value so far:\n",
        "        y_best = np.max(scalarized_surrogate_gp_mean)\n",
        "        # y_best = 0.60\n",
        "\n",
        "\n",
        "        # Fit a GP to each constraint:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "\n",
        "            # Glu Raff Purity:\n",
        "            constraint_1_gp = surrogate_model(population, c1_vals)\n",
        "            all_constraint_1_gps.append(constraint_1_gp)\n",
        "            # Fru Ext Purity:\n",
        "            constraint_2_gp = surrogate_model(population, c2_vals)\n",
        "            all_constraint_2_gps.append(constraint_2_gp)\n",
        "\n",
        "        # Define the constraint function for the ei optimizer\n",
        "        # Constraint function with correct shape\n",
        "\n",
        "        # Define the non-linear constraint functions with dependencies\n",
        "        # note that each constraint must be written independently\n",
        "        # --- CONSTANTS ---\n",
        "        eps = 0.01  # Safety margin (1%)\n",
        "        small_value = 1e-6  # To avoid division by zero\n",
        "\n",
        "        # --- Helper: safe t_index ---\n",
        "        def get_safe_tindex(x):\n",
        "            return max(x[-1]*60*t_reff, small_value)\n",
        "\n",
        "        # --- CONSTRAINT FUNCTIONS ---\n",
        "\n",
        "        # Standard Flow Ratio Constraints:\n",
        "        def constraint_m1_gt_m2(x): return x[0] - (1 + eps) * x[1]\n",
        "        def constraint_m1_gt_m4(x): return x[0] - (1 + eps) * x[3]\n",
        "\n",
        "        def constraint_m2_lt_m1(x): return (1 - eps) * x[0] - x[1]\n",
        "        def constraint_m2_lt_m3(x): return (1 - eps) * x[2] - x[1]\n",
        "\n",
        "        def constraint_m3_gt_m2(x): return x[2] - (1 + eps) * x[1]\n",
        "        def constraint_m3_gt_m4(x): return x[2] - (1 + eps) * x[3]\n",
        "\n",
        "        def constraint_m4_lt_m1(x): return (1 - eps) * x[0] - x[3]\n",
        "        def constraint_m4_lt_m3(x): return (1 - eps) * x[2] - x[3]\n",
        "\n",
        "        # Pump Constraints (flow differences divided by t_index)\n",
        "        def constraint_feed_pump_upper(x): return m_diff_max - (x[2] - x[1]) / get_safe_tindex(x)\n",
        "        def constraint_feed_pump_lower(x): return (x[2] - x[1]) / get_safe_tindex(x) - m_diff_min\n",
        "\n",
        "        def constraint_desorb_pump_upper(x): return m_diff_max - (x[0] - x[3]) / get_safe_tindex(x)\n",
        "        def constraint_desorb_pump_lower(x): return (x[0] - x[3]) / get_safe_tindex(x) - m_diff_min\n",
        "\n",
        "        def constraint_raff_pump_upper(x): return m_diff_max - (x[2] - x[3]) / get_safe_tindex(x)\n",
        "        def constraint_raff_pump_lower(x): return (x[2] - x[3]) / get_safe_tindex(x) - m_diff_min\n",
        "\n",
        "        def constraint_extract_pump_upper(x): return m_diff_max - (x[0] - x[1]) / get_safe_tindex(x)\n",
        "        def constraint_extract_pump_lower(x): return (x[0] - x[1]) / get_safe_tindex(x) - m_diff_min\n",
        "\n",
        "        # Fixed Feed Flow Constraint\n",
        "        def constraint_fixed_feed(x):\n",
        "            return (x[2] - x[1]) - (Q_fixed_feed / ((V_col * (1-e)) / get_safe_tindex(x)))\n",
        "\n",
        "        # --- Nonlinear Constraints Setup ---\n",
        "\n",
        "        nonlinear_constraints = [\n",
        "            NonlinearConstraint(constraint_m1_gt_m2, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_m1_gt_m4, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_m2_lt_m1, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_m2_lt_m3, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_m3_gt_m2, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_m3_gt_m4, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_m4_lt_m1, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_m4_lt_m3, 0, np.inf),\n",
        "\n",
        "            NonlinearConstraint(constraint_feed_pump_upper, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_feed_pump_lower, 0, np.inf),\n",
        "\n",
        "            NonlinearConstraint(constraint_desorb_pump_upper, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_desorb_pump_lower, 0, np.inf),\n",
        "\n",
        "            NonlinearConstraint(constraint_raff_pump_upper, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_raff_pump_lower, 0, np.inf),\n",
        "\n",
        "            NonlinearConstraint(constraint_extract_pump_upper, 0, np.inf),\n",
        "            NonlinearConstraint(constraint_extract_pump_lower, 0, np.inf),\n",
        "\n",
        "            # Fixed feed constraint (Optional  can comment if not needed)\n",
        "            # NonlinearConstraint(constraint_fixed_feed, -0.001, 0.001)\n",
        "        ]\n",
        "\n",
        "        # ? Now you can pass:\n",
        "        # constraints=nonlinear_constraints\n",
        "        # into your differential_evolution call\n",
        "\n",
        "\n",
        "        # --- Run the optimization ---\n",
        "        print(f'Maxing ECI')\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "\n",
        "            result = differential_evolution(\n",
        "                func=log_expected_constrained_improvement, # probability_of_improvement(x, surrogate_gp, y_best, xi=0.005), expected_improvement(x, surrogate_gp, y_best) | log_expected_constrained_improvement(x, scalarized_surrogate_gp, [constraint_1_gp, constraint_2_gp], constraint_thresholds, y_best, xi)\n",
        "                bounds=bounds,\n",
        "                args=(scalarized_surrogate_gp, [constraint_1_gp, constraint_2_gp], constraint_thresholds, y_best, job_max_or_min, PF_weight, xi),\n",
        "                strategy='best1bin',\n",
        "                maxiter=200,\n",
        "                popsize=15,\n",
        "                disp=False,\n",
        "                 constraints=(nonlinear_constraints)\n",
        "                 )\n",
        "\n",
        "                # Perform the optimization using L-BFGS-B method\n",
        "        # result = minimize(\n",
        "        #     expected_improvement,\n",
        "        #     initial_guess,\n",
        "        #     args=(scalarized_surrogate_gp, y_best),\n",
        "        #     method='L-BFGS-B',\n",
        "        #     bounds=bounds,\n",
        "        #     options={'maxiter': 100, 'disp': True})\n",
        "\n",
        "        x_new = result.x # [m1, m2, m3, m4, t_index_min]\n",
        "        # print(f\"x_new: { x_new}\")\n",
        "        f_new, c_new, mj_and_t_new = obj_con(x_new)\n",
        "\n",
        "\n",
        "\n",
        "        # Add the new row to all_inputs\n",
        "        all_inputs = np.vstack((all_inputs, mj_and_t_new))\n",
        "\n",
        "        # Add to population\n",
        "        population_all.append(population)\n",
        "        population = np.vstack((population, x_new))\n",
        "\n",
        "        f1_vals = np.vstack([f1_vals.reshape(-1,1), f_new[0]])\n",
        "        f2_vals = np.vstack([f2_vals.reshape(-1,1), f_new[1]])\n",
        "        c1_vals  = np.vstack([c1_vals.reshape(-1,1), c_new[0]])\n",
        "        c2_vals  = np.vstack([c2_vals.reshape(-1,1), c_new[1]])\n",
        "\n",
        "        print(f\"Gen {gen+1} Status:\\n | Sampled Inputs:{x_new[:-1]}, {x_new[-1]*t_reff} [m1, m2, m3, m4, t_index]|\\n Outputs: f1: {f_new[0]*100}%, f2: {f_new[1]*100} % | GPur, FPur: {c_new[0]*100}%, {c_new[1]*100}%\")\n",
        "\n",
        "    return population_all, f1_vals, f2_vals, c1_vals , c2_vals , all_inputs\n",
        "\n",
        "def cusotom_isotherm_func(cusotom_isotherm_params, c):\n",
        "    \"\"\"\n",
        "    c => liquid concentration of ci\n",
        "    q_star => solid concentration of ci @ equilibrium\n",
        "    cusotom_isotherm_params[i] => given parameter set of component, i\n",
        "    \"\"\"\n",
        "\n",
        "    # Uncomment as necessary\n",
        "\n",
        "    #------------------- 1. Single Parameters Models\n",
        "    ## Linear\n",
        "    K1 = cusotom_isotherm_params[0]\n",
        "    H = K1 # Henry's Constant\n",
        "    q_star_1 = H*c\n",
        "\n",
        "    # #------------------- 2. Two-Parameter Models\n",
        "    # K1 = cusotom_isotherm_params[0]\n",
        "    # K2 = cusotom_isotherm_params[1]\n",
        "\n",
        "    # # #  Langmuir\n",
        "    # Q_max = K1\n",
        "    # b = K2\n",
        "    # #-------------------------------\n",
        "    # q_star_2 = Q_max*b*c/(1 + b*c)\n",
        "    # #-------------------------------\n",
        "\n",
        "    #------------------- 3. Three-Parameter models\n",
        "    # K1 = cusotom_isotherm_params[0]\n",
        "    # K2 = cusotom_isotherm_params[1]\n",
        "    # K3 = cusotom_isotherm_params[2]\n",
        "\n",
        "    # # Linear + Langmuir\n",
        "    # H = K1\n",
        "    # Q_max = K2\n",
        "    # b = K3\n",
        "    # ##-------------------------------\n",
        "    # q_star_3 = H*c + Q_max*b*c/(1 + b*c)\n",
        "    # ##-------------------------------\n",
        "\n",
        "    return q_star_1 # [qA, ...]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#%%\n",
        "# --------------- FUNCTION EVALUATION SECTION\n",
        "\n",
        "# SMB VARIABLES\n",
        "#######################################################\n",
        "# What tpye of isoherm is required?\n",
        "# Coupled: \"CUP\"\n",
        "# Uncoupled: \"UNC\"\n",
        "iso_type = \"UNC\"\n",
        "\n",
        "###################### PRIMARY INPUTS #########################\n",
        "# Define the names, colors, and parameter sets for 6 components\n",
        "Names = [\"Glucose\", \"Fructose\"]#, 'C', 'D']#, \"C\"]#, \"D\", \"E\", \"F\"]\n",
        "color = [\"g\", \"orange\"]#, \"purple\", \"brown\"]#, \"b\"]#, \"r\", \"purple\", \"brown\"]\n",
        "num_comp = len(Names) # Number of components\n",
        "e = 0.40         # bed voidage\n",
        "Bm = 300\n",
        "\n",
        "# Column Dimensions\n",
        "\n",
        "# How many columns in each Zone?\n",
        "\n",
        "Z1, Z2, Z3, Z4 = 1,3,3,1 # *3 for smb config\n",
        "zone_config = np.array([Z1, Z2, Z3, Z4])\n",
        "nnn = Z1 + Z2 + Z3 + Z4\n",
        "\n",
        "L = 70 # cm # Length of one column\n",
        "d_col = 8.66 # cm # column internal diameter\n",
        "\n",
        "# Calculate the radius\n",
        "r_col = d_col / 2\n",
        "# Calculate the area of the base\n",
        "A_col = np.pi * (r_col ** 2) # cm^2\n",
        "V_col = A_col*L # cm^3\n",
        "# Dimensions of the tubing and from each column:\n",
        "# Assuming the pipe diameter is 20% of the column diameter:\n",
        "d_in = 0.2 * d_col # cm\n",
        "nx_per_col = 15\n",
        "\n",
        "\n",
        "################ Time Specs #################################################################################\n",
        "t_index_min = 10 # min # Index time # How long the pulse holds before swtiching\n",
        "n_num_cycles = 12    # Number of Cycles you want the SMB to run for\n",
        "###############  FLOWRATES   #################################################################################\n",
        "\n",
        "# Jochen et al:\n",
        "Q_P, Q_Q, Q_R, Q_S = 5.21, 4, 5.67, 4.65 # x10-7 m^3/s\n",
        "conv_fac = 0.1 # x10-7 m^3/s => cm^3/s\n",
        "Q_P, Q_Q, Q_R, Q_S  = Q_P*conv_fac, Q_Q*conv_fac, Q_R*conv_fac, Q_S*conv_fac\n",
        "\n",
        "Q_I, Q_II, Q_III, Q_IV = Q_R,  Q_S, Q_P, Q_Q\n",
        "\n",
        "\n",
        "Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV])\n",
        "\n",
        "\n",
        "\n",
        "# # Parameter Sets for different components\n",
        "################################################################\n",
        "\n",
        "# Units:\n",
        "# - Concentrations: g/cm^3\n",
        "# - kh: 1/s\n",
        "# - Da: cm^2/s\n",
        "\n",
        "# A must have a less affinity to resin that B - FOUND IN EXtract purity\n",
        "# Parameter sets for different components\n",
        "# Units:\n",
        "# - Concentrations: g/cm^3\n",
        "# - kfp: 1/s\n",
        "parameter_sets = [\n",
        "    {\"kh\": 0.467256957, \"C_feed\": 0.42},    # Glucose SMB Launch\n",
        "    {\"kh\": 0.462, \"C_feed\": 0.42798}] #, # Fructose\n",
        "\n",
        "D_all = np.array([6.54255218e-6, 6.54255218e-6])\n",
        "\n",
        "# ISOTHERM PARAMETERS\n",
        "###########################################################################################\n",
        "# Uncomment as necessary:\n",
        "\n",
        "# Linear, H\n",
        "cusotom_isotherm_params_all = np.array([[3.2069715], [3.54]]) # H_glu, H_fru\n",
        "# Sub et al = np.array([[0.27], [0.53]])\n",
        "\n",
        "# # Langmuir, [Q_max, b]\n",
        "# cusotom_isotherm_params_all = np.array([[2.51181596, 1.95381598], [3.55314612, 1.65186647]])\n",
        "\n",
        "# Linear + Langmuir, [H, Q_max, b]\n",
        "# cusotom_isotherm_params_all = np.array([[1, 2.70420148, 1.82568197], [1, 3.4635919, 1.13858329]])\n"
      ],
      "metadata": {
        "id": "auDYWjDRWEP4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#%%\n",
        "# STORE/INITALIZE SMB VAIRABLES\n",
        "SMB_inputs = [iso_type, Names, color, num_comp, nx_per_col, e, D_all, Bm, zone_config, L, d_col, d_in, t_index_min, n_num_cycles, Q_internal, parameter_sets, cusotom_isotherm_func, cusotom_isotherm_params_all]\n",
        "\n",
        "# # ---------- SAMPLE RUN IF NECESSARY\n",
        "# start_test = time.time()\n",
        "# results = SMB(SMB_inputs)\n",
        "# # ref:  [y_matrices, nx, t, t_sets, t_schedule, C_feed, m_in, m_out, raff_cprofile, ext_cprofile, raff_intgral_purity, raff_recov, ext_intgral_purity, ext_recov, raff_vflow, ext_vflow, Model_Acc, Expected_Acc, Error_percent]\n",
        "# # STORE\n",
        "# Raffinate_Purity = results[10]\n",
        "# Raffinate_Recovery = results[11]\n",
        "# Extract_Purity = results[12]\n",
        "# Extract_Recovery = results[13]\n",
        "# Mass_Balance_Error_Percent = results[-1]\n",
        "# end_test = time.time()\n",
        "# test_duration = end_test-start_test\n",
        "# # DISPLAY\n",
        "# print(f'Time Taken for 1 SMB Run: {test_duration/60} min')\n",
        "# print(f'Raffinate_Recovery: {Raffinate_Recovery} ')\n",
        "# print(f'Extract_Recovery:  {Extract_Recovery}')\n",
        "# print(f'Raffinate_Purity: {Raffinate_Purity} ')\n",
        "# print(f'Extract_Purity: {Extract_Purity}')\n",
        "# print(f'Mass_Balance_Error_Percent: {Mass_Balance_Error_Percent}%')\n"
      ],
      "metadata": {
        "id": "DkYun1HvWIht"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# #%%\n",
        "\n",
        "# ----- MAIN ROUTINE\n",
        "if __name__ == \"__main__\":\n",
        "    # ------- OPTIMIZATION VARIABLES\n",
        "    # Do you want to maximize or minimize the objective function?\n",
        "    job_max_or_min = 'maximize'\n",
        "\n",
        "    # - - - - -\n",
        "    Q_fixed_feed = 4 # L/h # minimum (25%) flowrate on pump 6 (smallest pump)\n",
        "    Q_fixed_feed = Q_fixed_feed/3.6 # L/h --> cm^3/s\n",
        "    # - - - - -\n",
        "    t_reff = 10 # min\n",
        "    # - - - - -\n",
        "    Q_max = 40 # L/h\n",
        "    Q_min = 0.5 # L/h\n",
        "\n",
        "\n",
        "    Q_max = Q_max/3.6 # L/h --> cm^3/s\n",
        "    Q_min = Q_min/3.6 # L/h --> cm^3/s\n",
        "    # - - - - -\n",
        "    m_max = 4\n",
        "    m_min = 0.27\n",
        "    # - - - - -\n",
        "    sampling_budget = 1 #\n",
        "    optimization_budget = 5 # 100\n",
        "    constraint_threshold = [0.995, 0.995] # [Glu, Fru]\n",
        "    # - - - - -\n",
        "\n",
        "    # - - - - -\n",
        "    m_diff_max = Q_max/(V_col*(1-e))\n",
        "    m_diff_min = Q_min/(V_col*(1-e))\n",
        "    m_fixed_feed = Q_fixed_feed/(V_col*(1-e))\n",
        "    # - - - - -\n",
        "    PF_weight = 10 # Weight applied to the probability of feasibility\n",
        "    # - - - - -\n",
        "\n",
        "    bounds = [\n",
        "    (0.5, m_max), # m1\n",
        "    (0.4, m_max), # m2\n",
        "    (0.5, m_max), # m3\n",
        "    (0.2, m_max), # m4\n",
        "    (0.2, 1) # t_index/t_reff (normalized)\n",
        "    ]\n",
        "\n",
        "\n",
        "    initial_guess = 0 # min\n",
        "\n",
        "    print(f'Column Volume: {V_col} cm^3 | {V_col/1000} L')\n",
        "    print(f'Column CSA: {A_col} cm^2')\n",
        "    print(f'Column Length: {L} cm')\n",
        "    print(f'Column Diameter: {d_col} cm')\n",
        "    print(f'Optimization Budget: {optimization_budget}')\n",
        "    print(f'Sampling Budget: {sampling_budget}')\n",
        "    print(f\"bounds:\\nm1: ({bounds[0][0]}, {bounds[0][1]})\\nm2: ({bounds[1][0]}, {bounds[1][1]})\\nm3: ({bounds[2][0]}, {bounds[2][1]})\\nm4: ({bounds[3][0]}, {bounds[3][1]})\\nt_index: ({bounds[4][0]*t_reff}, {bounds[4][1]*t_reff}) min\")\n"
      ],
      "metadata": {
        "id": "pubPVytuWLWN",
        "outputId": "6550cb7a-7aa3-46d0-9765-f20322270df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Volume: 4123.098455202277 cm^3 | 4.123098455202277 L\n",
            "Column CSA: 58.90140650288967 cm^2\n",
            "Column Length: 70 cm\n",
            "Column Diameter: 8.66 cm\n",
            "Optimization Budget: 5\n",
            "Sampling Budget: 1\n",
            "bounds:\n",
            "m1: (0.5, 4)\n",
            "m2: (0.4, 4)\n",
            "m3: (0.5, 4)\n",
            "m4: (0.2, 4)\n",
            "t_index: (2.0, 10) min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#%%\n",
        "# generate iniital samples\n",
        "all_initial_inputs, all_initial_outputs = generate_initial_data(sampling_budget)\n",
        "print(f'all_initial_inputs\\n{ all_initial_inputs}')\n",
        "print(f'all_initial_outputs\\n{ all_initial_outputs}')\n"
      ],
      "metadata": {
        "id": "xOI6BWd_Wqja",
        "outputId": "adb88df2-bf26-42e6-993c-b8ff98a20a49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k:[0]\n",
            "[m1, m2, m3, m4]: [4.0, 3.54, 3.64, 2.0], t_index: 6.0\n",
            "Q_internal: [19.24112612 17.34450083 17.75681068 10.99492921] cm^s/s\n",
            "Q_internal: [69.26805405 62.44020301 63.92451845 39.58174517] L/h\n",
            "----------------------------------\n",
            "Q_external: [0.41230985 6.76188147 8.24619691 1.89662529] cm^s/s\n",
            "Q_external: [ 1.48431544 24.34277328 29.68630888  6.82785104] L/h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#%%\n",
        "# OPTIMIZATION\n",
        "population_all, f1_vals, f2_vals, c1_vals, c2_vals, all_inputs  = constrained_BO(optimization_budget, bounds, initial_guess, all_initial_inputs, all_initial_outputs, job_max_or_min, constraint_threshold, 0.001)\n",
        "\n",
        "#%%\n",
        "# ----------- SAVE\n",
        "# Convert NumPy array to list\n",
        "# Inputs:\n",
        "all_inputs_list = all_inputs.tolist()\n",
        "# Outputs:\n",
        "data_dict = {\n",
        "    \"f1_vals\": f1_vals.tolist(),\n",
        "    \"f2_vals\": f2_vals.tolist(),\n",
        "    \"c1_vals\": c1_vals.tolist(),\n",
        "    \"c2_vals\": c2_vals.tolist(),\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# SAVE all_inputs to JSON:\n",
        "with open(\"SIMS_20_iter_all_inputs.json\", \"w\") as f:\n",
        "    json.dump(all_inputs_list, f, indent=4)\n",
        "\n",
        "# SAVE recoveries_and_purities to JSON:\n",
        "with open(\"SIMS_20_iter_all_outputs.json\", \"w\") as f:\n",
        "    json.dump(data_dict, f, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "#     # -------- VISULAIZATIONS\n",
        "# def load_inputs_outputs(inputs_path, outputs_path):\n",
        "#     \"\"\"\n",
        "#     Loads all_inputs and output values (f1, f2, c1, c2) from saved JSON files and reconstructs them as numpy arrays.\n",
        "\n",
        "#     Args:\n",
        "#         inputs_path (str): Path to 'all_inputs.json'.\n",
        "#         outputs_path (str): Path to 'all_outputs.json'.\n",
        "\n",
        "#     Returns:\n",
        "#         all_inputs (np.ndarray): Loaded inputs array.\n",
        "#         f1_vals (np.ndarray): Glucose recovery values.\n",
        "#         f2_vals (np.ndarray): Fructose recovery values.\n",
        "#         c1_vals (np.ndarray): Glucose purity values.\n",
        "#         c2_vals (np.ndarray): Fructose purity values.\n",
        "#     \"\"\"\n",
        "#     # Load inputs\n",
        "#     with open(inputs_path, \"r\") as f:\n",
        "#         all_inputs_list = json.load(f)\n",
        "#     all_inputs = np.array(all_inputs_list)\n",
        "\n",
        "#     # Load outputs\n",
        "#     with open(outputs_path, \"r\") as f:\n",
        "#         data_dict = json.load(f)\n",
        "#     f1_vals = np.array(data_dict[\"f1_vals\"])\n",
        "#     f2_vals = np.array(data_dict[\"f2_vals\"])\n",
        "#     c1_vals = np.array(data_dict[\"c1_vals\"])\n",
        "#     c2_vals = np.array(data_dict[\"c2_vals\"])\n",
        "\n",
        "#     return all_inputs, f1_vals, f2_vals, c1_vals, c2_vals\n",
        "\n",
        "\n",
        "# #%%\n",
        "\n",
        "# #------------------------------------------------------- 1. Table\n",
        "\n",
        "# def create_output_optimization_table(f1_vals, f2_vals, c1_vals, c2_vals, sampling_budget):\n",
        "#     # Create a data table with recoveries first\n",
        "#     data = np.column_stack((f1_vals*100, f2_vals*100, c1_vals*100, c2_vals*100))\n",
        "#     columns = ['Recovery F1 (%)', 'Recovery F2 (%)', 'Purity C1 (%)', 'Purity C2 (%)']\n",
        "#     rows = [f'Iter {i+1}' for i in range(len(c1_vals))]\n",
        "\n",
        "#     # Identify \"star\" entries (where f1_vals, f2_vals > 70 and c1_vals, c2_vals > 90)\n",
        "#     star_indices = np.where((f1_vals*100 > 50) & (f2_vals*100 > 50) & (c1_vals*100 > 80) & (c2_vals*100 > 80))[0]\n",
        "\n",
        "#     # Create figure\n",
        "#     fig, ax = plt.subplots(figsize=(8, len(c1_vals) * 0.4))\n",
        "#     ax.set_title(\"Optimization Iterations: Recovery & Purity Table\", fontsize=12, fontweight='bold', pad=5)  # Reduced padding\n",
        "#     ax.axis('tight')\n",
        "#     ax.axis('off')\n",
        "\n",
        "#     # Create the table\n",
        "#     table = ax.table(cellText=data.round(2),\n",
        "#                      colLabels=columns,\n",
        "#                      rowLabels=rows,\n",
        "#                      cellLoc='center',\n",
        "#                      loc='center')\n",
        "\n",
        "#     # Adjust font size\n",
        "#     table.auto_set_font_size(False)\n",
        "#     table.set_fontsize(10)\n",
        "#     table.auto_set_column_width(col=list(range(len(columns))))\n",
        "\n",
        "#     # Apply colors\n",
        "#     for i in range(len(c1_vals)):\n",
        "#         for j in range(len(columns)):\n",
        "#             cell = table[(i+1, j)]  # (row, column) -> +1 because row labels shift index\n",
        "#             if i < sampling_budget:\n",
        "#                 cell.set_facecolor('lightgray')  # Grey out first 20 rows\n",
        "#             if i in star_indices:\n",
        "#                 cell.set_facecolor('yellow')  # Highlight star entries in yellow\n",
        "\n",
        "#     # Save the figure as an image\n",
        "#     image_filename = \"output_optimization_table.png\"\n",
        "#     fig.savefig(image_filename, dpi=300, bbox_inches='tight')\n",
        "#     plt.show()\n",
        "\n",
        "#     return image_filename\n",
        "\n",
        "\n",
        "\n",
        "# def calculate_flowrates(input_array, V_col, e):\n",
        "#     # Initialize the external flowrate array with the same shape as input_array\n",
        "#     internal_flowrate = np.zeros_like(input_array[:,:-1])\n",
        "#     external_flowrate = np.zeros_like(input_array)\n",
        "\n",
        "#     # Reshape the last column to be a 2D array for broadcasting\n",
        "#     input_last_col = input_array[:, -1]\n",
        "\n",
        "#     for i, t_index in enumerate(input_last_col):\n",
        "#         # Calculate the flow rates using the provided formula\n",
        "#         # Fill each row in external_flowrate:\n",
        "#         print(f't_index: {t_index}')\n",
        "#         internal_flowrate[i, :] = (input_array[i, :-1] * V_col * (1 - e) + V_col * e) / (t_index * 60)  # cm^3/s\n",
        "\n",
        "\n",
        "#     internal_flowrate = internal_flowrate*3.6 # cm^3/s => L/h\n",
        "#     print(f'internal_flowrate: {internal_flowrate}')\n",
        "#     # Calculate Internal FLowtates:\n",
        "#     Qfeed = internal_flowrate[:,2] - internal_flowrate[:,1] # Q_III - Q_II\n",
        "#     Qraffinate = internal_flowrate[:,2] - internal_flowrate[:,3] # Q_III - Q_IV\n",
        "#     Qdesorbent = internal_flowrate[:,0] - internal_flowrate[:,3] # Q_I - Q_IV\n",
        "#     Qextract = internal_flowrate[:,0] - internal_flowrate[:,1] # Q_I - Q_II\n",
        "\n",
        "#     external_flowrate[:,0] = Qfeed\n",
        "#     external_flowrate[:,1] = Qraffinate\n",
        "#     external_flowrate[:,2] = Qdesorbent\n",
        "#     external_flowrate[:,3] = Qextract\n",
        "#     external_flowrate[:,4] = input_last_col\n",
        "\n",
        "#     return internal_flowrate, external_flowrate\n",
        "\n",
        "# def create_input_optimization_table(input_array, V_col, e, sampling_budget, f1_vals, f2_vals, c1_vals, c2_vals):\n",
        "#     # Calculate flow rates\n",
        "#     internal_flowrate, external_flowrate = calculate_flowrates(input_array, V_col, e)\n",
        "#     flowrates = external_flowrate\n",
        "#     # Create a data table with flow rates\n",
        "#     data = external_flowrate\n",
        "#     columns = ['Feed (L/h)', 'Raffinate (L/h)', 'Desorbent (L/h)', 'Extract(L/h)', 'Index Time (min)']\n",
        "#     rows = [f'Iter {i+1}' for i in range(len(input_array))]\n",
        "\n",
        "#     # Identify \"star\" entries (example condition: flowrate > threshold)\n",
        "#     star_indices = np.where((f1_vals*100 > 50) & (f2_vals*100 > 50) & (c1_vals*100 > 80) & (c2_vals*100 > 80))[0]\n",
        "#     # Create figure\n",
        "#     fig, ax = plt.subplots(figsize=(8, len(input_array) * 0.4))\n",
        "#     ax.set_title(\"Optimization Iterations: Flowrate Table\", fontsize=12, fontweight='bold', pad=5)  # Reduced padding\n",
        "#     ax.axis('tight')\n",
        "#     ax.axis('off')\n",
        "\n",
        "#     # Create the table\n",
        "#     table = ax.table(cellText=data.round(3),\n",
        "#                      colLabels=columns,\n",
        "#                      rowLabels=rows,\n",
        "#                      cellLoc='center',\n",
        "#                      loc='center')\n",
        "\n",
        "#     # Adjust font size\n",
        "#     table.auto_set_font_size(False)\n",
        "#     table.set_fontsize(5)\n",
        "#     table.auto_set_column_width(col=list(range(len(columns))))\n",
        "\n",
        "#     # Apply colors\n",
        "#     for i in range(len(input_array)):\n",
        "#         for j in range(len(columns)):\n",
        "#             cell = table[(i+1, j)]  # (row, column) -> +1 because row labels shift index\n",
        "#             if i < sampling_budget:\n",
        "#                 cell.set_facecolor('lightgray')  # Grey out first sampling_budget rows\n",
        "#             if i in star_indices:\n",
        "#                 cell.set_facecolor('yellow')  # Highlight star entries in yellow\n",
        "\n",
        "#     # Save the figure as an image\n",
        "#     image_filename = \"input_optimization_table.png\"\n",
        "#     fig.savefig(image_filename, dpi=300, bbox_inches='tight')\n",
        "#     plt.show()\n",
        "\n",
        "#     return image_filename\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #------------------------------------------------------- 2. Recovery Pareto\n",
        "\n",
        "# def create_recovery_pareto_plot(f1_vals, f2_vals, zone_config, sampling_budget, optimization_budget):\n",
        "#     # Convert to percentages\n",
        "#     f1_vals_plot = f1_vals * 100\n",
        "#     f2_vals_plot = f2_vals * 100\n",
        "\n",
        "#     # Function to find Pareto front\n",
        "#     def find_pareto_front(f1, f2):\n",
        "#         pareto_mask = np.ones(len(f1), dtype=bool)  # Start with all points assumed Pareto-optimal\n",
        "\n",
        "#         for i in range(len(f1)):\n",
        "#             if pareto_mask[i]:  # Check only if not already removed\n",
        "#                 pareto_mask[i] = not np.any((f1 >= f1[i]) & (f2 >= f2[i]) & ((f1 > f1[i]) | (f2 > f2[i])))\n",
        "\n",
        "#         return pareto_mask\n",
        "\n",
        "#     # Identify Pareto-optimal points\n",
        "#     pareto_mask = find_pareto_front(f1_vals_plot, f2_vals_plot)\n",
        "\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "\n",
        "#     # Plot non-Pareto points in blue\n",
        "#     plt.scatter(f1_vals_plot[~pareto_mask], f2_vals_plot[~pareto_mask], c='blue', marker='o', label='Optimization Iterations')\n",
        "#     # Plot Pareto-optimal points in red\n",
        "#     plt.scatter(f1_vals_plot[pareto_mask], f2_vals_plot[pareto_mask], c='red', marker='o', label='Pareto Frontier')\n",
        "\n",
        "#     # Plot initial samples in grey\n",
        "#     # plt.scatter(f1_initial, f2_initial, c='grey', marker='o', label='Initial Samples')\n",
        "\n",
        "#     # Labels and formatting\n",
        "#     plt.title(f'Pareto Curve of Recoveries of Glu in Raff vs Fru in Ext\\n Config: {zone_config} \\nInitial Samples: {sampling_budget}, Opt Iterations: {optimization_budget}')\n",
        "#     plt.xlabel('Glucose Recovery in Raffinate (%)')\n",
        "#     plt.ylabel('Fructose Recovery in Extract (%)')\n",
        "#     plt.xlim(0, 100)\n",
        "#     plt.ylim(0, 100)\n",
        "#     plt.grid(True)\n",
        "#     plt.legend()\n",
        "\n",
        "#     # Save the figure as an image\n",
        "#     image_filename = \"recovery_pareto.png\"\n",
        "#     plt.savefig(image_filename, dpi=300, bbox_inches='tight')\n",
        "#     plt.show()\n",
        "\n",
        "#     return image_filename\n",
        "\n",
        "\n",
        "# #------------------------------------------------------- 2. Purity Pareto\n",
        "\n",
        "# def create_purity_pareto_plot(c1_vals, c2_vals, zone_config, sampling_budget, optimization_budget):\n",
        "#     # Convert to percentages\n",
        "#     c1_vals_plot = c1_vals * 100\n",
        "#     c2_vals_plot = c2_vals * 100\n",
        "\n",
        "#     # Function to find Pareto front\n",
        "#     def find_pareto_front(c1, c2):\n",
        "#         pareto_mask = np.ones(len(c1), dtype=bool)  # Start with all points assumed Pareto-optimal\n",
        "\n",
        "#         for i in range(len(c1)):\n",
        "#             if pareto_mask[i]:  # Check only if not already removed\n",
        "#                 pareto_mask[i] = not np.any((c1 >= c1[i]) & (c2 >= c2[i]) & ((c1 > c1[i]) | (c2 > c2[i])))\n",
        "\n",
        "#         return pareto_mask\n",
        "\n",
        "#     # Identify Pareto-optimal points\n",
        "#     pareto_mask = find_pareto_front(c1_vals_plot, c2_vals_plot)\n",
        "\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "\n",
        "#     # Plot non-Pareto points in blue\n",
        "#     plt.scatter(c1_vals_plot[~pareto_mask], c2_vals_plot[~pareto_mask], c='blue', marker='o', label='Optimization Iterations')\n",
        "#     # Plot Pareto-optimal points in red\n",
        "#     plt.scatter(c1_vals_plot[pareto_mask], c2_vals_plot[pareto_mask], c='red', marker='o', label='Pareto Frontier')\n",
        "\n",
        "#     # Plot initial samples in grey\n",
        "#     # plt.scatter(c1_initial, c2_initial, c='grey', marker='o', label='Initial Samples')\n",
        "\n",
        "#     # Labels and formatting\n",
        "#     plt.title(f'Pareto Curve of Purities of Glu in Raff vs Fru in Ext\\n Config: {zone_config} \\nInitial Samples: {sampling_budget}, Opt Iterations: {optimization_budget}')\n",
        "#     plt.xlabel('Glucose Purity in Raffinate (%)')\n",
        "#     plt.ylabel('Fructose Purity in Extract (%)')\n",
        "#     plt.xlim(0, 100)\n",
        "#     plt.ylim(0, 100)\n",
        "#     plt.grid(True)\n",
        "#     plt.legend()\n",
        "\n",
        "#     # Save the figure as an image\n",
        "#     image_filename = \"purity_pareto.png\"\n",
        "#     plt.savefig(image_filename, dpi=300, bbox_inches='tight')\n",
        "#     plt.show()\n",
        "\n",
        "#     return image_filename\n",
        "\n",
        "\n",
        "\n",
        "# #------------------------------------------------------- 4. Pareto Outputs Trace\n",
        "# def find_pareto_front(f1, f2):\n",
        "#     pareto_mask = np.ones(len(f1), dtype=bool)  # Start with all points assumed Pareto-optimal\n",
        "\n",
        "#     for i in range(len(f1)):\n",
        "#         if pareto_mask[i]:  # Check only if not already removed\n",
        "#             pareto_mask[i] = not np.any((f1 >= f1[i]) & (f2 >= f2[i]) & ((f1 > f1[i]) | (f2 > f2[i])))\n",
        "\n",
        "#     return pareto_mask\n",
        "\n",
        "# def plot_inputs_vs_iterations(input_array, f1_vals, f2_vals):\n",
        "#     input_names = ['Feed (L/h)', 'Raffinate (L/h)', 'Desorbent (L/h)', 'Extract (L/h)', 'Index Time (min)']\n",
        "#     # Convert to percentages\n",
        "#     f1_vals_plot = f1_vals * 100\n",
        "#     f2_vals_plot = f2_vals * 100\n",
        "\n",
        "#     # Identify Pareto-optimal points\n",
        "#     pareto_mask = find_pareto_front(f1_vals_plot, f2_vals_plot)\n",
        "\n",
        "#     # Filter input_array for Pareto-optimal points\n",
        "#     pareto_inputs = input_array[pareto_mask]\n",
        "\n",
        "#     # Plot inputs vs iterations for Pareto-optimal points\n",
        "#     iterations = np.arange(1, len(pareto_inputs) + 1)\n",
        "\n",
        "#     fig, ax1 = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "#     # Plot all inputs except the last one\n",
        "#     for i in range(pareto_inputs.shape[1] - 1):\n",
        "#         ax1.plot(iterations, pareto_inputs[:, i], marker='o', label=f'{input_names[i]}')\n",
        "\n",
        "#     ax1.set_xlabel('Iteration')\n",
        "#     ax1.set_ylabel('Flowrates (L/h)')\n",
        "#     ax1.grid(True)\n",
        "#     ax1.legend(loc='upper left')\n",
        "\n",
        "#     # Create a second y-axis for the indexing time\n",
        "#     ax2 = ax1.twinx()\n",
        "#     ax2.plot(iterations, pareto_inputs[:, -1], marker='o', color='grey', linestyle = \"--\", label=f'Input {input_names[-1]}')\n",
        "#     ax2.set_ylabel('Index Time (min)')\n",
        "#     ax2.legend(loc='upper right')\n",
        "\n",
        "#     plt.title('Operating Conditions at Pareto-Optimal Operating Points\\nInputs=[Flowrates, Indexing Time]')\n",
        "#     plt.show()\n",
        "\n",
        "# %%\n",
        "# Run the Functions and Visualise\n",
        "# --- Paretos\n",
        "# rec_pareto_image_filename = create_recovery_pareto_plot(f1_vals, f2_vals, zone_config, sampling_budget, optimization_budget)\n",
        "# pur_pareto_image_filename = create_purity_pareto_plot(c1_vals, c2_vals, zone_config, sampling_budget, optimization_budget)\n",
        "# plot_inputs_vs_iterations(all_inputs, f1_vals, f2_vals)\n",
        "\n",
        "# # ---- Tables\n",
        "# opt_table_for_outputs_image_filename = create_output_optimization_table(f1_vals, f2_vals, c1_vals, c2_vals, sampling_budget)\n",
        "# opt_table_for_inputs_image_filename = create_input_optimization_table(all_inputs, V_col, e, sampling_budget, f1_vals, f2_vals, c1_vals, c2_vals)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y7oHpMghWuE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}