{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCNwI9BtUaIU0dM90PGwfd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nawa-Punabantu/Opt_Algos/blob/main/Opt_with_triangle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.__version__)\n",
        "import scipy as sp\n",
        "print(sp.__version__)\n",
        "\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, Matern\n",
        "from scipy.optimize import differential_evolution\n",
        "from scipy.optimize import minimize, NonlinearConstraint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy import integrate\n",
        "import warnings\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwjkYaoiawYC",
        "outputId": "3b67e50d-5a01-4fc2-b9be-76935db04b43"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.2\n",
            "1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# smb model\n",
        "# # tips:\n",
        "    # - the Error: \"IndexError: index 10 is out of bounds for axis 0 with size 9\"\n",
        "    # may be due to a miss-match in size between the initial conditons and c, q in the ode func.\n",
        "# IMPORTING LIBRARIES\n",
        "###########################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import solve_ivp\n",
        "import matplotlib.pyplot as plt\n",
        "# Loading the Plotting Libraries\n",
        "from matplotlib.pyplot import subplots\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "# from PIL import Image\n",
        "from scipy import integrate\n",
        "# import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "###########################################\n",
        "# IMPORTING MY OWN FUNCTIONS\n",
        "###########################################\n",
        "# from post_pre_processing_funcs import\n",
        "\n",
        "# INPUTS\n",
        "#######################################################\n",
        "\n",
        "# UNITS:\n",
        "# All units must conform to:\n",
        "# Time - s\n",
        "# Lengths - cm^2\n",
        "# Volumes - cm^3\n",
        "# Masses - g\n",
        "# Concentrations - g\n",
        "# Volumetric flowrates - cm^3/s\n",
        "\n",
        "\n",
        "def SMB(SMB_inputs):\n",
        "    iso_type, Names, color, num_comp, nx_per_col, e, Pe_all, Bm, zone_config, L, d_col, d_in, t_index_min, n_num_cycles, Q_internal, parameter_sets = SMB_inputs[0:]\n",
        "\n",
        "    ###################### (CALCUALTED) SECONDARY INPUTS #########################\n",
        "\n",
        "    # Column Dimensions:\n",
        "    ################################################################\n",
        "    F = (1-e)/e     # Phase ratio\n",
        "    t=0\n",
        "    t_sets = 0\n",
        "    Ncol_num = np.sum(zone_config) # Total number of columns\n",
        "    L_total = L*Ncol_num # Total Lenght of all columns\n",
        "    A_col = np.pi*0.25*d_col**2 # cm^2\n",
        "    V_col = A_col * L # cm^3\n",
        "    V_col_total = Ncol_num * V_col # cm^3\n",
        "    A_in = np.pi * (d_in/2)**2 # cm^2\n",
        "    alpha = A_in / A_col\n",
        "\n",
        "\n",
        "\n",
        "    # Time Specs:\n",
        "    ################################################################\n",
        "\n",
        "    t_index = t_index_min*60 # s #\n",
        "\n",
        "    # Notes:\n",
        "    # - Cyclic Steady state typically happens only after 10 cycles (ref: https://doi.org/10.1205/026387603765444500)\n",
        "    # - The system is not currently designed to account for periods of no external flow\n",
        "\n",
        "    n_1_cycle = t_index * Ncol_num  # s How long a single cycle takes\n",
        "\n",
        "    total_cycle_time = n_1_cycle*n_num_cycles # s\n",
        "\n",
        "    tend = total_cycle_time # s # Final time point in ODE solver\n",
        "\n",
        "    tend_min = tend/60\n",
        "\n",
        "    t_span = (0, tend) # +dt)  # from t=0 to t=n\n",
        "\n",
        "    num_of_injections = int(np.round(tend/t_index)) # number of switching periods\n",
        "\n",
        "    # 't_start_inject_all' is a vecoter containing the times when port swithes occur for each port\n",
        "    # Rows --> Different Ports\n",
        "    # Cols --> Different time points\n",
        "    t_start_inject_all = [[] for _ in range(Ncol_num)]  # One list for each node (including the main list)\n",
        "\n",
        "    # Calculate start times for injections\n",
        "    for k in range(num_of_injections):\n",
        "        t_start_inject = k * t_index\n",
        "        t_start_inject_all[0].append(t_start_inject)  # Main list\n",
        "        for node in range(1, Ncol_num):\n",
        "            t_start_inject_all[node].append(t_start_inject + node * 0)  # all rows in t_start_inject_all are identical\n",
        "\n",
        "    t_schedule = t_start_inject_all[0]\n",
        "\n",
        "    # REQUIRED FUNCTIONS:\n",
        "    ################################################################\n",
        "\n",
        "    # 1.\n",
        "    # Func to Generate Indices for the columns\n",
        "    # def generate_repeated_numbers(n, m):\n",
        "    #     result = []\n",
        "    #     n = int(n)\n",
        "    #     m = int(m)\n",
        "    #     for i in range(m):\n",
        "    #         result.extend([i] * n)\n",
        "    #     return result\n",
        "\n",
        "    # 3.\n",
        "    # Func to divide the column into nodes\n",
        "\n",
        "    # DOES NOT INCLUDE THE C0 NODE (BY DEFAULT)\n",
        "    def set_x(L, Ncol_num,nx_col,dx):\n",
        "        if nx_col == None:\n",
        "            x = np.arange(0, L+dx, dx)\n",
        "            nnx = len(x)\n",
        "            nnx_col = int(np.round(nnx/Ncol_num))\n",
        "            nx_BC = Ncol_num - 1 # Number of Nodes (mixing points/boundary conditions) in between columns\n",
        "\n",
        "            # Indecies belonging to the mixing points between columns are stored in 'start'\n",
        "            # These can be thought of as the locations of the nx_BC nodes.\n",
        "            return x, dx, nnx_col,  nnx, nx_BC\n",
        "\n",
        "        elif dx == None:\n",
        "            nx = Ncol_num * nx_col\n",
        "            nx_BC = Ncol_num - 1 # Number of Nodes in between columns\n",
        "            x = np.linspace(0,L_total,nx)\n",
        "            ddx = x[1] - x[0]\n",
        "\n",
        "            # Indecies belonging to the mixing points between columns are stored in 'start'\n",
        "            # These can be thought of as the locations of the nx_BC nodes.\n",
        "\n",
        "            return x, ddx, nx_col, nx, nx_BC\n",
        "\n",
        "    # 4. A func that:\n",
        "    # (i) Calcualtes the internal flowrates given the external OR (ii) Visa-versa\n",
        "    def set_flowrate_values(set_Q_int, set_Q_ext, Q_rec):\n",
        "        if set_Q_ext is None and Q_rec is None:  # Chosen to specify internal/zone flowrates\n",
        "            Q_I = set_Q_int[0]\n",
        "            Q_II = set_Q_int[1]\n",
        "            Q_III = set_Q_int[2]\n",
        "            Q_IV = set_Q_int[3]\n",
        "\n",
        "            QX = -(Q_I - Q_II)\n",
        "            QF = Q_III - Q_II\n",
        "            QR = -(Q_III - Q_IV)\n",
        "            QD = -(QF + QX + QR) # OR: Q_I - Q_IV\n",
        "\n",
        "            Q_ext = np.array([QF, QR, QD, QX]) # cm^3/s\n",
        "\n",
        "            return Q_ext\n",
        "\n",
        "        elif set_Q_int is None and Q_rec is not None:  # Chosen to specify external flowrates\n",
        "            QF = set_Q_ext[0]\n",
        "            QR = set_Q_ext[1]\n",
        "            QD = set_Q_ext[2]\n",
        "            QX = set_Q_ext[3]\n",
        "\n",
        "            Q_I = Q_rec  # m^3/s\n",
        "            Q_III = (QX + QF) + Q_I\n",
        "            Q_IV = (QD - QX) + Q_I  # Fixed Q_II to Q_I as the variable was not defined yet\n",
        "            Q_II = (QR - QX) + Q_IV\n",
        "            Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV])\n",
        "            return Q_internal\n",
        "\n",
        "\n",
        "    # 5. Function to Build Port Schedules:\n",
        "\n",
        "    # This is done in two functions: (i) repeat_array and (ii) build_matrix_from_vector\n",
        "    # (i) repeat_array\n",
        "    # Summary: Creates the schedule for the 1st port, port 0, only. This is the port boadering Z2 & Z3 and always starts as a Feed port at t=0\n",
        "    # (i) build_matrix_from_vector\n",
        "    # Summary: Takes the output from \"repeat_array\" and creates schedules for all other ports.\n",
        "    # The \"trick\" is that the states of each of the, n, ports at t=0, is equal to the first, n, states of port 0.\n",
        "    # Once we know the states for each port at t=0, we form a loop that adds the next state.\n",
        "\n",
        "    # 5.1\n",
        "    def position_maker(schedule_quantity_name, F, R, D, X, Z_config):\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        Function that initializes the starting schedueles for a given quantitiy at all positions\n",
        "\n",
        "        F, R, D and X are the values of the quantiity at the respective feed ports\n",
        "\n",
        "        \"\"\"\n",
        "        # Initialize:\n",
        "        X_j = np.zeros(Ncol_num)\n",
        "\n",
        "\n",
        "        # We set each port in the appropriate position, depending on the nuber of col b/n Zones:\n",
        "        # By default, Position i = 0 (entrance to col,0) is reserved for the feed node.\n",
        "\n",
        "        # Initialize Positions:\n",
        "        # Q_position is a vector whose len is = number of mixing points (ports) b/n columns\n",
        "\n",
        "        X_j[0] = F        # FEED\n",
        "        X_j[Z_config[2]] = R     # RAFFINATE\n",
        "        X_j[Z_config[2] + Z_config[3]] = D    # DESORBENT\n",
        "        X_j[Z_config[2] + Z_config[3]+  Z_config[0]] = X   # EXTRACT\n",
        "\n",
        "        return X_j\n",
        "\n",
        "    # 5.2\n",
        "    def repeat_array(vector, start_time_num):\n",
        "        # vector = the states of all ports at t=0, vector[0] = is always the Feed port\n",
        "        # start_time_num = The number of times the state changes == num of port switches == num_injections\n",
        "        repeated_array = np.tile(vector, (start_time_num // len(vector) + 1))\n",
        "        return repeated_array[:start_time_num]\n",
        "\n",
        "    def initial_u_col(Zconfig, Qint):\n",
        "        \"\"\"\n",
        "        Fun that returns the the inital state at t=0 of the volumetric\n",
        "        flows in all the columns.\n",
        "\n",
        "        \"\"\"\n",
        "        # First row is for col0, which is the feed to zone 3\n",
        "        Zconfig_roll = np.roll(Zconfig, -2)\n",
        "        Qint_roll = np.roll(Qint, -2)\n",
        "\n",
        "        # print(Qint)\n",
        "        X = np.array([])\n",
        "\n",
        "        for i in range(len(Qint_roll)):\n",
        "            X_add = np.ones(Zconfig_roll[i])*Qint_roll[i]\n",
        "            # print('X_add:\\n', X_add)\n",
        "\n",
        "            X = np.append(X, X_add)\n",
        "        # X = np.concatenate(X)\n",
        "        # print('X:\\n', X)\n",
        "        return X\n",
        "\n",
        "\n",
        "    def build_matrix_from_vector(vector, t_schedule):\n",
        "        \"\"\"\n",
        "        Fun that returns the schedeule given the inital state at t=0\n",
        "        vector: inital state of given quantity at t=0 at all nodes\n",
        "        t_schedule: times at which port changes happen\n",
        "\n",
        "        \"\"\"\n",
        "        # vector = the states of all ports at t=0, vector[0] = is always the Feed port\n",
        "        start_time_num = int(len(t_schedule))\n",
        "        vector = np.array(vector)  # Convert the vector to a NumPy array\n",
        "        n = len(vector) # number of ports/columns\n",
        "\n",
        "        # Initialize the matrix for repeated elements, ''ALL''\n",
        "        ALL = np.zeros((n, start_time_num), dtype=vector.dtype)  # Shape is (n, start_time_num)\n",
        "\n",
        "        for i in range(start_time_num):\n",
        "            # print('i:',i)\n",
        "            ALL[:, i] = np.roll(vector, i)\n",
        "        return ALL\n",
        "\n",
        "\n",
        "\n",
        "    # # Uncomment as necessary depending on specification of either:\n",
        "    # # (1) Internal OR (2) External flowrates :\n",
        "    # # (1)\n",
        "    # Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV])\n",
        "    Q_external = set_flowrate_values(Q_internal, None, None) # Order: QF, QR, QD, QX\n",
        "    QF, QR, QD, QX = Q_external[0], Q_external[1], Q_external[2], Q_external[3]\n",
        "    # print('Q_external:', Q_external)\n",
        "\n",
        "    # (2)\n",
        "    # QX, QF, QR = -0.277, 0.315, -0.231  # cm^3/s\n",
        "    # QD = - (QF + QX + QR)\n",
        "    # Q_external = np.array([QF, QR, QD, QX])\n",
        "    # Q_rec = 33.69 # cm^3/s\n",
        "    # Q_internal = set_flowrate_values(None, Q_external, Q_rec) # Order: QF, QR, QD, Q\n",
        "\n",
        "    ################################################################################################\n",
        "\n",
        "\n",
        "    # Make concentration schedules for each component\n",
        "\n",
        "    Cj_pulse_all = [[] for _ in range(num_comp)]\n",
        "    for i in range(num_comp):\n",
        "        Cj_position = []\n",
        "        Cj_position = position_maker('Feed Conc Schedule:', parameter_sets[i]['C_feed'], 0, 0, 0, zone_config)\n",
        "        Cj_pulse = build_matrix_from_vector(Cj_position,  t_schedule)\n",
        "        Cj_pulse_all[i] = Cj_pulse\n",
        "\n",
        "\n",
        "    Q_position = position_maker('Vol Flow Schedule:', Q_external[0], Q_external[1], Q_external[2], Q_external[3], zone_config)\n",
        "    Q_pulse_all = build_matrix_from_vector(Q_position,  t_schedule)\n",
        "\n",
        "    # Spacial Discretization:\n",
        "    # Info:\n",
        "    # nx --> Total Number of Nodes (EXCLUDING mixing points b/n nodes)\n",
        "    # nx_col --> Number of nodes in 1 column\n",
        "    # nx_BC --> Number of mixing points b/n nodes\n",
        "    x, dx, nx_col, nx, nx_BC = set_x(L=L_total, Ncol_num = Ncol_num, nx_col = nx_per_col, dx = None)\n",
        "    start = [i*nx_col for i in range(0,Ncol_num)] # Locations of the BC indecies\n",
        "    u_col_at_t0 = initial_u_col(zone_config, Q_internal)\n",
        "    Q_col_all = build_matrix_from_vector(u_col_at_t0, t_schedule)\n",
        "\n",
        "\n",
        "    # DISPLAYING INPUT INFORMATION:\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('Number of Components:', num_comp)\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nTime Specs:\\n')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('Number of Cycles:', n_num_cycles)\n",
        "    # print('Time Per Cycle:', n_1_cycle/60, \"min\")\n",
        "    # print('Simulation Time:', tend_min, 'min')\n",
        "    # print('Index Time:', t_index, 's OR', t_index/60, 'min' )\n",
        "    # print('Number of Port Switches:', num_of_injections)\n",
        "    # print('Injections happen at t(s) = :', t_schedule, 'seconds')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nColumn Specs:\\n')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('Configuration:', zone_config, '[Z1,Z2,Z3,Z4]')\n",
        "    # print(f\"Number of Columns: {Ncol_num}\")\n",
        "    # print('Column Length:', L, 'cm')\n",
        "    # print('Column Diameter:', d_col, 'cm')\n",
        "    # print('Column Volume:', V_col, 'cm^3')\n",
        "    # print(\"alpha:\", alpha, '(alpha = A_in / A_col)')\n",
        "    # print(\"Nodes per Column:\",nx_col)\n",
        "    # print(\"Boundary Nodes locations,x[i], i =\", start)\n",
        "    # print(\"Total Number of Nodes (nx):\",nx)\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nFlowrate Specs:\\n')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print(\"External Flowrates =\", Q_external, '[F,R,D,X] ml/min')\n",
        "    # print(\"Ineternal Flowrates =\", Q_internal, 'ml/min')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nPort Schedules:')\n",
        "    # for i in range(num_comp):\n",
        "    #     print(f\"Concentration Schedule:\\nShape:\\n {Names[i]}:\\n\",np.shape(Cj_pulse_all[i]),'\\n', Cj_pulse_all[i], \"\\n\")\n",
        "    # print(\"Injection Flowrate Schedule:\\nShape:\",np.shape(Q_pulse_all),'\\n', Q_pulse_all, \"\\n\")\n",
        "    # print(\"Respective Column Flowrate Schedule:\\nShape:\",np.shape(Q_col_all),'\\n', Q_col_all, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    # ###########################################################################################\n",
        "\n",
        "    # Isotherm Models:\n",
        "\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "    # 1. LINEAR\n",
        "    def iso_lin(theta_lin, c):\n",
        "        # params: [HA, HB]\n",
        "        H = theta_lin\n",
        "        q_star = H*c\n",
        "\n",
        "        return q_star # [qA, qB, ...]\n",
        "\n",
        "    # 2.  LANGMUIR\n",
        "\n",
        "    # 2.1 Independent Langmuir\n",
        "    def iso_langmuir(theta_lang, c, comp_idx): # already for specific comp\n",
        "        H = theta_lang\n",
        "        q_star = H*c/(1 + H*c)\n",
        "        #q_star = H[comp_idx]*c/(1 + K[0]*c + K[1]*c)\n",
        "        # q_star = theta_lang[0]*c/(1 + theta_lang[1]*c + theta_lang[2]*c) +\\\n",
        "        #     theta_lang[3]*c/(1 + theta_lang[4]*c + theta_lang[5]*c)\n",
        "        return q_star\n",
        "\n",
        "    # 2.3 Coupled Langmuir\n",
        "    def iso_cup_langmuir(theta_cuplang, c, IDX, comp_idx): # already for specific comp\n",
        "        H = theta_cuplang[:2] # [HA, HB]\n",
        "        K = theta_cuplang[2:] # [KA, KB]\n",
        "        cA = c[IDX[0] + 0: IDX[0] + nx ]\n",
        "        cB = c[IDX[1] + 0: IDX[1] + nx ]\n",
        "        c_i = [cA, cB]\n",
        "        q_star = H[comp_idx]*c_i[comp_idx]/(1 + K[0]*cA + K[1]*cB)\n",
        "        return q_star\n",
        "\n",
        "    # 2.3 Bi-Langmuir\n",
        "    def iso_bi_langmuir(theta_bl, c, IDX, comp_idx): # already for specific comp\n",
        "        cA = c[IDX[0] + 0: IDX[0] + nx ]\n",
        "        cB = c[IDX[1] + 0: IDX[1] + nx ]\n",
        "        c_i = [cA, cB]\n",
        "\n",
        "        q_star = theta_bl[0]*c_i[comp_idx]/(1 + theta_bl[1]*cA + theta_bl[2]*cB) +\\\n",
        "                theta_bl[3]*c_i[comp_idx]/(1 + theta_bl[4]*cA + theta_bl[5]*cB)\n",
        "\n",
        "        return q_star\n",
        "\n",
        "\n",
        "    # 3. FREUDLICH:\n",
        "    def iso_freundlich(theta_fre, c): # already for specific comp\n",
        "        q_star = theta_fre[0]*c**(1/theta_fre[1])\n",
        "        return q_star\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "    # Mass Transfer (MT) Models:\n",
        "\n",
        "    def mass_transfer(kav_params, q_star, q): # already for specific comp\n",
        "        # kav_params: [kA, kB]\n",
        "        kav =  kav_params\n",
        "        MT = kav * Bm/(5 + Bm) * (q_star - q)\n",
        "        # MT = kav * (q_star - q)\n",
        "        return MT\n",
        "\n",
        "    # MT PARAMETERS\n",
        "    ###########################################################################################\n",
        "    # print('np.shape(parameter_sets[:][\"kh\"]):', np.shape(parameter_sets[3]))\n",
        "    kav_params = [parameter_sets[i][\"kh\"] for i in range(num_comp)]  # [kA, kB, kC, kD, kE, kF]\n",
        "    # print('kav_params:', kav_params)\n",
        "    # print('----------------------------------------------------------------')\n",
        "    ###########################################################################################\n",
        "\n",
        "    # # FORMING THE ODES\n",
        "\n",
        "\n",
        "    # Form the remaining schedule matrices that are to be searched by the funcs\n",
        "\n",
        "    # Column velocity schedule:\n",
        "    u_col_all = -Q_col_all/A_col/e\n",
        "\n",
        "    # Column Dispersion schedule:\n",
        "    # Different matrices for each comp because diff Pe's for each comp\n",
        "    D_col_all = []\n",
        "    for i in range(num_comp): # for each comp\n",
        "        D_col = -(u_col_all*L)/Pe_all[i] # constant dispersion coeff\n",
        "        D_col_all.append(D_col)\n",
        "\n",
        "    # Storage Spaces:\n",
        "    coef_0 = np.zeros_like(u_col_all)\n",
        "    coef_1 = np.zeros_like(u_col_all)\n",
        "    coef_2 = np.zeros_like(u_col_all)\n",
        "\n",
        "    # coef_0, coef_1, & coef_2 correspond to the coefficents of ci-1, ci & ci+1 respectively\n",
        "    # These depend on u and so change with time, thus have a schedule\n",
        "\n",
        "    # From descritization:\n",
        "    coef_0_all = []\n",
        "    coef_1_all = []\n",
        "    coef_2_all = []\n",
        "    for j in range(num_comp): # for each comp\n",
        "        for i  in range(Ncol_num): # coefficients for each col\n",
        "            coef_0[i,:] = ( D_col_all[j][i,:]/dx**2 ) - ( u_col_all[i,:]/dx ) # coefficeint of i-1\n",
        "            coef_1[i,:] = ( u_col_all[i,:]/dx ) - (2*D_col_all[j][i,:]/(dx**2))# coefficeint of i\n",
        "            coef_2[i,:] = (D_col_all[j][i,:]/(dx**2))    # coefficeint of i+1\n",
        "        coef_0_all.append(coef_0)\n",
        "        coef_1_all.append(coef_1)\n",
        "        coef_2_all.append(coef_2)\n",
        "\n",
        "    # All shedules:\n",
        "    # For each shceudle, rows => col idx, columns => Time idx\n",
        "    # :\n",
        "    # - Q_pulse_all: Injection flowrates\n",
        "    # - C_pulse_all: Injection concentrations for each component\n",
        "    # - Q_col_all:  Flowrates WITHIN each col\n",
        "    # - u_col_all: Linear velocities WITHIN each col\n",
        "    # - D_col_all: Dispersion coefficeints WITHIN each col\n",
        "    # - coef_0, 1 and 2: ci, ci-1 & ci+1 ceofficients\n",
        "\n",
        "    # print('coef_0:\\n',coef_0)\n",
        "    # print('coef_1:\\n',coef_1)\n",
        "    # print('coef_2:\\n',coef_2)\n",
        "    # print('\\nD_col_all:\\n',D_col_all)\n",
        "    # print('Q_col_all:\\n',Q_col_all)\n",
        "    # print('A_col:\\n',A_col)\n",
        "    # print('u_col_all:\\n',u_col_all)\n",
        "\n",
        "\n",
        "    def coeff_matrix_builder_UNC(t, Q_col_all, Q_pulse_all, dx, start, alpha, c, nx_col, comp_idx): # note that c_length must include nx_BC\n",
        "\n",
        "        # Define the functions that call the appropriate schedule matrices:\n",
        "        # Because all scheudels are of the same from, only one function is required\n",
        "        # Calling volumetric flows:\n",
        "        get_X = lambda t, X_schedule, col_idx: next((X_schedule[col_idx][j] for j in range(len(X_schedule[col_idx])) if t_start_inject_all[col_idx][j] <= t < t_start_inject_all[col_idx][j] + t_index), 1/100000000)\n",
        "        get_C = lambda t, C_schedule, col_idx, comp_idx: next((C_schedule[comp_idx][col_idx][j] for j in range(len(C_schedule[comp_idx][col_idx])) if t_start_inject_all[col_idx][j] <= t < t_start_inject_all[col_idx][j] + t_index), 1/100000000)\n",
        "\n",
        "        def small_col_matix(nx_col, col_idx):\n",
        "        # Initialize small_col_coeff ('small' = for 1 col)\n",
        "\n",
        "            small_col_coeff = np.zeros((int(nx_col),int(nx_col))) #(5,5)\n",
        "\n",
        "            # Where the 1st (0th) row and col are for c1\n",
        "            # get_C(t, coef_0_all, k, comp_idx)\n",
        "            # small_col_coeff[0,0], small_col_coeff[0,1] = get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "            small_col_coeff[0,0], small_col_coeff[0,1] = get_C(t,coef_1_all,col_idx, comp_idx), get_C(t,coef_2_all,col_idx, comp_idx)\n",
        "            # for c2:\n",
        "            # small_col_coeff[1,0], small_col_coeff[1,1], small_col_coeff[1,2] = get_X(t,coef_0,col_idx), get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "            small_col_coeff[1,0], small_col_coeff[1,1], small_col_coeff[1,2] = get_C(t,coef_0_all,col_idx, comp_idx), get_C(t, coef_1_all, col_idx, comp_idx), get_C(t, coef_2_all,col_idx, comp_idx)\n",
        "\n",
        "            for i in range(2,nx_col): # from row i=2 onwards\n",
        "                # np.roll the row entries from the previous row, for all the next rows\n",
        "                new_row = np.roll(small_col_coeff[i-1,:],1)\n",
        "                small_col_coeff[i:] = new_row\n",
        "\n",
        "            small_col_coeff[-1,0] = 0\n",
        "            small_col_coeff[-1,-1] = small_col_coeff[-1,-1] +  get_C(t,coef_2_all,col_idx, comp_idx) # coef_1 + coef_2 account for rolling boundary\n",
        "\n",
        "\n",
        "            return small_col_coeff\n",
        "\n",
        "\n",
        "        larger_coeff_matrix = np.zeros((nx,nx)) # ''large'' = for all cols # (20, 20)\n",
        "\n",
        "        # Add the cols\n",
        "        for col_idx in range(Ncol_num):\n",
        "            larger_coeff_matrix[col_idx*nx_col:(col_idx+1)*nx_col, col_idx*nx_col:(col_idx+1)*nx_col] = small_col_matix(nx_col,col_idx)\n",
        "        # print('np.shape(larger_coeff_matrix)\\n',np.shape(larger_coeff_matrix))\n",
        "\n",
        "        # vector_add: vector that applies the boundary conditions to each boundary node\n",
        "        def vector_add(nx, c, start, comp_idx):\n",
        "            vec_add = np.zeros(nx)\n",
        "            c_BC = np.zeros(Ncol_num)\n",
        "            # Indeceis for the boundary nodes are stored in \"start\"\n",
        "            # Each boundary node is affected by the form:\n",
        "            # c_BC = V1 * C_IN - V2 * c[i] + V3 * c[i+1]\n",
        "\n",
        "            # R1 = ((beta * alpha) / gamma)\n",
        "            # R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "            # R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "\n",
        "            # Where:\n",
        "            # C_IN is the weighted conc exiting the port facing the column entrance.\n",
        "            # alpha , bata and gamma depend on the column vecolity and are thus time dependant\n",
        "            # Instead of forming schedules for alpha , bata and gamma, we calculate them in-line\n",
        "\n",
        "            for i in range(len(start)):\n",
        "                #  start[i] => the node at the entrance to the ith col\n",
        "                # So start[3] is the node representing the 1st node in col 3\n",
        "\n",
        "                Q_1 = get_X(t, Q_col_all, i-1) # Vol_flow from previous column (which for column 0, is the last column in the chain)\n",
        "                Q_2 = get_X(t, Q_pulse_all, i) # Vol_flow injected IN port i\n",
        "\n",
        "                Q_out_port = get_X(t, Q_col_all, i) # Vol_flow OUT of port 0 (Also could have used Q_1 + Q_2)\n",
        "\n",
        "\n",
        "                W1 = Q_1/Q_out_port # Weighted flowrate to column i\n",
        "                W2 = Q_2/Q_out_port # Weighted flowrate to column i\n",
        "\n",
        "                # Calcualte Weighted Concentration:\n",
        "\n",
        "                c_injection = get_C(t, Cj_pulse_all, i, comp_idx)\n",
        "\n",
        "                if Q_2 > 0: # Concentration in the next column is only affected for injection flows IN\n",
        "                    C_IN = W1 * c[i*nx_col-1] + W2 * c_injection\n",
        "                else:\n",
        "                    # C_IN = c[i*nx_col-1] # no change in conc during product collection\n",
        "                    C_IN = c[start[i]-1] # no change in conc during product collection\n",
        "\n",
        "                # Calcualte alpha, bata and gamma:\n",
        "                # Da = get_X(t, D_col_all, i)\n",
        "                Da = get_C(t, D_col_all, i, comp_idx)\n",
        "                u =  get_X(t, u_col_all, i)\n",
        "                beta = 1 / alpha\n",
        "                gamma = 1 - 3 * Da / (2 * u * dx)\n",
        "\n",
        "                ##\n",
        "                R1 = ((beta * alpha) / gamma)\n",
        "                R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "                R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "                ##\n",
        "\n",
        "                # Calcualte the BC effects:\n",
        "                j = start[i]\n",
        "                c_BC[i] = R1 * C_IN - R2 * c[j] + R3 * c[j+1] # the boundary concentration for that node\n",
        "\n",
        "            # print('c_BC:\\n', c_BC)\n",
        "\n",
        "            for k in range(len(c_BC)):\n",
        "                # vec_add[start[k]]  = get_X(t,coef_0,k)*c_BC[k]\n",
        "                vec_add[start[k]]  = get_C(t, coef_0_all, k, comp_idx)*c_BC[k]\n",
        "\n",
        "            return vec_add\n",
        "            # print('np.shape(vect_add)\\n',np.shape(vec_add(nx, c, start)))\n",
        "        return larger_coeff_matrix, vector_add(nx, c, start, comp_idx)\n",
        "\n",
        "    def coeff_matrix_builder_CUP(t, Q_col_all, Q_pulse_all, dx, start_CUP, alpha, c, nx_col,IDX): # note that c_length must include nx_BC\n",
        "\n",
        "        # Define the functions that call the appropriate schedule matrices:\n",
        "        # Because all scheudels are of the same from, only one function is required\n",
        "        # Calling volumetric flows:\n",
        "        get_X = lambda t, X_schedule, col_idx: next((X_schedule[col_idx][j] for j in range(len(X_schedule[col_idx])) if t_start_inject_all[col_idx][j] <= t < t_start_inject_all[col_idx][j] + t_index), 1/100000000)\n",
        "\n",
        "\n",
        "        # 1. From coefficent \"small\" matrix for movement of single comp through single col\n",
        "        # 2. Form  \"large\" coefficent matrix for movement through one all cols\n",
        "        # 3. The large  coefficent matrix for each comp will then be combined into Final Matrix\n",
        "\n",
        "        # 1.\n",
        "        def small_col_matrix(nx_col, col_idx):\n",
        "\n",
        "        # Initialize small_col_coeff ('small' = for 1 col)\n",
        "\n",
        "            small_col_coeff = np.zeros((int(nx_col),int(nx_col))) #(5,5)\n",
        "\n",
        "            # Where the 1st (0th) row and col are for c1\n",
        "            #\n",
        "            small_col_coeff[0,0], small_col_coeff[0,1] = get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "            # for c2:\n",
        "            small_col_coeff[1,0], small_col_coeff[1,1], small_col_coeff[1,2] = get_X(t,coef_0,col_idx), get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "\n",
        "            for i in range(2,nx_col): # from row i=2 onwards\n",
        "                # np.roll the row entries from the previous row, for all the next rows\n",
        "                new_row = np.roll(small_col_coeff[i-1,:],1)\n",
        "                small_col_coeff[i:] = new_row\n",
        "\n",
        "            small_col_coeff[-1,0] = 0\n",
        "            small_col_coeff[-1,-1] = small_col_coeff[-1,-1] +  get_X(t,coef_2,col_idx) # coef_1 + coef_2 account for rolling boundary\n",
        "\n",
        "            return small_col_coeff\n",
        "\n",
        "        # 2. Func to Build Large Matrix\n",
        "\n",
        "        def matrix_builder(M, M0):\n",
        "            # M = Matrix to add (small)\n",
        "            # M0 = Initial state of the larger matrix to be added to\n",
        "            nx_col = M.shape[0]\n",
        "            repeat = int(np.round(M0.shape[0]/M.shape[0]))# numbner of times the small is added to the larger matrix\n",
        "            for col_idx in range(repeat):\n",
        "                        M0[col_idx*nx_col:(col_idx+1)*nx_col, col_idx*nx_col:(col_idx+1)*nx_col] = M\n",
        "            return M0\n",
        "\n",
        "\n",
        "        # 3. Generate and Store the Large Matrices\n",
        "        # Storage Space:\n",
        "        # NOTE: Assuming all components have the same Dispersion coefficients,\n",
        "        # all components will have the same large_col_matrix\n",
        "        # Add the cols\n",
        "        larger_coeff_matrix = np.zeros((nx,nx)) # ''large'' = for all cols # (20, 20)\n",
        "\n",
        "        for col_idx in range(Ncol_num):\n",
        "            larger_coeff_matrix[col_idx*nx_col:(col_idx+1)*nx_col, col_idx*nx_col:(col_idx+1)*nx_col] = small_col_matrix(nx_col,col_idx)\n",
        "\n",
        "        # print('np.shape(larger_coeff_matrix)\\n',np.shape(larger_coeff_matrix))\n",
        "\n",
        "        # Inital final matrix:\n",
        "        n = nx*num_comp\n",
        "        final_matrix0 = np.zeros((n,n))\n",
        "\n",
        "\n",
        "        final_matrix = matrix_builder(larger_coeff_matrix, final_matrix0)\n",
        "\n",
        "            # vector_add: vector that applies the boundary conditions to each boundary node\n",
        "        def vector_add(nx, c, start):\n",
        "            vec_add = np.zeros(nx*num_comp)\n",
        "            c_BC = np.zeros(nx*num_comp)\n",
        "            # Indeceis for the boundary nodes are stored in \"start\"\n",
        "            # Each boundary node is affected by the form:\n",
        "            # c_BC = V1 * C_IN - V2 * c[i] + V3 * c[i+1]\n",
        "\n",
        "            # R1 = ((beta * alpha) / gamma)\n",
        "            # R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "            # R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "\n",
        "            # Where:\n",
        "            # C_IN is the weighted conc exiting the port facing the column entrance.\n",
        "            # alpha , bata and gamma depend on the column vecolity and are thus time dependant\n",
        "            # Instead of forming schedules for alpha , bata and gamma, we calculate them in-line\n",
        "\n",
        "            for i in range(len(start)):\n",
        "                #k = i%len(start) # Recounts columns for B\n",
        "                Q_1 = get_X(t, Q_col_all, i-1) # Vol_flow from previous column (which for column 0, is the last column in the chain)\n",
        "                Q_2 = get_X(t, Q_pulse_all, i) # Vol_flow injected IN port i\n",
        "\n",
        "                Q_out_port = get_X(t, Q_col_all, i) # Vol_flow OUT of port 0 (Also could have used Q_1 + Q_2)\n",
        "\n",
        "\n",
        "                W1 = Q_1/Q_out_port # Weighted flowrate to column i\n",
        "                W2 = Q_2/Q_out_port # Weighted flowrate to column i\n",
        "\n",
        "                # Calcualte Weighted Concentration:\n",
        "                # Identifiers:\n",
        "                A = IDX[0]\n",
        "                B = IDX[1]\n",
        "\n",
        "                # C_IN_A = W1 * c[A + i*nx_col-1] + W2 * get_X(t, C_pulse_all_A, i) # c[-1] conc out the last col\n",
        "                # C_IN_B = W1 * c[B + i*nx_col-1] + W2 * get_X(t, C_pulse_all_B, i) # c[-1] conc out the last col\n",
        "\n",
        "                C_IN_A = W1 * c[A + i*nx_col-1] + W2 * get_X(t, Cj_pulse_all[0], i) # c[-1] conc out the last col\n",
        "                C_IN_B = W1 * c[B + i*nx_col-1] + W2 * get_X(t, Cj_pulse_all[1], i) # c[-1] conc out the last col\n",
        "\n",
        "\n",
        "                # Calcualte alpha, bata and gamma:\n",
        "                Da = get_X(t, D_col_all, i)\n",
        "                u =  get_X(t, u_col_all, i)\n",
        "                beta = 1 / alpha\n",
        "                gamma = 1 - 3 * Da / (2 * u * dx)\n",
        "\n",
        "                ##\n",
        "                R1 = ((beta * alpha) / gamma)\n",
        "                R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "                R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "                ##\n",
        "\n",
        "                # Calcualte the BC effects:\n",
        "                j = start[i]\n",
        "                # print('j:', j)\n",
        "                c_BC[i] = R1 * C_IN_A - R2 * c[j] + R3 * c[j+1] # the boundary concentration for that node\n",
        "                c_BC[B + i] = R1 * C_IN_B - R2 * c[B+j] + R3 * c[B+j+1]\n",
        "            # print('c_BC:\\n', c_BC)\n",
        "            # print('c_BC.shape:\\n', c_BC.shape)\n",
        "\n",
        "            for k in range(len(start)):\n",
        "                vec_add[start[k]]  = get_X(t,coef_0,k)*c_BC[k]\n",
        "                vec_add[B + start[k]]  = get_X(t,coef_0,k)*c_BC[B+ k]\n",
        "\n",
        "            return vec_add\n",
        "            # print('np.shape(vect_add)\\n',np.shape(vec_add(nx, c, start)))\n",
        "        return final_matrix, vector_add(nx, c, start_CUP)\n",
        "\n",
        "    # ###########################################################################################\n",
        "\n",
        "    # # mod1: UNCOUPLED ISOTHERM:\n",
        "    # # Profiles for each component can be solved independently\n",
        "\n",
        "    # ###########################################################################################\n",
        "    def mod1(t, v, comp_idx, Q_pulse_all):\n",
        "        # call.append(\"call\")\n",
        "        # print(len(call))\n",
        "        c = v[:nx]\n",
        "        q = v[nx:]\n",
        "\n",
        "        # Initialize the derivatives\n",
        "        dc_dt = np.zeros_like(c)\n",
        "        dq_dt = np.zeros_like(q)\n",
        "        # print('v size\\n',np.shape(v))\n",
        "\n",
        "        # Isotherm:\n",
        "        #########################################################################\n",
        "        isotherm = iso_lin(theta_lin[comp_idx], c)\n",
        "        #isotherm = iso_langmuir(theta_lang[comp_idx], c, comp_idx)\n",
        "        #isotherm = iso_freundlich(theta_fre, c)\n",
        "\n",
        "\n",
        "        # Mass Transfer:\n",
        "        #########################################################################\n",
        "        # print('isotherm size\\n',np.shape(isotherm))\n",
        "        MT = mass_transfer(kav_params[comp_idx], isotherm, q)\n",
        "        #print('MT:\\n', MT)\n",
        "\n",
        "        coeff_matrix, vec_add = coeff_matrix_builder_UNC(t, Q_col_all, Q_pulse_all, dx, start, alpha, c, nx_col, comp_idx)\n",
        "        # print('coeff_matrix:\\n',coeff_matrix)\n",
        "        # print('vec_add:\\n',vec_add)\n",
        "        dc_dt = coeff_matrix @ c + vec_add - F * MT\n",
        "        dq_dt = MT\n",
        "\n",
        "        return np.concatenate([dc_dt, dq_dt])\n",
        "\n",
        "    # ##################################################################################\n",
        "\n",
        "    def mod2(t, v):\n",
        "\n",
        "        # where, v = [c, q]\n",
        "        c = v[:num_comp*nx] # c = [cA, cB] | cA = c[:nx], cB = c[nx:]\n",
        "        q = v[num_comp*nx:] # q = [qA, qB]| qA = q[:nx], qB = q[nx:]\n",
        "\n",
        "        # Craate Lables so that we know the component assignement in the c vecotor:\n",
        "        A, B = 0*nx, 1*nx # Assume Binary 2*nx, 3*nx, 4*nx, 5*nx\n",
        "        IDX = [A, B]\n",
        "\n",
        "        # Thus to refer to the liquid concentration of the i = nth row of component B: c[C + n]\n",
        "        # Or the the solid concentration 10th row of component B: q[B + 10]\n",
        "        # Or to refer to all A's OR B's liquid concentrations: c[A + 0: A + nx] OR c[B + 0: B + nx]\n",
        "\n",
        "\n",
        "        # Initialize the derivatives\n",
        "        dc_dt = np.zeros_like(c)\n",
        "        dq_dt = np.zeros_like(q)\n",
        "\n",
        "\n",
        "        coeff_matrix, vec_add = coeff_matrix_builder_CUP(t, Q_col_all, Q_pulse_all, dx, start, alpha, c, nx_col, IDX)\n",
        "        # print('coeff_matrix:\\n',coeff_matrix)\n",
        "        # print('vec_add:\\n',vec_add)\n",
        "\n",
        "\n",
        "\n",
        "        ####################### Building MT Terms ####################################################################\n",
        "\n",
        "        # Initialize\n",
        "\n",
        "        MT = np.zeros(len(c)) # column vector: MT kinetcis for each comp: MT = [MT_A MT_B]\n",
        "\n",
        "        for comp_idx in range(num_comp): # for each component\n",
        "\n",
        "            ######################(i) Isotherm ####################################################################\n",
        "\n",
        "            # Comment as necessary for required isotherm:\n",
        "            # isotherm = iso_bi_langmuir(theta_blang[comp_idx], c, IDX, comp_idx)\n",
        "            isotherm = iso_cup_langmuir(theta_cup_lang, c, IDX, comp_idx)\n",
        "            # print('qstar:\\n', isotherm.shape)\n",
        "            ################### (ii) MT ##########################################################\n",
        "            MT_comp = mass_transfer(kav_params[comp_idx], isotherm, q[IDX[comp_idx] + 0: IDX[comp_idx] + nx ])\n",
        "            MT[IDX[comp_idx] + 0: IDX[comp_idx] + nx ] = MT_comp\n",
        "            # [MT_A, MT_B, . . . ] KINETICS FOR EACH COMP\n",
        "\n",
        "\n",
        "\n",
        "        dc_dt = coeff_matrix @ c + vec_add - F * MT\n",
        "        dq_dt = MT\n",
        "\n",
        "        return np.concatenate([dc_dt, dq_dt])\n",
        "\n",
        "    # ##################################################################################\n",
        "\n",
        "    # SOLVING THE ODES\n",
        "    # creat storage spaces:\n",
        "    y_matrices = []\n",
        "\n",
        "    t_sets = []\n",
        "    t_lengths = []\n",
        "\n",
        "    c_IN_values_all = []\n",
        "    F_in_values_all = []\n",
        "    call = []\n",
        "\n",
        "    # print('----------------------------------------------------------------')\n",
        "    # print(\"\\n\\nSolving the ODEs. . . .\")\n",
        "\n",
        "\n",
        "\n",
        "    if iso_type == \"UNC\": # UNCOUPLED - solve 1 comp at a time\n",
        "        for comp_idx in range(num_comp): # for each component\n",
        "            # print(f'Solving comp {comp_idx}. . . .')\n",
        "            # print('\\nSolution Size:')\n",
        "            v0 = np.zeros(Ncol_num* (nx_col + nx_col)) #  for both c and q\n",
        "            solution = solve_ivp(mod1, t_span, v0, args=(comp_idx , Q_pulse_all))\n",
        "            y_solution, t = solution.y, solution.t\n",
        "            y_matrices.append(y_solution)\n",
        "            t_sets.append(t)\n",
        "            t_lengths.append(len(t))\n",
        "            # print(f'y_matrices[{i}]', y_matrices[i].shape)\n",
        "\n",
        "\n",
        "    # Assuming only a binary coupled system\n",
        "    if iso_type == \"CUP\": # COUPLED - solve\n",
        "            # nx = nx_col*num_comp\n",
        "            v0 = np.zeros(num_comp*(nx)*2) # for c and 2, for each comp\n",
        "            solution = solve_ivp(mod2, t_span, v0)\n",
        "            y_solution, t = solution.y, solution.t\n",
        "            # Convert y_solution from: [cA, cB, qA, qB] ,  TO: [[cA, qA ], [cB, qB]]\n",
        "            # Write a function to do that\n",
        "\n",
        "            def reshape_ysol(x, nx, num_comp):\n",
        "                # Initialize a list to store the reshaped components\n",
        "                reshaped_list = []\n",
        "\n",
        "                # Iterate over the number of components\n",
        "                for i in range(num_comp):\n",
        "                    # Extract cX and qX submatrices for the i-th component\n",
        "                    cX = x[i*nx:(i+1)*nx, :]      # Extract cX submatrix\n",
        "                    qX = x[i*nx + num_comp*nx : (i+1)*nx + num_comp*nx, :]       # Extract qX submatrix\n",
        "                    concat = np.concatenate([cX, qX])\n",
        "                    # print('i:', i)\n",
        "                    # print('cX:\\n',cX)\n",
        "                    # print('qX:\\n',qX)\n",
        "                    # Append the reshaped pair [cX, qX] to the list\n",
        "                    reshaped_list.append(concat)\n",
        "\n",
        "                # Convert the list to a NumPy array\n",
        "                result = np.array(reshaped_list)\n",
        "\n",
        "                return result\n",
        "\n",
        "            y_matrices = reshape_ysol(y_solution, nx, num_comp)\n",
        "            # print('len(t_sets) = ', len(t_sets[0]))\n",
        "            # print('len(t) = ', len(t))\n",
        "\n",
        "    # print('----------------------------------------------------------------')\n",
        "    # print('\\nSolution Size:')\n",
        "    # for i in range(num_comp):\n",
        "    #     print(f'y_matrices[{i}]', y_matrices[i].shape)\n",
        "    # print('----------------------------------------------------------------')\n",
        "    # print('----------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ###########################################################################################\n",
        "\n",
        "    # VISUALIZATION\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # MASS BALANCE AND PURITY CURVES\n",
        "    ###########################################################################################\n",
        "\n",
        "    def find_indices(t_ode_times, t_schedule):\n",
        "        \"\"\"\n",
        "        t_schedule -> vector of times when (events) port switches happen e.g. at [0,5,10] seconds\n",
        "        t_ode_times -> vector of times from ODE\n",
        "\n",
        "        We want to know where in t_ode_times, t_schedule occures\n",
        "        These iwll be stored as indecies in t_idx\n",
        "        Returns:np.ndarray: An array of indices in t_ode_times corresponding to each value in t_schedule.\n",
        "        \"\"\"\n",
        "        t_idx = np.searchsorted(t_ode_times, t_schedule)\n",
        "        t_idx = np.append(t_idx, len(t_ode_times))\n",
        "\n",
        "        return t_idx\n",
        "\n",
        "    # Fucntion to find the values of scheduled quantities\n",
        "    # at all t_ode_times points\n",
        "\n",
        "    def get_all_values(X, t_ode_times, t_schedule_times, Name):\n",
        "\n",
        "        \"\"\"\n",
        "        X -> Matrix of Quantity at each schedule time. e.g:\n",
        "        At t_schedule_times = [0,5,10] seconds feed:\n",
        "        a concentraction of, X = [1,2,3] g/m^3\n",
        "\n",
        "        \"\"\"\n",
        "        # Get index times\n",
        "        t_idx = find_indices(t_ode_times, t_schedule_times)\n",
        "        # print('t_idx:\\n', t_idx)\n",
        "\n",
        "        # Initialize:\n",
        "        nrows = np.shape(X)[0]\n",
        "        # print('nrows', nrows)\n",
        "\n",
        "        values = np.zeros((nrows, len(t_ode_times))) # same num of rows, we just extend the times\n",
        "        # print('np.shape(values):\\n',np.shape(values))\n",
        "\n",
        "        # Modify:\n",
        "        k = 0\n",
        "\n",
        "        for i in range(len(t_idx)-1): # during each schedule interval\n",
        "            j = i%nrows\n",
        "\n",
        "            # # k is a counter that pushes the row index to the RHS every time it loops back up\n",
        "            # if j == 0 and i == 0:\n",
        "            #     pass\n",
        "            # elif j == 0:\n",
        "            #     k += 1\n",
        "\n",
        "            # print('j',j)\n",
        "\n",
        "            X_new = np.tile(X[:,j], (len(t_ode_times[t_idx[i]:t_idx[i+1]]), 1))\n",
        "\n",
        "            values[:, t_idx[i]:t_idx[i+1]] = X_new.T # apply appropriate quantity value at approprite time intrval\n",
        "\n",
        "        # Visualize:\n",
        "        # # Table\n",
        "        # print(Name,\" Values.shape:\\n\", np.shape(values))\n",
        "        # print(Name,\" Values:\\n\", values)\n",
        "        # # Plot\n",
        "        # plt.plot(t_ode_times, values)\n",
        "        # plt.xlabel('Time (s)')\n",
        "        # plt.ylabel('X')\n",
        "        # plt.show()\n",
        "\n",
        "        return values, t_idx\n",
        "\n",
        "\n",
        "    # Function that adds row slices from a matrix M into one vector\n",
        "    def get_X_row(M, row_start, jump, width):\n",
        "\n",
        "        \"\"\"\n",
        "        M  => Matrix whos rows are to be searched and sliced\n",
        "        row_start => Starting row - the row that the 1st slice comes from\n",
        "        jump => How far the row index jumps to caputre the next slice\n",
        "        width => the widths of each slice e.g. slice 1 is M[row, width[0]:width[1]]\n",
        "\n",
        "        \"\"\"\n",
        "        # Quick look at the inpiuts\n",
        "        # print('M.shape:\\n', M.shape)\n",
        "        # print('width:', width)\n",
        "\n",
        "        # Initialize\n",
        "        values = []\n",
        "        nrows = M.shape[0]\n",
        "\n",
        "        for i in range(len(width)-1):\n",
        "            j = i%nrows\n",
        "            # print('i', i)\n",
        "            # print('j', j)\n",
        "            t_start = int(width[i])\n",
        "            tend = int(width[i+1])\n",
        "\n",
        "            kk = (row_start+j*jump)%nrows\n",
        "\n",
        "            MM = M[kk, t_start:tend]\n",
        "\n",
        "            values.extend(MM)\n",
        "\n",
        "        return values\n",
        "\n",
        "\n",
        "\n",
        "    #  MASS INTO SYSMEM\n",
        "\n",
        "    # - Only the feed port allows material to FLOW IN\n",
        "    ###########################################################################################\n",
        "\n",
        "    # Convert the Feed concentration schedule to show feed conc for all time\n",
        "    # Do this for each component\n",
        "    # C_feed_all = [[] for _ in range(num_comp)]\n",
        "\n",
        "    row_start = 0 # iniital feed port row in schedule matrix\n",
        "\n",
        "    row_start_matrix_raff = nx_col*Z3\n",
        "    row_start_matrix_ext = (nx_col*(Z3 + Z4 + Z1))\n",
        "\n",
        "    row_start_schedule_raff = row_start+Z3\n",
        "    row_start_schedule_ext = row_start+Z3+Z4+Z1\n",
        "\n",
        "    jump_schedule = 1\n",
        "    jump_matrix = nx_col\n",
        "\n",
        "\n",
        "    def feed_profile(t_odes, Cj_pulse_all, t_schedule, row_start, jump):\n",
        "\n",
        "        \"\"\"\"\n",
        "        Function that returns :\n",
        "        (i) The total mass fed of each component\n",
        "        (ii) Vector of feed conc profiles of each component\n",
        "        \"\"\"\n",
        "\n",
        "        # Storage Locations:\n",
        "        C_feed_all = []\n",
        "        t_idx_all = []\n",
        "        m_feed = []\n",
        "\n",
        "        C_feed = [[] for _ in range(num_comp)]\n",
        "\n",
        "        for i in range(num_comp):\n",
        "\n",
        "            if iso_type == 'UNC':\n",
        "\n",
        "                C, t_idx = get_all_values(Cj_pulse_all[i], t_odes[i], t_schedule, 'Concentration')\n",
        "                t_idx_all.append(t_idx)\n",
        "\n",
        "            elif iso_type == 'CUP':\n",
        "                C, t_idx_all = get_all_values(Cj_pulse_all[i], t_odes, t_schedule, 'Concentration')\n",
        "\n",
        "            C_feed_all.append(C)\n",
        "\n",
        "            # print('t_idx_all:\\n', t_idx_all )\n",
        "\n",
        "        for i in range(num_comp):\n",
        "            if iso_type == 'UNC':\n",
        "                C_feed[i] = get_X_row( C_feed_all[i], row_start, jump, t_idx_all[i]) # g/cm^3\n",
        "            elif iso_type == 'CUP':\n",
        "                C_feed[i] = get_X_row( C_feed_all[i], row_start, jump, t_idx_all) # g/cm^3\n",
        "        # print('C_feed[0]:',C_feed[0])\n",
        "\n",
        "        for i in range(num_comp):\n",
        "            F_feed = np.array([C_feed[i]]) * QF # (g/cm^3 * cm^3/s)  =>  g/s | mass flow into col (for comp, i)\n",
        "            F_feed = np.array([F_feed]) # g/s\n",
        "\n",
        "            if iso_type == 'UNC':\n",
        "                m_feed_add = integrate.simpson(F_feed, x=t_odes[i]) # g\n",
        "            if iso_type == 'CUP':\n",
        "                m_feed_add = integrate.simpson(F_feed, x=t_odes) # g\n",
        "\n",
        "            m_feed.append(m_feed_add)\n",
        "\n",
        "        m_feed = np.concatenate(m_feed) # g\n",
        "        # print(f'm_feed: {m_feed} g')\n",
        "\n",
        "        return C_feed, m_feed, t_idx_all\n",
        "\n",
        "    if iso_type == 'UNC':\n",
        "        C_feed, m_feed, t_idx_all = feed_profile(t_sets, Cj_pulse_all, t_schedule, row_start, jump_schedule)\n",
        "    elif iso_type == 'CUP':\n",
        "        C_feed, m_feed, t_idx_all = feed_profile(t, Cj_pulse_all, t_schedule, row_start, jump_schedule)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def prod_profile(t_odes, y_odes, t_schedule, row_start_matrix, jump_matrix, t_idx_all, row_start_schedule):\n",
        "\n",
        "        \"\"\"\"\n",
        "        Function that can be used to return:\n",
        "\n",
        "        (i) The total mass exited at the Raffinate or Extract ports of each component\n",
        "        (ii) Vector of Raffinate or Extract mass flow profiles of each component\n",
        "        (iii) Vector of Raffinate or Extract vol flow profiles of each component\n",
        "\n",
        "        P = Product either raff or ext\n",
        "        \"\"\"\n",
        "        ######## Storages for the Raffinate #########\n",
        "        C_P1 = []\n",
        "        C_P2 = []\n",
        "\n",
        "        Q_all_flows = [] # Flowrates expirenced by each component\n",
        "        m_out_P = np.zeros(num_comp)\n",
        "\n",
        "        P_vflows_1 = []\n",
        "        P_mflows_1 = []\n",
        "        m_P_1 = []\n",
        "\n",
        "        P_vflows_2 = []\n",
        "        P_mflows_2 = []\n",
        "        m_P_2 = []\n",
        "        t_idx_all_Q = []\n",
        "\n",
        "        P_mprofile = []\n",
        "        P_cprofile = []\n",
        "        P_vflow = [[] for _ in range(num_comp)]\n",
        "\n",
        "\n",
        "        if iso_type == 'UNC':\n",
        "            for i in range(num_comp): # for each component\n",
        "                Q_all_flows_add, b = get_all_values(Q_col_all, t_odes[i], t_schedule, 'Column Flowrates')\n",
        "                # print('Q_all_flows_add:\\n', Q_all_flows_add)\n",
        "                Q_all_flows.append(Q_all_flows_add) # cm^3/s\n",
        "                t_idx_all_Q.append(b)\n",
        "\n",
        "        elif iso_type == 'CUP':\n",
        "            Q_all_flows, t_idx_all_Q = get_all_values(Q_col_all, t_odes, t_schedule, 'Column Flowrates')\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(num_comp):# for each component\n",
        "\n",
        "            # Search the ODE matrix\n",
        "            C_R1_add = np.array(get_X_row( y_odes[i][:nx,:], row_start_matrix-1, jump_matrix, t_idx_all[i])) # exclude q\n",
        "            C_R2_add = np.array(get_X_row( y_odes[i][:nx,:], row_start_matrix, jump_matrix, t_idx_all[i]))\n",
        "            # Search the Flowrate Schedule\n",
        "            P_vflows_1_add = np.array(get_X_row(Q_all_flows[i], row_start_schedule-1, jump_schedule, t_idx_all_Q[i]))\n",
        "            P_vflows_2_add = np.array(get_X_row(Q_all_flows[i], row_start_schedule, jump_schedule, t_idx_all_Q[i]))\n",
        "\n",
        "            # Raffinate Massflow Curves\n",
        "            # print('C_R1_add.type():\\n',type(C_R1_add))\n",
        "            # print('np.shape(C_R1_add):\\n', np.shape(C_R1_add))\n",
        "\n",
        "            # print('P_vflows_1_add.type():\\n',type(P_vflows_1_add))\n",
        "            # print('np.shape(P_vflows_1_add):\\n', np.shape(P_vflows_1_add))\n",
        "\n",
        "            # Assuming only conc change accross port when (i) adding feed or (ii) desorbent\n",
        "            C_R2_add = C_R1_add\n",
        "            # P_mflows_1_add = C_R1_add * P_vflows_1_add  # (g/cm^3 * cm^3/s)  =>  g/s\n",
        "            # P_mflows_2_add = C_R2_add * P_vflows_2_add  # g/s\n",
        "\n",
        "            if row_start_matrix == row_start_matrix_raff:\n",
        "                P_vflows_1_add = -QR*np.ones_like(C_R1_add)\n",
        "                P_mflows_1_add = C_R1_add * P_vflows_1_add  # (g/cm^3 * cm^3/s)  =>  g/s\n",
        "\n",
        "            elif row_start_matrix == row_start_matrix_ext:\n",
        "                P_vflows_1_add = -QX*np.ones_like(C_R1_add)\n",
        "                P_mflows_1_add = C_R1_add * P_vflows_1_add  # (g/cm^3 * cm^3/s)  =>  g/s\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Flow profiles:\n",
        "            # Concentration\n",
        "            P_cprofile.append(C_R1_add) # g/s\n",
        "            # Mass g/s\n",
        "            P_mprofile.append(P_mflows_1_add ) #- P_mflows_2_add) # g/s\n",
        "            # Volumetric cm^3/s\n",
        "            P_vflow[i] = P_vflows_1_add #- P_vflows_2_add # cm^3\n",
        "\n",
        "            # Integrate\n",
        "            if iso_type == 'UNC':\n",
        "                m_P_add_1 = integrate.simpson(P_mflows_1_add, x=t_odes[i]) # g\n",
        "                # m_P_add_2 = integrate.simpson(P_mflows_2_add, x=t_odes[i]) # g\n",
        "\n",
        "            if iso_type == 'CUP':\n",
        "                m_P_add_1 = integrate.simpson(P_mflows_1_add, x=t_odes) # g\n",
        "                # m_P_add_2 = integrate.simpson(P_mflows_2_add, x=t_odes) # g\n",
        "\n",
        "\n",
        "\n",
        "            # Storage\n",
        "            C_P1.append(C_R1_add)  # Concentration Profiles\n",
        "            C_P2.append(C_R2_add)\n",
        "\n",
        "            P_vflows_1.append(P_vflows_1_add)\n",
        "            P_vflows_2.append(P_vflows_2_add)\n",
        "\n",
        "            P_mflows_1.append(P_mflows_1_add)\n",
        "            # P_mflows_2.append(P_mflows_2_add)\n",
        "\n",
        "            m_P_1.append(m_P_add_1) # masses of each component\n",
        "            # m_P_2.append(m_P_add_2) # masses of each component\n",
        "\n",
        "        # Final Mass Exited\n",
        "        # Mass out from P and ext\n",
        "        for i in range(num_comp):\n",
        "            m_out_P_add = m_P_1[i] #- m_P_2[i]\n",
        "            # print(f'i:{i}')\n",
        "            # print(f'm_out_P_add = m_P_1[i] - m_P_2[i]: { m_P_1[i]} - {m_P_2[i]}')\n",
        "            m_out_P[i] = m_out_P_add # [A, B] g\n",
        "\n",
        "        return P_cprofile, P_mprofile, m_out_P, P_vflow\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluating the product flowrates\n",
        "    #######################################################\n",
        "    # raff_mprofile, m_out_raff, raff_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_R1, row_start_R2, jump_matrix, t_idx_all, row_start+Z3)\n",
        "    # ext_mprofile, m_out_ext, ext_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_X1, row_start_X2, jump_matrix, t_idx_all, row_start+Z3+Z4+Z1)\n",
        "    if iso_type == 'UNC':\n",
        "        raff_cprofile, raff_mprofile, m_out_raff, raff_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_matrix_raff, jump_matrix, t_idx_all, row_start_schedule_raff)\n",
        "        ext_cprofile, ext_mprofile, m_out_ext, ext_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_matrix_ext, jump_matrix, t_idx_all, row_start_schedule_ext)\n",
        "    elif iso_type == 'CUP':\n",
        "        raff_cprofile, raff_mprofile, m_out_raff, raff_vflow = prod_profile(t, y_matrices, t_schedule, row_start_matrix_raff, jump_matrix, t_idx_all, row_start_schedule_raff)\n",
        "        ext_cprofile, ext_mprofile, m_out_ext, ext_vflow = prod_profile(t, y_matrices, t_schedule, row_start_matrix_ext, jump_matrix, t_idx_all, row_start_schedule_ext)\n",
        "    #######################################################\n",
        "    # print(f'raff_vflow: {raff_vflow}')\n",
        "    # print(f'np.shape(raff_vflow): {np.shape(raff_vflow[0])}')\n",
        "    # print(f'ext_vflow: {ext_vflow}')\n",
        "    # print(f'np.shape(ext_vflow): {np.shape(ext_vflow[0])}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # MASS BALANCE:\n",
        "    #######################################################\n",
        "\n",
        "    # Error = Expected Accumulation - Model Accumulation\n",
        "\n",
        "    #######################################################\n",
        "\n",
        "    # Expected Accumulation = Mass In - Mass Out\n",
        "    # Model Accumulation = Integral in all col at tend (how much is left in col at end of sim)\n",
        "\n",
        "\n",
        "    # Calculate Expected Accumulation\n",
        "    #######################################################\n",
        "    m_out = np.array([m_out_raff]) + np.array([m_out_ext]) # g\n",
        "    m_out = np.concatenate(m_out)\n",
        "    m_in = np.concatenate(m_feed) # g\n",
        "    # ------------------------------------------\n",
        "    Expected_Acc = m_in - m_out # g\n",
        "    # ------------------------------------------\n",
        "\n",
        "\n",
        "    # Calculate Model Accumulation\n",
        "    #######################################################\n",
        "    def model_acc(y_ode, V_col_total, e, num_comp):\n",
        "        \"\"\"\n",
        "        Func to integrate the concentration profiles at tend and estimate the amount\n",
        "        of solute left on the solid and liquid phases\n",
        "        \"\"\"\n",
        "        mass_l = np.zeros(num_comp)\n",
        "        mass_r = np.zeros(num_comp)\n",
        "\n",
        "        for i in range(num_comp): # for each component\n",
        "\n",
        "            V_l = e * V_col_total # Liquid Volume cm^3\n",
        "            V_r = (1-e)* V_col_total # resin Volume cm^3\n",
        "\n",
        "            # conc => g/cm^3\n",
        "            # V => cm^3\n",
        "            # integrate to get => g\n",
        "\n",
        "            # # METHOD 1:\n",
        "            # V_l = np.linspace(0,V_l,nx) # cm^3\n",
        "            # V_r = np.linspace(0,V_r,nx) # cm^3\n",
        "            # mass_l[i] = integrate.simpson(y_ode[i][:nx,-1], x=x)*A_col*e # mass in liq at t=tend\n",
        "            # mass_r[i] = integrate.simpson(y_ode[i][nx:,-1], x=x)*A_col*(1-e) # mass in resin at t=tend\n",
        "\n",
        "            # METHOD 2:\n",
        "            V_l = np.linspace(0,V_l,nx) # cm^3\n",
        "            V_r = np.linspace(0,V_r,nx) # cm^3\n",
        "\n",
        "            mass_l[i] = integrate.simpson(y_ode[i][:nx,-1], x=V_l) # mass in liq at t=tend\n",
        "            mass_r[i] = integrate.simpson(y_ode[i][nx:,-1], x=V_r) # mass in resin at t=tend\n",
        "\n",
        "            # METHOD 3:\n",
        "            # c_avg[i] = np.average(y_ode[i][:nx,-1]) # Average conc at t=tend\n",
        "            # q_avg[i] = np.average(y_ode[i][:nx,-1])\n",
        "\n",
        "            # mass_l = c_avg * V_l\n",
        "            # mass_r = q_avg * V_r\n",
        "\n",
        "\n",
        "        Model_Acc = mass_l + mass_r # g\n",
        "\n",
        "        return Model_Acc\n",
        "\n",
        "    Model_Acc = model_acc(y_matrices, V_col_total, e, num_comp)\n",
        "\n",
        "    # ------------------------------------------\n",
        "    Error = Model_Acc - Expected_Acc\n",
        "\n",
        "    Error_percent = (sum(Error)/sum(Expected_Acc))*100\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # Calculate KEY PERORMANCE PARAMETERS:\n",
        "    #######################################################\n",
        "    # 1. Purity\n",
        "    # 2. Recovery\n",
        "    # 3. Productivity\n",
        "\n",
        "\n",
        "    # 1. Purity\n",
        "    #######################################################\n",
        "    # 1.1 Instantanoues:\n",
        "    # raff_in_purity = raff_mprofile/sum(raff_mprofile)\n",
        "    # ext_insant_purity = ext_mprofile/sum(ext_mprofile)\n",
        "\n",
        "    # 1.2 Integral:\n",
        "    raff_intgral_purity = m_out_raff/sum(m_out_raff)*100\n",
        "    ext_intgral_purity = m_out_ext/sum(m_out_ext)*100\n",
        "\n",
        "    # Final Attained Purity in the Stream\n",
        "    raff_stream_final_purity = np.zeros(num_comp)\n",
        "    ext_stream_final_purity = np.zeros(num_comp)\n",
        "\n",
        "    for i in range(num_comp):\n",
        "        raff_stream_final_purity[i] = raff_cprofile[i][-1]\n",
        "        ext_stream_final_purity[i] = ext_cprofile[i][-1]\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Recovery\n",
        "    #######################################################\n",
        "    # 2.1 Instantanoues:\n",
        "\n",
        "    # raff_in_recovery = raff_mprofile/sum(C_feed*QF)\n",
        "    # ext_insant_recovery = ext_mprofile/sum(C_feed*QF)\n",
        "\n",
        "    # 2.2 Integral:\n",
        "    raff_recov = m_out_raff/m_in*100\n",
        "    ext_recov = m_out_ext/m_in*100\n",
        "\n",
        "    # 3. Productivity\n",
        "    #######################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Visuliization of PERORMANCE PARAMETERS:\n",
        "    #######################################################\n",
        "\n",
        "    ############## TABLES ##################\n",
        "\n",
        "\n",
        "\n",
        "    # Define the data for the table\n",
        "    # data = {\n",
        "    #     'Metric': [\n",
        "    #         'Total Mass IN',\n",
        "    #         'Total Mass OUT',\n",
        "    #         'Total Expected Acc (IN-OUT)',\n",
        "    #         'Total Model Acc (r+l)',\n",
        "    #         'Total Error (Mod-Exp)',\n",
        "    #         'Total Error Percent (relative to Exp_Acc)',\n",
        "    #         'Final Raffinate Collected Purity [A, B,. . ]',\n",
        "    #         'Final Extract Collected Purity [A, B,. . ]',\n",
        "    #         'Final Raffinate Dimensionless Stream Concentration  [A, B,. . ]',\n",
        "    #         'Final Extract Dimensionless Stream Concentration  [A, B,. . ]',\n",
        "    #         'Final Raffinate Recovery[A, B,. . ]',\n",
        "    #         'Final Extract Recovery[A, B,. . ]'\n",
        "    #     ],\n",
        "    #     'Value': [\n",
        "    #         f\"{m_in} g\",\n",
        "    #         f\"{m_out} g\",\n",
        "    #         f'{sum(Expected_Acc)} g',\n",
        "    #         f'{sum(Model_Acc)} g',\n",
        "    #         f'{sum(Error)} g',\n",
        "    #         f'{Error_percent} %',\n",
        "\n",
        "    #         f'{raff_intgral_purity} %',\n",
        "    #         f'{ext_intgral_purity} %',\n",
        "    #         f'{raff_stream_final_purity} g/cm^3',\n",
        "    #         f'{ext_stream_final_purity}',\n",
        "    #         f'{raff_recov} %',\n",
        "    #         f'{ext_recov} %'\n",
        "    #     ]\n",
        "    # }\n",
        "\n",
        "    # # Create a DataFrame\n",
        "    # df = pd.DataFrame(data)\n",
        "\n",
        "    # # Display the DataFrame\n",
        "    # print(df)\n",
        "\n",
        "    return y_matrices, nx, t, t_sets, t_schedule, C_feed, m_in, m_out, raff_cprofile, ext_cprofile, raff_intgral_purity, raff_recov, ext_intgral_purity, ext_recov, raff_vflow, ext_vflow, Model_Acc, Expected_Acc, Error_percent\n",
        "\n"
      ],
      "metadata": {
        "id": "IUcyZ2DE0t85"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "# What tpye of isoherm is required?\n",
        "# Coupled: \"CUP\"\n",
        "# Uncoupled: \"UNC\"\n",
        "iso_type = \"UNC\"\n",
        "\n",
        "###################### PRIMARY INPUTS #########################\n",
        "# Define the names, colors, and parameter sets for 6 components\n",
        "Names = [\"Glucose\", \"Fructose\"]#, 'C', 'D']#, \"C\"]#, \"D\", \"E\", \"F\"]\n",
        "color = [\"g\", \"orange\"]#, \"purple\", \"brown\"]#, \"b\"]#, \"r\", \"purple\", \"brown\"]\n",
        "num_comp = len(Names) # Number of components\n",
        "e = 0.40         # bed voidage\n",
        "Pe_all = [500, 500] #, 200, 200]\n",
        "Bm = 300\n",
        "\n",
        "# Column Dimensions\n",
        "\n",
        "# How many columns in each Zone?\n",
        "\n",
        "Z1, Z2, Z3, Z4 = 1,3,3,1 # *3 for smb config\n",
        "zone_config = np.array([Z1, Z2, Z3, Z4])\n",
        "nnn = Z1 + Z2 + Z3 + Z4\n",
        "\n",
        "L = 70 # cm # Length of one column\n",
        "d_col = 5 # cm # column internal diameter\n",
        "# Calculate the radius\n",
        "r_col = d_col / 2\n",
        "# Calculate the area of the base\n",
        "A_col = np.pi * (r_col ** 2) # cm^2\n",
        "V_col = A_col*L # cm^3\n",
        "# Dimensions of the tubing and from each column:\n",
        "# Assuming the pipe diameter is 20% of the column diameter:\n",
        "d_in = 0.2 * d_col # cm\n",
        "nx_per_col = 15\n",
        "\n",
        "\n",
        "################ Time Specs #################################################################################\n",
        "t_index_min = 3.30 # min # Index time # How long the pulse holds before swtiching\n",
        "n_num_cycles = 15    # Number of Cycles you want the SMB to run for\n",
        "###############  FLOWRATES   #################################################################################\n",
        "\n",
        "# Jochen et al:\n",
        "Q_P, Q_Q, Q_R, Q_S = 5.21, 4, 5.67, 4.65 # x10-7 m^3/s\n",
        "conv_fac = 0.1 # x10-7 m^3/s => cm^3/s\n",
        "Q_P, Q_Q, Q_R, Q_S  = Q_P*conv_fac, Q_Q*conv_fac, Q_R*conv_fac, Q_S*conv_fac\n",
        "\n",
        "Q_I, Q_II, Q_III, Q_IV = Q_R,  Q_S, Q_P, Q_Q\n",
        "\n",
        "\n",
        "Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV])\n",
        "\n",
        "\n",
        "\n",
        "# # Parameter Sets for different components\n",
        "################################################################\n",
        "\n",
        "# Units:\n",
        "# - Concentrations: g/cm^3\n",
        "# - kh: 1/s\n",
        "# - Da: cm^2/s\n",
        "\n",
        "# A must have a less affinity to resin that B - FOUND IN EXtract purity\n",
        "parameter_sets = [\n",
        "    {\"kh\": 3.15/100, \"H\": 0.27, \"C_feed\": 0.02},  # Component A\n",
        "    {\"kh\": 2.217/100, \"H\": 0.53, \"C_feed\": 0.02}] #, # Component B\n",
        "\n",
        "# ISOTHERM PARAMETERS\n",
        "###########################################################################################\n",
        "theta_lin = [parameter_sets[i]['H'] for i in range(num_comp)] # [HA, HB]\n",
        "print('theta_lin:', theta_lin)\n",
        "# theta_lang = [1, 2, 3, 4 ,5, 6] # [HA, HB]\n",
        "theta_cup_lang = [5.29, 3.24, 2.02, 0.03] # [HA, HB, KA, KB]\n",
        "# theta_fre = [1.2, 0.5]\n",
        "# theta_blang = [[2.69, 0.0336, 0.0466, 0.1, 1, 3],\n",
        "#                 [3.73, 0.0336, 0.0466, 0.3, 1, 3]] # [HA, HB]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0VTadDf01Dd",
        "outputId": "de495043-9799-4356-a1df-0e1424b8e2d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta_lin: [0.27, 0.53]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimization Settings"
      ],
      "metadata": {
        "id": "k9VGA_Q5h1eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - - - - -\n",
        "Q_fixed_feed = 2 # L/h\n",
        "Q_fixed_D = 2 # L/h\n",
        "# - - - - -\n",
        "t_index_min = 5 # min\n",
        "# - - - - -\n",
        "Q_max = 7 # L/h\n",
        "Q_min = 1 # L/h\n",
        "# - - - - -\n",
        "m_max = 0.53\n",
        "m_min = 0.27\n",
        "# - - - - -\n",
        "m1_fixed = 0.8\n",
        "m4_fixed = 0.2\n",
        "# - - - - -\n",
        "sampling_budget = 3\n",
        "optimization_budget = 2\n",
        "constraint_threshold = 0.995\n",
        "# - - - - -\n",
        "# L/h --> cm^3/s:\n",
        "Q_max = Q_max/3.6 # L/h --> cm^3/s\n",
        "Q_max = Q_min/3.6 # L/h --> cm^3/s\n",
        "Q_fixed_feed = Q_fixed_feed/3.6 # L/h --> cm^3/s\n",
        "Q_fixed_D = Q_fixed_D/3.6 # L/h --> cm^3/s\n",
        "\n",
        "# - - - - -"
      ],
      "metadata": {
        "id": "CpNHd2DGh0aK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SMB_inputs = [iso_type, Names, color, num_comp, nx_per_col, e, Pe_all, Bm, zone_config, L, d_col, d_in, t_index_min, n_num_cycles, Q_internal, parameter_sets]\n"
      ],
      "metadata": {
        "id": "J-t9zQRu03se"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different Methods to Sample [m1, m2, m3, m4]"
      ],
      "metadata": {
        "id": "niwccMVzjayK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lhq_sample_mj(m_min, m_max, n_samples, diff=0.1):\n",
        "    \"\"\"\n",
        "    Function that performs Latin Hypercube (LHS) sampling for [m1, m2, m3, m4]\n",
        "    Note that for all mjs: (m_min < m_j < m_max)\n",
        "    And that:\n",
        "          (i)   m4 < m1 - (diff*m1) and m2 < m1 - (diff*m1)\n",
        "          (ii)  m2 < m3 - (diff*m3)\n",
        "          (iii) m3 > m4 + (diff*m4)\n",
        "    Final result is an np.array of size: (n_samples, 4)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the array to store the samples\n",
        "    samples = np.zeros((n_samples, 4))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Sample m1, m2, m3, m4 within bounds and respecting constraints\n",
        "        m1 = np.random.uniform(m_min, m_max)\n",
        "        m4 = np.random.uniform(m_min, m1-diff*m1)\n",
        "\n",
        "        # Sample m2 such that it respects the constraint: m2 < m1 - (diff*m1)\n",
        "        m2 = np.random.uniform(m_min, m_max)\n",
        "        while m2 >= m1 - (diff * m1):  # Ensuring m2 < m1 - (diff*m1)\n",
        "            m2 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Sample m3 such that it respects the constraint: m3 > m2 + (diff*m2)\n",
        "        m3 = np.random.uniform(m_min, m_max)\n",
        "        while m3 <= m2 + (diff * m2):  # Ensuring m3 > m2 + (diff*m2)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Ensure the constraint: m3 > m4 + (diff * m4)\n",
        "        while m3 <= m4 + (diff * m4):  # Ensuring m3 > m4 + (diff*m4)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Store the sample in the array\n",
        "        samples[i] = [m1, m2, m3, m4]\n",
        "\n",
        "\n",
        "    return samples\n",
        "\n",
        "def fixed_feed_lhq_sample_mj(t_index_min, Q_fixed_feed, m_min, m_max, n_samples, diff=0.1):\n",
        "    \"\"\"\n",
        "    Since the feed is fixed, m3 is caluclated\n",
        "\n",
        "    Function that performs Latin Hypercube (LHS) sampling for [m1, m2, m3, m4]\n",
        "    Note that for all mjs: (m_min < m_j < m_max)\n",
        "    And that:\n",
        "          (i)   m4 < m1 - (diff*m1) and m2 < m1 - (diff*m1)\n",
        "          (ii)  m2 < m3 - (diff*m3)\n",
        "          (iii) m3 > m4 + (diff*m4)\n",
        "    Final result is an np.array of size: (n_samples, 4)\n",
        "    \"\"\"\n",
        "    # Initialize the array to store the samples\n",
        "    samples = np.zeros((n_samples, 4))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Sample m1, m2, m3, m4 within bounds and respecting constraints\n",
        "        m1 = np.random.uniform(m_min, m_max)\n",
        "        m4 = np.random.uniform(m_min, m1-diff*m1)\n",
        "\n",
        "        # Sample m2 such that it respects the constraint: m2 < m1 - (diff*m1)\n",
        "        m2 = np.random.uniform(m_min, m_max)\n",
        "        while m2 >= m1 - (diff * m1):  # Ensuring m2 < m1 - (diff*m1)\n",
        "            m2 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Sample m3 such that it respects the constraint: m3 > m2 + (diff*m2)\n",
        "        m3 = np.random.uniform(m_min, m_max)\n",
        "        while m3 <= m2 + (diff * m2):  # Ensuring m3 > m2 + (diff*m2)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Ensure the constraint: m3 > m4 + (diff * m4)\n",
        "        while m3 <= m4 + (diff * m4):  # Ensuring m3 > m4 + (diff*m4)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Store the sample in the array\n",
        "        samples[i] = [m1, m2, m3, m4]\n",
        "\n",
        "    return samples\n",
        "\n",
        "def fixed_m1_and_m4_lhq_sample_mj(t_index_min, m1, m4, m_min, m_max, n_samples, diff=0.1):\n",
        "    \"\"\"\n",
        "    - Since the feed is fixed, m3 is caluclated AND\n",
        "    - Since the desorbant is fixed, m1 is caluclated\n",
        "\n",
        "    Function that performs Latin Hypercube (LHS) sampling for [m1, m2, m3, m4]\n",
        "    Note that for all mjs: (m_min < m_j < m_max)\n",
        "    And that:\n",
        "          (i)   m4 < m1 - (diff*m1) and m2 < m1 - (diff*m1)\n",
        "          (ii)  m2 < m3 - (diff*m3)\n",
        "          (iii) m3 > m4 + (diff*m4)\n",
        "    Final result is an np.array of size: (n_samples, 4)\n",
        "    \"\"\"\n",
        "    # Initialize the array to store the samples\n",
        "    samples = np.zeros((n_samples, 4))\n",
        "    samples[:,0] = np.ones(n_samples)*m1\n",
        "    samples[:,-1] = np.ones(n_samples)*m4\n",
        "    # print(f'samples: {samples}')\n",
        "\n",
        "    m2_set = np.linspace(m_min, m_max, n_samples)\n",
        "    # print(f'm2_set: {m2_set}')\n",
        "    num_of_m3_per_m2 = 3\n",
        "    i = np.arange(n_samples)\n",
        "    k = np.repeat(i,num_of_m3_per_m2)\n",
        "\n",
        "    #Sample from the separation triangle:\n",
        "    for i in range(n_samples): # for each vertical line\n",
        "        # print(f'k: {k[i]}')\n",
        "        m2 = m2_set[k[i]]\n",
        "\n",
        "        samples[i, 1] = m2\n",
        "\n",
        "        if i == 0:\n",
        "          m3 = m_max\n",
        "          samples[i, 2] = m3 # apex of trianlge\n",
        "        else:\n",
        "          m3 = np.random.uniform(m2, m_max)\n",
        "          samples[i, 2] = m3\n",
        "\n",
        "    return samples"
      ],
      "metadata": {
        "id": "N0r7I1X-jhN5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to Generate Inital Data"
      ],
      "metadata": {
        "id": "G5V9UfAbjsxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the obj and constraint functions\n",
        "def obj_con(X, t_index_min):\n",
        "  \"\"\"Feasibility weighted objective; zero if not feasible.\n",
        "\n",
        "    X = [m1, m2, m3, m4]; type= torche_tensor\n",
        "    Objective: WAR = Weighted Average Recovery\n",
        "    Constraint: WAP = Weighted Average Purity\n",
        "\n",
        "    Use WAP to calculate the feasibility weights. Which\n",
        "    will scale teh EI output.\n",
        "\n",
        "  \"\"\"\n",
        "  X = np.array(X)\n",
        "  def mj_to_Qj(mj):\n",
        "    Qj = (mj*V_col*(1-e) + V_col*e)/(t_index_min*60) # cm^3/s\n",
        "    return Qj\n",
        "\n",
        "  print(f'np.shape(x_new)[0]: {np.shape(X)}')\n",
        "  if X.ndim == 1:\n",
        "\n",
        "      Pur = np.zeros(2)\n",
        "      Rec = np.zeros(2)\n",
        "      # Unpack and convert to float and np.arrays from torch.tensors:\n",
        "      m1, m2, m3, m4 = float(X[0]), float(X[1]), float(X[2]), float(X[3])\n",
        "      print(f'[m1, m2, m3, m4]: [{m1}, {m2}, {m3}, {m4}]')\n",
        "      Q_I, Q_II, Q_III, Q_IV = mj_to_Qj(m1), mj_to_Qj(m2), mj_to_Qj(m3), mj_to_Qj(m4)\n",
        "      Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV]) # cm^3/s\n",
        "      print(f'Q_internal: {Q_internal} cm^s/s')\n",
        "      print(f'Q_internal: {Q_internal*3.6} L/h')\n",
        "      print(f'Q_internal type: {type(Q_internal)}')\n",
        "\n",
        "      SMB_inputs[12] = t_index_min  # Update t_index\n",
        "      SMB_inputs[14] = Q_internal # Update Q_internal\n",
        "\n",
        "      results = SMB(SMB_inputs)\n",
        "\n",
        "      # print(f'done solving sample {i+1}')\n",
        "\n",
        "      raff_purity = results[10]  # [Glu, Fru]\n",
        "      ext_purity = results[12]  # [Glu, Fru]\n",
        "\n",
        "      raff_recovery = results[11]  # [Glu, Fru]\n",
        "      ext_recovery = results[13]  # [Glu, Fru]\n",
        "\n",
        "      pur1 = raff_purity[0] / 100\n",
        "      pur2 = ext_purity[1] / 100\n",
        "\n",
        "      rec1 = raff_recovery[0] / 100\n",
        "      rec2 = ext_recovery[1] / 100\n",
        "\n",
        "      # Pack\n",
        "      # WAP[i] = WAP_add\n",
        "      # WAR[i] = WAR_add\n",
        "      Pur[:] = [pur1, pur2]\n",
        "      Rec[:] = [rec1, rec2]\n",
        "  elif X.ndim > 1:\n",
        "      Pur = np.zeros((len(X[:,0]), 2))\n",
        "      Rec = np.zeros((len(X[:,0]), 2))\n",
        "\n",
        "      for i in range(len(X[:,0])):\n",
        "\n",
        "          # print(f't_index: {t_index}')\n",
        "          # print(f't_index type: {type(t_index)}')\n",
        "\n",
        "          if np.shape(X)[-1] == 4: # when we are generating the initial smaples (we have all the flowrates already)\n",
        "              # Unpack and convert to float and np.arrays from torch.tensors:\n",
        "              m1, m2, m3, m4 = float(X[i,0]), float(X[i,1]), float(X[i,2]), float(X[i,3])\n",
        "              print(f'[m1, m2, m3, m4]: [{m1}, {m2}, {m3}, {m4}]')\n",
        "              Q_I, Q_II, Q_III, Q_IV = mj_to_Qj(m1), mj_to_Qj(m2), mj_to_Qj(m3), mj_to_Qj(m4)\n",
        "              Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV]) # cm^3/s\n",
        "              print(f'Q_internal: {Q_internal} cm^s/s')\n",
        "              print(f'Q_internal: {Q_internal*3.6} L/h')\n",
        "              print(f'Q_internal type: {type(Q_internal)}')\n",
        "\n",
        "          # elif np.shape(X)[-1] < 4: # During optimization - (we only have QX, Q_rec and t_index)\n",
        "          #     # Unpack and convert to float and np.arrays from torch.tensors:\n",
        "          #     print(f'-----------------')\n",
        "          #     print(f't_index: {t_index}')\n",
        "          #     # print(f't_index type: {type(t_index)}')\n",
        "          #     QX= float(X[i,0]) # cm^3/s\n",
        "          #     Q_rec = -float(X[i,1])# cm^3/s ENSURE this is negative value\n",
        "\n",
        "          #     # ----- Caclulate QR, using vol balance\n",
        "          #     vol_in = Q_fixed_feed + Q_fixed_D # cm^3/s\n",
        "          #     QR = -(vol_in + QX) # SINCE abs(vol_in) > abs(QX)\n",
        "\n",
        "          #     Q_external = np.array([Q_fixed_feed, QR, Q_fixed_D, QX])\n",
        "          # -------------------------------------------------------------\n",
        "          # -------------------------------------------------------------\n",
        "\n",
        "          # print(f'Q_internal type: {type(Q_internal)}')\n",
        "          # Update SMB_inputs:\n",
        "          SMB_inputs[12] = t_index_min  # Update t_index\n",
        "          SMB_inputs[14] = Q_internal # Update Q_internal\n",
        "\n",
        "          results = SMB(SMB_inputs)\n",
        "\n",
        "          # print(f'done solving sample {i+1}')\n",
        "\n",
        "          raff_purity = results[10]  # [Glu, Fru]\n",
        "          ext_purity = results[12]  # [Glu, Fru]\n",
        "\n",
        "          raff_recovery = results[11]  # [Glu, Fru]\n",
        "          ext_recovery = results[13]  # [Glu, Fru]\n",
        "\n",
        "          pur1 = raff_purity[0] / 100\n",
        "          pur2 = ext_purity[1] / 100\n",
        "\n",
        "          rec1 = raff_recovery[0] / 100\n",
        "          rec2 = ext_recovery[1] / 100\n",
        "\n",
        "          # Pack\n",
        "          # WAP[i] = WAP_add\n",
        "          # WAR[i] = WAR_add\n",
        "          Pur[i,:] = [pur1, pur2]\n",
        "          Rec[i,:] = [rec1, rec2]\n",
        "\n",
        "  return  Rec, Pur\n",
        "\n",
        "# ------ Generate Initial Data\n",
        "def generate_initial_data(sampling_budget, t_index_min):\n",
        "\n",
        "    # generate training data\n",
        "    # print(f'Getting {sampling_budget} Samples')\n",
        "    # train_x = lhq_sample_mj(0.2, 1.7, n, diff=0.1)\n",
        "    # train_x = fixed_feed_lhq_sample_mj(t_index_min, Q_fixed_feed, 0.2, 1.7, n, diff=0.1)\n",
        "    train_all = fixed_m1_and_m4_lhq_sample_mj(t_index_min, m1_fixed, m4_fixed, m_min, m_max, sampling_budget, diff=0.1)\n",
        "    # print(f'train_all: {train_all}')\n",
        "    # print(f'Done Getting {sampling_budget} Samples')\n",
        "\n",
        "    # exact_obj, exact_con = weighted_obj(train_x) # add output dimension\n",
        "    # print(f'Solving Over {sampling_budget} Samples')\n",
        "    Rec, Pur = obj_con(train_all, t_index_min)\n",
        "    # print(f'Done Getting {sampling_budget} Samples')\n",
        "    all_outputs = np.hstack((Rec, Pur))\n",
        "    return train_all, all_outputs"
      ],
      "metadata": {
        "id": "KHfU2oVHjwbP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the generate_initial_data Function"
      ],
      "metadata": {
        "id": "WcsFD02pj3oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_all, all_initial_inputs = generate_initial_data(2,t_index_min) # sampling_budget\n",
        "print(f'train_all{ train_all}')\n",
        "print(f'all_initial_inputs{ all_initial_inputs}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "8q5aQnZ4j1ev",
        "outputId": "805f6edb-5c6d-44b0-9220-8b1e53d4d356"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'obj_con' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e802a5c1b866>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_initial_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_initial_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_index_min\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sampling_budget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'train_all{ train_all}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'all_initial_inputs{ all_initial_inputs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8fa390a24875>\u001b[0m in \u001b[0;36mgenerate_initial_data\u001b[0;34m(sampling_budget, t_index_min)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# exact_obj, exact_con = weighted_obj(train_x) # add output dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(f'Solving Over {sampling_budget} Samples')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mRec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_con\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_index_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# print(f'Done Getting {sampling_budget} Samples')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mall_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'obj_con' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Setup\n",
        "\n",
        "\n",
        "First, we define the constraint used in the example in outcome_constraint. The second function weighted_obj is a \"feasibility-weighted objective,\" which returns zero when not feasible. Not that both the constraint and the objective function come from the same experiment."
      ],
      "metadata": {
        "id": "CCHRLIYT-2Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Surrogate model creation ---\n",
        "def surrogate_model(X_train, y_train):\n",
        "    X_train = np.atleast_2d(X_train)\n",
        "    y_train = np.atleast_1d(y_train)\n",
        "\n",
        "    if y_train.ndim == 2 and y_train.shape[1] == 1:\n",
        "        y_train = y_train.ravel()\n",
        "\n",
        "    # kernel = C(1.0, (1e-4, 10.0)) * RBF(1.0, (1e-4, 10.0))\n",
        "    kernel = Matern(length_scale=1.0, nu=1.5)\n",
        "\n",
        "    gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-5, normalize_y=True, n_restarts_optimizer=5)\n",
        "\n",
        "    gp.fit(X_train, y_train)\n",
        "\n",
        "    return gp\n",
        "\n",
        "# --- AQ funcs:\n",
        "\n",
        "# --- AQ func: Expected Constrained Improvement ---\n",
        "def log_expected_constrained_improvement(x, surrogate_obj_gp, constraint_gps, constraint_thresholds, y_best, xi=0.01):\n",
        "    x = np.asarray(x).reshape(1, -1)\n",
        "\n",
        "    mu_obj, sigma_obj = surrogate_obj_gp.predict(x, return_std=True)\n",
        "    # print(f'mu_obj: {mu_obj}, sigma_obj: {sigma_obj} ')\n",
        "\n",
        "\n",
        "    with np.errstate(divide='warn'):\n",
        "        Z = (y_best - mu_obj - xi) / sigma_obj\n",
        "        ei = (y_best - mu_obj - xi) * norm.cdf(Z) + sigma_obj * norm.pdf(Z)\n",
        "    # print(f'ei: {ei}')\n",
        "\n",
        "\n",
        "    # Calcualte the probability of Feasibility, \"prob_feas\"\n",
        "    prob_feas = 1.0 # initialize\n",
        "\n",
        "    for gp_c, lam in zip(constraint_gps, constraint_thresholds):\n",
        "\n",
        "        mu_c, sigma_c = gp_c.predict(x, return_std=True)\n",
        "\n",
        "        # lam -> inf = 1- (-inf -> lam)\n",
        "        prob_that_LESS_than_mu = norm.cdf((lam - mu_c) / sigma_c)\n",
        "\n",
        "        prob_that_GREATER_than_mu = 1 - prob_that_LESS_than_mu\n",
        "\n",
        "        pf = prob_that_GREATER_than_mu\n",
        "\n",
        "        # pf is a vector,\n",
        "        # We just want the non-zero part\n",
        "        pf = pf[pf != 0]\n",
        "\n",
        "        # print(f'pf: {pf}')\n",
        "\n",
        "        # if we assume that the condtions are independent,\n",
        "        # then we can \"multiply\" the weights to get the \"joint probability\" of feasility\n",
        "        prob_feas *= pf\n",
        "\n",
        "\n",
        "    log_eic = np.log(ei) + np.log(prob_feas)\n",
        "\n",
        "    return -log_eic\n",
        "\n",
        "# 1. Expected Improvement ---\n",
        "def expected_improvement(x, surrogate_gp, y_best):\n",
        "    \"\"\"\n",
        "    Computes the Expected Improvement at a point x.\n",
        "    Scalarizes the surrogate predictions using Tchebycheff, then computes EI.\n",
        "\n",
        "    Note that the surrogate GP already has the weights applied to it\n",
        "    \"\"\"\n",
        "    x = np.array(x).reshape(1, -1)\n",
        "\n",
        "    mu, sigma = surrogate_gp.predict(x, return_std=True)\n",
        "\n",
        "    # print(f'mu: {mu}')\n",
        "    # print(f'y_best: {y_best}')\n",
        "    # Compute EI\n",
        "\n",
        "    xi = 0.2 # the greater the value of xi, the more we encourage exploration\n",
        "    with np.errstate(divide='warn'):\n",
        "        Z = (y_best - mu - xi) / sigma\n",
        "        ei = (y_best - mu - xi) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
        "        ei[sigma == 0.0] = 0.0\n",
        "\n",
        "    return -ei[0]  # Negative for minimization\n",
        "\n",
        "    # 2. Probability of Imporovement:\n",
        "def probability_of_improvement(x, surrogate_gp, y_best, xi=0.005):\n",
        "    \"\"\"\n",
        "    Computes the Probability of Improvement (PI) acquisition function.\n",
        "\n",
        "    Parameters:\n",
        "    - mu: np.array of predicted means (shape: [n_samples])\n",
        "    - sigma: np.array of predicted std deviations (shape: [n_samples])\n",
        "    - best_f: scalar, best objective value observed so far\n",
        "    - xi: float, small value to balance exploration/exploitation (default: 0.01)\n",
        "\n",
        "    Returns:\n",
        "    - PI: np.array of probability of improvement values\n",
        "    \"\"\"\n",
        "    x = np.array(x).reshape(1, -1)\n",
        "\n",
        "    mu, sigma = surrogate_gp.predict(x, return_std=True)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if sigma == 0:\n",
        "      sigma = 1e-8\n",
        "\n",
        "    z = (y_best - mu - xi) / sigma\n",
        "\n",
        "    pi = 1 - norm.cdf(z)\n",
        "\n",
        "    return -pi\n",
        "\n",
        "# --- ParEGO Main Loop ---\n",
        "def constrained_BO(optimization_budget, bounds, initial_guess, all_initial_inputs, all_initial_ouputs, constraint_thresholds, xi):\n",
        "\n",
        "    # xi = exploration parameter (the larger it is, the more we explore)\n",
        "\n",
        "    # Initial values\n",
        "    # Unpack from: all_initial_ouputs: [GPur, FPur, GRec, FRec]\n",
        "    # Recovery Objectives\n",
        "    f1_vals = all_initial_ouputs[:,0]\n",
        "    f2_vals = all_initial_ouputs[:,1]\n",
        "    print(f'f1_vals: {f1_vals}')\n",
        "    print(f'f2_vals: {f2_vals}')\n",
        "    # Purity constraints\n",
        "    c1_vals  = all_initial_ouputs[:,2]\n",
        "    c2_vals  = all_initial_ouputs[:,3]\n",
        "\n",
        "    population = all_initial_inputs\n",
        "\n",
        "    all_inputs = np.zeros((sampling_budget+optimization_budget, 4)) # [m1, m2, m3, m4]\n",
        "    # print(f'np.shape(all_inputs):{np.shape(all_inputs)}')\n",
        "    # print(f'np.shape(all_initial_inputs):{np.shape(all_initial_inputs)}')\n",
        "    all_inputs = np.vstack((all_inputs, all_initial_inputs))\n",
        "\n",
        "    # Unpack from: all_initial_inputs\n",
        "\n",
        "    # print(f'shpae_f1_vals = {np.shape(f1_vals)}')\n",
        "\n",
        "    # Initialize where we will store solutions\n",
        "    population_all = []\n",
        "    all_constraint_1_gps = []\n",
        "    all_constraint_2_gps = []\n",
        "    ei_all = []\n",
        "\n",
        "\n",
        "    for gen in range(optimization_budget):\n",
        "        # generation = iteration\n",
        "        print(f\"\\n\\nStarting gen {gen+1}\")\n",
        "\n",
        "\n",
        "        # Generate random weights for scalarization\n",
        "        lam = np.random.rand()\n",
        "        weights = [lam, 1 - lam]\n",
        "        print(f'weights: {weights}')\n",
        "        # Note that we generate new weights in each iteration/generation\n",
        "        # i.e. each time we update the training set\n",
        "\n",
        "        #SCALARIZE THE OBJECTIVES (BEFORE APPLYING GP)\n",
        "        scalarized_f_vals = weights[0]*f1_vals + weights[1]*f2_vals\n",
        "\n",
        "        # Fit GP to scalarized_surrogate_objective\n",
        "        print(f'population { population}, \\nscalarized_f_vals {scalarized_f_vals} ')\n",
        "        scalarized_surrogate_gp = surrogate_model(population, scalarized_f_vals)\n",
        "        # Pull mean at relevant poputlation points\n",
        "        # Mean & Varriance\n",
        "        scalarized_surrogate_gp_mean, scalarized_surrogate_gp_std = scalarized_surrogate_gp.predict(population, return_std=True)\n",
        "        # The best value so far:\n",
        "        y_best = np.max(scalarized_surrogate_gp_mean)\n",
        "        # y_best = 0.60\n",
        "\n",
        "\n",
        "        # Fit a GP to each constraint:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "\n",
        "            # Glu Raff Purity:\n",
        "            constraint_1_gp = surrogate_model(population, c1_vals)\n",
        "            all_constraint_1_gps.append(constraint_1_gp)\n",
        "            # Fru Ext Purity:\n",
        "            constraint_2_gp = surrogate_model(population, c2_vals)\n",
        "            all_constraint_2_gps.append(constraint_2_gp)\n",
        "\n",
        "        # Define the constraint function for the ei optimizer\n",
        "        # Constraint function with correct shape\n",
        "        eps = 0.05  # 5% margin to prevent equality\n",
        "        def constraint_fun(x):\n",
        "            return np.array([\n",
        "                x[0] - x[1] + 0.01*x[1],  # x[0] > x[1]\n",
        "                x[1] - x[2],  # x[2] > x[1]\n",
        "                x[2] - x[3],  # x[2] > x[3]\n",
        "                x[0] - x[3],  # x[0] > x[3]\n",
        "\n",
        "                # Ensure all elements are distinct by enforcing a minimum difference\n",
        "                abs(x[0] - x[1]) - eps,\n",
        "                abs(x[0] - x[2]) - eps,\n",
        "                abs(x[0] - x[3]) - eps,\n",
        "                abs(x[1] - x[2]) - eps,\n",
        "                abs(x[1] - x[3]) - eps,\n",
        "                abs(x[2] - x[3]) - eps,\n",
        "            ])\n",
        "\n",
        "        nonlinear_constraint = NonlinearConstraint(\n",
        "            constraint_fun,\n",
        "            lb=[0, 0, 0, 0] + [eps] * 6,  # Lower bound (inequality → ≥ 0)\n",
        "            ub=[np.inf, np.inf, np.inf, np.inf] + [np.inf] * 6  # Upper bound (no upper limit)\n",
        "        )\n",
        "\n",
        "        # --- Run the optimization ---\n",
        "        print(f'Maxing ECI')\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "\n",
        "            result = differential_evolution(\n",
        "                func=log_expected_constrained_improvement, # probability_of_improvement(x, surrogate_gp, y_best, xi=0.005), expected_improvement(x, surrogate_gp, y_best) | log_expected_constrained_improvement(x, scalarized_surrogate_gp, [constraint_1_gp, constraint_2_gp], constraint_thresholds, y_best, xi)\n",
        "                bounds=bounds,\n",
        "                args=(scalarized_surrogate_gp, [constraint_1_gp, constraint_2_gp], constraint_thresholds, y_best, xi),\n",
        "                strategy='best1bin',\n",
        "                maxiter=200,\n",
        "                popsize=15,\n",
        "                disp=False,\n",
        "                constraints=(nonlinear_constraint,)\n",
        "            )\n",
        "                # Perform the optimization using L-BFGS-B method\n",
        "        # result = minimize(\n",
        "        #     expected_improvement,\n",
        "        #     initial_guess,\n",
        "        #     args=(scalarized_surrogate_gp, y_best),\n",
        "        #     method='L-BFGS-B',\n",
        "        #     bounds=bounds,\n",
        "        #     options={'maxiter': 100, 'disp': True})\n",
        "\n",
        "        x_new = result.x # [m1, m2, m3, m4]\n",
        "        print(f\"x_new: { x_new}\")\n",
        "\n",
        "        # f1_new = objective_f1(x_new)\n",
        "        # f2_new = objective_f2(x_new)\n",
        "\n",
        "        # print(f'Calling smb_func for the {gen}th time')\n",
        "        f_new, c_new = obj_con(x_new, t_index_min)\n",
        "\n",
        "\n",
        "\n",
        "        # Add the new row to all_inputs\n",
        "        all_inputs = np.vstack((all_inputs, x_new))\n",
        "\n",
        "        # Add to population\n",
        "        population_all.append(population)\n",
        "        population = np.vstack((population, x_new))\n",
        "\n",
        "        f1_vals = np.vstack([f1_vals.reshape(-1,1), f_new[0]])\n",
        "        f2_vals = np.vstack([f2_vals.reshape(-1,1), f_new[1]])\n",
        "        c1_vals  = np.vstack([c1_vals.reshape(-1,1), c_new[0]])\n",
        "        c2_vals  = np.vstack([c1_vals.reshape(-1,1), c_new[1]])\n",
        "\n",
        "        print(f\"Gen {gen+1} Status:\\n | Sampled Inputs:{x_new} [m1, m2, m3, m4]|\\n Outputs: f1: {f_new[0]}, f2: {f_new[1]} | GPur, FPur: {c_new[0]}, {c_new[1]}\")\n",
        "\n",
        "    return population_all, f1_vals, f2_vals, c1_vals , c2_vals , all_inputs"
      ],
      "metadata": {
        "id": "Hc1_C2ukpT07"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Initial Inputs"
      ],
      "metadata": {
        "id": "LEpsAFFaqHAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_initial_inputs, all_initial_outputs = generate_initial_data(sampling_budget,t_index_min)\n",
        "\n",
        "print(f'all_initial_inputs{ all_initial_inputs}')\n",
        "print(f'all_initial_outputs{ all_initial_outputs}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grUZvNMrqC_j",
        "outputId": "194a20df-c570-4feb-bbcd-f4c08c417e6a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[m1, m2, m3, m4]: [0.8, 0.27, 0.53, 0.2]\n",
            "Q_internal: [4.03171057 2.57479698 3.28950931 2.38237443] cm^s/s\n",
            "Q_internal: [14.51415806  9.26926912 11.84223351  8.57654794] L/h\n",
            "Q_internal type: <class 'numpy.ndarray'>\n",
            "[m1, m2, m3, m4]: [0.8, 0.27, 0.3910941474740689, 0.2]\n",
            "Q_internal: [4.03171057 2.57479698 2.9076719  2.38237443] cm^s/s\n",
            "Q_internal: [14.51415806  9.26926912 10.46761885  8.57654794] L/h\n",
            "Q_internal type: <class 'numpy.ndarray'>\n",
            "[m1, m2, m3, m4]: [0.8, 0.27, 0.318665537508155, 0.2]\n",
            "Q_internal: [4.03171057 2.57479698 2.70857336 2.38237443] cm^s/s\n",
            "Q_internal: [14.51415806  9.26926912  9.7508641   8.57654794] L/h\n",
            "Q_internal type: <class 'numpy.ndarray'>\n",
            "all_initial_inputs[[0.8        0.27       0.53       0.2       ]\n",
            " [0.8        0.27       0.39109415 0.2       ]\n",
            " [0.8        0.27       0.31866554 0.2       ]]\n",
            "all_initial_outputs[[0.7313338  0.69208053 0.83080271 0.84001779]\n",
            " [0.544118   0.77838865 0.92124001 0.79643292]\n",
            " [0.31216254 0.82308464 0.92092223 0.75538786]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTIMIZATION LOOP"
      ],
      "metadata": {
        "id": "gGm6Rcm0q2DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constraint_thresholds = [0.95, 0.95] # (Purity constraints) will scale with increased col#\n",
        "\n",
        "bounds = [  (m_min, m_max), # m1\n",
        "    (m_min, m_max), # m2\n",
        "    (m_min, m_max), # m3\n",
        "    (m_min, m_max)  # m4]\n",
        "]\n",
        "\n",
        "initial_guess = 0 # min\n",
        "\n",
        "population_all, f1_vals, f2_vals, c1_vals, c2_vals, all_inputs  = constrained_BO(optimization_budget, bounds, initial_guess, all_initial_inputs, all_initial_outputs, constraint_thresholds, 0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PVONiTUqq4yf",
        "outputId": "65c08eec-8918-4929-a404-3f4e532b5871"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1_vals: [0.7313338  0.544118   0.31216254]\n",
            "f2_vals: [0.69208053 0.77838865 0.82308464]\n",
            "\n",
            "\n",
            "Starting gen 1\n",
            "weights: [0.9259548659236013, 0.07404513407639868]\n",
            "population [[0.8        0.27       0.53       0.2       ]\n",
            " [0.8        0.27       0.39109415 0.2       ]\n",
            " [0.8        0.27       0.31866554 0.2       ]], \n",
            "scalarized_f_vals [0.72842728 0.5614646  0.34999383] \n",
            "Maxing ECI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_new: [0.51535882 0.43065805 0.37762038 0.27958119]\n",
            "np.shape(x_new)[0]: (4,)\n",
            "[m1, m2, m3, m4]: [0.5153588227267064, 0.43065804848612804, 0.37762038322081826, 0.2795811886758276]\n",
            "Q_internal: [3.24926227 3.01642886 2.87063396 2.60113465] cm^s/s\n",
            "Q_internal: [11.69734417 10.85914388 10.33428225  9.36408473] L/h\n",
            "Q_internal type: <class 'numpy.ndarray'>\n",
            "Gen 1 Status:\n",
            " | Sampled Inputs:[0.51535882 0.43065805 0.37762038 0.27958119] [m1, m2, m3, m4]|\n",
            " Outputs: f1: -0.0, f2: -0.0 | GPur, FPur: nan, nan\n",
            "\n",
            "\n",
            "Starting gen 2\n",
            "weights: [0.8383720726851747, 0.16162792731482534]\n",
            "population [[0.8        0.27       0.53       0.2       ]\n",
            " [0.8        0.27       0.39109415 0.2       ]\n",
            " [0.8        0.27       0.31866554 0.2       ]\n",
            " [0.51535882 0.43065805 0.37762038 0.27958119]], \n",
            "scalarized_f_vals [[ 0.72498937]\n",
            " [ 0.58198268]\n",
            " [ 0.39474182]\n",
            " [-0.        ]] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-12d95deacb06>:1304: RuntimeWarning: invalid value encountered in divide\n",
            "  raff_intgral_purity = m_out_raff/sum(m_out_raff)*100\n",
            "<ipython-input-2-12d95deacb06>:1305: RuntimeWarning: invalid value encountered in divide\n",
            "  ext_intgral_purity = m_out_ext/sum(m_out_ext)*100\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input y contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-bc547ca4e09b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minitial_guess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpopulation_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mconstrained_BO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimization_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_guess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_initial_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_initial_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint_thresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-9964af8b48bf>\u001b[0m in \u001b[0;36mconstrained_BO\u001b[0;34m(optimization_budget, bounds, initial_guess, all_initial_inputs, all_initial_ouputs, constraint_thresholds, xi)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# Glu Raff Purity:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mconstraint_1_gp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurrogate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mall_constraint_1_gps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstraint_1_gp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;31m# Fru Ext Purity:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-9964af8b48bf>\u001b[0m in \u001b[0;36msurrogate_model\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianProcessRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     )\n\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1387\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     \u001b[0;34m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1397\u001b[0;31m         y = check_array(\n\u001b[0m\u001b[1;32m   1398\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
          ]
        }
      ]
    }
  ]
}