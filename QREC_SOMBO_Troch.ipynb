{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU/poHXA8OnCgepdJnbpON",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nawa-Punabantu/Opt_Algos/blob/main/QREC_SOMBO_Troch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install botorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ265s_spoTL",
        "outputId": "98357743-3cb2-4d06-f066-4a762f387666"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting botorch\n",
            "  Downloading botorch-0.13.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from botorch) (4.12.2)\n",
            "Collecting pyre_extensions (from botorch)\n",
            "  Downloading pyre_extensions-0.0.32-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting gpytorch==1.14 (from botorch)\n",
            "  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting linear_operator==0.6 (from botorch)\n",
            "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from botorch) (2.6.0+cu124)\n",
            "Collecting pyro-ppl>=1.8.4 (from botorch)\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from botorch) (1.14.1)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.11/dist-packages (from botorch) (1.0.0)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from botorch) (3.6.0)\n",
            "Collecting jaxtyping (from gpytorch==1.14->botorch)\n",
            "  Downloading jaxtyping-0.3.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch==1.14->botorch) (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (2.0.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.4.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.8.4->botorch)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.1->botorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.1->botorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.1->botorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.1->botorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.1->botorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.1->botorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.1->botorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.1->botorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.1->botorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.1->botorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->botorch) (1.13.1)\n",
            "Collecting typing-inspect (from pyre_extensions->botorch)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch==1.14->botorch)\n",
            "  Downloading wadler_lindig-0.1.4-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1->botorch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch==1.14->botorch) (1.4.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre_extensions->botorch)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading botorch-0.13.0-py3-none-any.whl (706 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gpytorch-1.14-py3-none-any.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.7/277.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyre_extensions-0.0.32-py3-none-any.whl (12 kB)\n",
            "Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Downloading jaxtyping-0.3.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading wadler_lindig-0.1.4-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: pyro-api, wadler-lindig, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, pyre_extensions, nvidia-cusolver-cu12, pyro-ppl, linear_operator, gpytorch, botorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed botorch-0.13.0 gpytorch-1.14 jaxtyping-0.3.0 linear_operator-0.6 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyre_extensions-0.0.32 pyro-api-0.1.2 pyro-ppl-1.9.1 typing-inspect-0.9.0 wadler-lindig-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "TgNE-ztUpRfz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from botorch.fit import fit_gpytorch_mll\n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.optim.optimize import optimize_acqf\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "# from botorch.acquisition import qExpectedConstrainedImprovement\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import scipy as sp\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.optimize import differential_evolution\n",
        "from scipy.optimize import minimize, NonlinearConstraint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy import integrate\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.double\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
      ],
      "metadata": {
        "id": "hnaGUFeSpniT"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# smb model\n",
        "# # tips:\n",
        "    # - the Error: \"IndexError: index 10 is out of bounds for axis 0 with size 9\"\n",
        "    # may be due to a miss-match in size between the initial conditons and c, q in the ode func.\n",
        "# IMPORTING LIBRARIES\n",
        "###########################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import solve_ivp\n",
        "import matplotlib.pyplot as plt\n",
        "# Loading the Plotting Libraries\n",
        "from matplotlib.pyplot import subplots\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "# from PIL import Image\n",
        "from scipy import integrate\n",
        "# import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "###########################################\n",
        "# IMPORTING MY OWN FUNCTIONS\n",
        "###########################################\n",
        "# from post_pre_processing_funcs import\n",
        "\n",
        "# INPUTS\n",
        "#######################################################\n",
        "\n",
        "# UNITS:\n",
        "# All units must conform to:\n",
        "# Time - s\n",
        "# Lengths - cm^2\n",
        "# Volumes - cm^3\n",
        "# Masses - g\n",
        "# Concentrations - g\n",
        "# Volumetric flowrates - cm^3/s\n",
        "\n",
        "\n",
        "def SMB(SMB_inputs):\n",
        "    iso_type, Names, color, num_comp, nx_per_col, e, Pe_all, Bm, zone_config, L, d_col, d_in, t_index_min, n_num_cycles, Q_internal, parameter_sets = SMB_inputs[0:]\n",
        "\n",
        "    ###################### (CALCUALTED) SECONDARY INPUTS #########################\n",
        "\n",
        "    # Column Dimensions:\n",
        "    ################################################################\n",
        "    F = (1-e)/e     # Phase ratio\n",
        "    t=0\n",
        "    t_sets = 0\n",
        "    Ncol_num = np.sum(zone_config) # Total number of columns\n",
        "    L_total = L*Ncol_num # Total Lenght of all columns\n",
        "    A_col = np.pi*0.25*d_col**2 # cm^2\n",
        "    V_col = A_col * L # cm^3\n",
        "    V_col_total = Ncol_num * V_col # cm^3\n",
        "    A_in = np.pi * (d_in/2)**2 # cm^2\n",
        "    alpha = A_in / A_col\n",
        "\n",
        "\n",
        "\n",
        "    # Time Specs:\n",
        "    ################################################################\n",
        "\n",
        "    t_index = t_index_min*60 # s #\n",
        "\n",
        "    # Notes:\n",
        "    # - Cyclic Steady state typically happens only after 10 cycles (ref: https://doi.org/10.1205/026387603765444500)\n",
        "    # - The system is not currently designed to account for periods of no external flow\n",
        "\n",
        "    n_1_cycle = t_index * Ncol_num  # s How long a single cycle takes\n",
        "\n",
        "    total_cycle_time = n_1_cycle*n_num_cycles # s\n",
        "\n",
        "    tend = total_cycle_time # s # Final time point in ODE solver\n",
        "\n",
        "    tend_min = tend/60\n",
        "\n",
        "    t_span = (0, tend) # +dt)  # from t=0 to t=n\n",
        "\n",
        "    num_of_injections = int(np.round(tend/t_index)) # number of switching periods\n",
        "\n",
        "    # 't_start_inject_all' is a vecoter containing the times when port swithes occur for each port\n",
        "    # Rows --> Different Ports\n",
        "    # Cols --> Different time points\n",
        "    t_start_inject_all = [[] for _ in range(Ncol_num)]  # One list for each node (including the main list)\n",
        "\n",
        "    # Calculate start times for injections\n",
        "    for k in range(num_of_injections):\n",
        "        t_start_inject = k * t_index\n",
        "        t_start_inject_all[0].append(t_start_inject)  # Main list\n",
        "        for node in range(1, Ncol_num):\n",
        "            t_start_inject_all[node].append(t_start_inject + node * 0)  # all rows in t_start_inject_all are identical\n",
        "\n",
        "    t_schedule = t_start_inject_all[0]\n",
        "\n",
        "    # REQUIRED FUNCTIONS:\n",
        "    ################################################################\n",
        "\n",
        "    # 1.\n",
        "    # Func to Generate Indices for the columns\n",
        "    # def generate_repeated_numbers(n, m):\n",
        "    #     result = []\n",
        "    #     n = int(n)\n",
        "    #     m = int(m)\n",
        "    #     for i in range(m):\n",
        "    #         result.extend([i] * n)\n",
        "    #     return result\n",
        "\n",
        "    # 3.\n",
        "    # Func to divide the column into nodes\n",
        "\n",
        "    # DOES NOT INCLUDE THE C0 NODE (BY DEFAULT)\n",
        "    def set_x(L, Ncol_num,nx_col,dx):\n",
        "        if nx_col == None:\n",
        "            x = np.arange(0, L+dx, dx)\n",
        "            nnx = len(x)\n",
        "            nnx_col = int(np.round(nnx/Ncol_num))\n",
        "            nx_BC = Ncol_num - 1 # Number of Nodes (mixing points/boundary conditions) in between columns\n",
        "\n",
        "            # Indecies belonging to the mixing points between columns are stored in 'start'\n",
        "            # These can be thought of as the locations of the nx_BC nodes.\n",
        "            return x, dx, nnx_col,  nnx, nx_BC\n",
        "\n",
        "        elif dx == None:\n",
        "            nx = Ncol_num * nx_col\n",
        "            nx_BC = Ncol_num - 1 # Number of Nodes in between columns\n",
        "            x = np.linspace(0,L_total,nx)\n",
        "            ddx = x[1] - x[0]\n",
        "\n",
        "            # Indecies belonging to the mixing points between columns are stored in 'start'\n",
        "            # These can be thought of as the locations of the nx_BC nodes.\n",
        "\n",
        "            return x, ddx, nx_col, nx, nx_BC\n",
        "\n",
        "    # 4. A func that:\n",
        "    # (i) Calcualtes the internal flowrates given the external OR (ii) Visa-versa\n",
        "    def set_flowrate_values(set_Q_int, set_Q_ext, Q_rec):\n",
        "        if set_Q_ext is None and Q_rec is None:  # Chosen to specify internal/zone flowrates\n",
        "            Q_I = set_Q_int[0]\n",
        "            Q_II = set_Q_int[1]\n",
        "            Q_III = set_Q_int[2]\n",
        "            Q_IV = set_Q_int[3]\n",
        "\n",
        "            QX = -(Q_I - Q_II)\n",
        "            QF = Q_III - Q_II\n",
        "            QR = -(Q_III - Q_IV)\n",
        "            QD = -(QF + QX + QR) # OR: Q_I - Q_IV\n",
        "\n",
        "            Q_ext = np.array([QF, QR, QD, QX]) # cm^3/s\n",
        "\n",
        "            return Q_ext\n",
        "\n",
        "        elif set_Q_int is None and Q_rec is not None:  # Chosen to specify external flowrates\n",
        "            QF = set_Q_ext[0]\n",
        "            QR = set_Q_ext[1]\n",
        "            QD = set_Q_ext[2]\n",
        "            QX = set_Q_ext[3]\n",
        "\n",
        "            Q_I = Q_rec  # m^3/s\n",
        "            Q_III = (QX + QF) + Q_I\n",
        "            Q_IV = (QD - QX) + Q_I  # Fixed Q_II to Q_I as the variable was not defined yet\n",
        "            Q_II = (QR - QX) + Q_IV\n",
        "            Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV])\n",
        "            return Q_internal\n",
        "\n",
        "\n",
        "    # 5. Function to Build Port Schedules:\n",
        "\n",
        "    # This is done in two functions: (i) repeat_array and (ii) build_matrix_from_vector\n",
        "    # (i) repeat_array\n",
        "    # Summary: Creates the schedule for the 1st port, port 0, only. This is the port boadering Z2 & Z3 and always starts as a Feed port at t=0\n",
        "    # (i) build_matrix_from_vector\n",
        "    # Summary: Takes the output from \"repeat_array\" and creates schedules for all other ports.\n",
        "    # The \"trick\" is that the states of each of the, n, ports at t=0, is equal to the first, n, states of port 0.\n",
        "    # Once we know the states for each port at t=0, we form a loop that adds the next state.\n",
        "\n",
        "    # 5.1\n",
        "    def position_maker(schedule_quantity_name, F, R, D, X, Z_config):\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        Function that initializes the starting schedueles for a given quantitiy at all positions\n",
        "\n",
        "        F, R, D and X are the values of the quantiity at the respective feed ports\n",
        "\n",
        "        \"\"\"\n",
        "        # Initialize:\n",
        "        X_j = np.zeros(Ncol_num)\n",
        "\n",
        "\n",
        "        # We set each port in the appropriate position, depending on the nuber of col b/n Zones:\n",
        "        # By default, Position i = 0 (entrance to col,0) is reserved for the feed node.\n",
        "\n",
        "        # Initialize Positions:\n",
        "        # Q_position is a vector whose len is = number of mixing points (ports) b/n columns\n",
        "\n",
        "        X_j[0] = F        # FEED\n",
        "        X_j[Z_config[2]] = R     # RAFFINATE\n",
        "        X_j[Z_config[2] + Z_config[3]] = D    # DESORBENT\n",
        "        X_j[Z_config[2] + Z_config[3]+  Z_config[0]] = X   # EXTRACT\n",
        "\n",
        "        return X_j\n",
        "\n",
        "    # 5.2\n",
        "    def repeat_array(vector, start_time_num):\n",
        "        # vector = the states of all ports at t=0, vector[0] = is always the Feed port\n",
        "        # start_time_num = The number of times the state changes == num of port switches == num_injections\n",
        "        repeated_array = np.tile(vector, (start_time_num // len(vector) + 1))\n",
        "        return repeated_array[:start_time_num]\n",
        "\n",
        "    def initial_u_col(Zconfig, Qint):\n",
        "        \"\"\"\n",
        "        Fun that returns the the inital state at t=0 of the volumetric\n",
        "        flows in all the columns.\n",
        "\n",
        "        \"\"\"\n",
        "        # First row is for col0, which is the feed to zone 3\n",
        "        Zconfig_roll = np.roll(Zconfig, -2)\n",
        "        Qint_roll = np.roll(Qint, -2)\n",
        "\n",
        "        # print(Qint)\n",
        "        X = np.array([])\n",
        "\n",
        "        for i in range(len(Qint_roll)):\n",
        "            X_add = np.ones(Zconfig_roll[i])*Qint_roll[i]\n",
        "            # print('X_add:\\n', X_add)\n",
        "\n",
        "            X = np.append(X, X_add)\n",
        "        # X = np.concatenate(X)\n",
        "        # print('X:\\n', X)\n",
        "        return X\n",
        "\n",
        "\n",
        "    def build_matrix_from_vector(vector, t_schedule):\n",
        "        \"\"\"\n",
        "        Fun that returns the schedeule given the inital state at t=0\n",
        "        vector: inital state of given quantity at t=0 at all nodes\n",
        "        t_schedule: times at which port changes happen\n",
        "\n",
        "        \"\"\"\n",
        "        # vector = the states of all ports at t=0, vector[0] = is always the Feed port\n",
        "        start_time_num = int(len(t_schedule))\n",
        "        vector = np.array(vector)  # Convert the vector to a NumPy array\n",
        "        n = len(vector) # number of ports/columns\n",
        "\n",
        "        # Initialize the matrix for repeated elements, ''ALL''\n",
        "        ALL = np.zeros((n, start_time_num), dtype=vector.dtype)  # Shape is (n, start_time_num)\n",
        "\n",
        "        for i in range(start_time_num):\n",
        "            # print('i:',i)\n",
        "            ALL[:, i] = np.roll(vector, i)\n",
        "        return ALL\n",
        "\n",
        "\n",
        "\n",
        "    # # Uncomment as necessary depending on specification of either:\n",
        "    # # (1) Internal OR (2) External flowrates :\n",
        "    # # (1)\n",
        "    # Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV])\n",
        "    Q_external = set_flowrate_values(Q_internal, None, None) # Order: QF, QR, QD, QX\n",
        "    QF, QR, QD, QX = Q_external[0], Q_external[1], Q_external[2], Q_external[3]\n",
        "    # print('Q_external:', Q_external)\n",
        "\n",
        "    # (2)\n",
        "    # QX, QF, QR = -0.277, 0.315, -0.231  # cm^3/s\n",
        "    # QD = - (QF + QX + QR)\n",
        "    # Q_external = np.array([QF, QR, QD, QX])\n",
        "    # Q_rec = 33.69 # cm^3/s\n",
        "    # Q_internal = set_flowrate_values(None, Q_external, Q_rec) # Order: QF, QR, QD, Q\n",
        "\n",
        "    ################################################################################################\n",
        "\n",
        "\n",
        "    # Make concentration schedules for each component\n",
        "\n",
        "    Cj_pulse_all = [[] for _ in range(num_comp)]\n",
        "    for i in range(num_comp):\n",
        "        Cj_position = []\n",
        "        Cj_position = position_maker('Feed Conc Schedule:', parameter_sets[i]['C_feed'], 0, 0, 0, zone_config)\n",
        "        Cj_pulse = build_matrix_from_vector(Cj_position,  t_schedule)\n",
        "        Cj_pulse_all[i] = Cj_pulse\n",
        "\n",
        "\n",
        "    Q_position = position_maker('Vol Flow Schedule:', Q_external[0], Q_external[1], Q_external[2], Q_external[3], zone_config)\n",
        "    Q_pulse_all = build_matrix_from_vector(Q_position,  t_schedule)\n",
        "\n",
        "    # Spacial Discretization:\n",
        "    # Info:\n",
        "    # nx --> Total Number of Nodes (EXCLUDING mixing points b/n nodes)\n",
        "    # nx_col --> Number of nodes in 1 column\n",
        "    # nx_BC --> Number of mixing points b/n nodes\n",
        "    x, dx, nx_col, nx, nx_BC = set_x(L=L_total, Ncol_num = Ncol_num, nx_col = nx_per_col, dx = None)\n",
        "    start = [i*nx_col for i in range(0,Ncol_num)] # Locations of the BC indecies\n",
        "    u_col_at_t0 = initial_u_col(zone_config, Q_internal)\n",
        "    Q_col_all = build_matrix_from_vector(u_col_at_t0, t_schedule)\n",
        "\n",
        "\n",
        "    # DISPLAYING INPUT INFORMATION:\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('Number of Components:', num_comp)\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nTime Specs:\\n')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('Number of Cycles:', n_num_cycles)\n",
        "    # print('Time Per Cycle:', n_1_cycle/60, \"min\")\n",
        "    # print('Simulation Time:', tend_min, 'min')\n",
        "    # print('Index Time:', t_index, 's OR', t_index/60, 'min' )\n",
        "    # print('Number of Port Switches:', num_of_injections)\n",
        "    # print('Injections happen at t(s) = :', t_schedule, 'seconds')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nColumn Specs:\\n')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('Configuration:', zone_config, '[Z1,Z2,Z3,Z4]')\n",
        "    # print(f\"Number of Columns: {Ncol_num}\")\n",
        "    # print('Column Length:', L, 'cm')\n",
        "    # print('Column Diameter:', d_col, 'cm')\n",
        "    # print('Column Volume:', V_col, 'cm^3')\n",
        "    # print(\"alpha:\", alpha, '(alpha = A_in / A_col)')\n",
        "    # print(\"Nodes per Column:\",nx_col)\n",
        "    # print(\"Boundary Nodes locations,x[i], i =\", start)\n",
        "    # print(\"Total Number of Nodes (nx):\",nx)\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nFlowrate Specs:\\n')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print(\"External Flowrates =\", Q_external, '[F,R,D,X] ml/min')\n",
        "    # print(\"Ineternal Flowrates =\", Q_internal, 'ml/min')\n",
        "    # print('---------------------------------------------------')\n",
        "    # print('\\nPort Schedules:')\n",
        "    # for i in range(num_comp):\n",
        "    #     print(f\"Concentration Schedule:\\nShape:\\n {Names[i]}:\\n\",np.shape(Cj_pulse_all[i]),'\\n', Cj_pulse_all[i], \"\\n\")\n",
        "    # print(\"Injection Flowrate Schedule:\\nShape:\",np.shape(Q_pulse_all),'\\n', Q_pulse_all, \"\\n\")\n",
        "    # print(\"Respective Column Flowrate Schedule:\\nShape:\",np.shape(Q_col_all),'\\n', Q_col_all, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    # ###########################################################################################\n",
        "\n",
        "    # Isotherm Models:\n",
        "\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "    # 1. LINEAR\n",
        "    def iso_lin(theta_lin, c):\n",
        "        # params: [HA, HB]\n",
        "        H = theta_lin\n",
        "        q_star = H*c\n",
        "\n",
        "        return q_star # [qA, qB, ...]\n",
        "\n",
        "    # 2.  LANGMUIR\n",
        "\n",
        "    # 2.1 Independent Langmuir\n",
        "    def iso_langmuir(theta_lang, c, comp_idx): # already for specific comp\n",
        "        H = theta_lang\n",
        "        q_star = H*c/(1 + H*c)\n",
        "        #q_star = H[comp_idx]*c/(1 + K[0]*c + K[1]*c)\n",
        "        # q_star = theta_lang[0]*c/(1 + theta_lang[1]*c + theta_lang[2]*c) +\\\n",
        "        #     theta_lang[3]*c/(1 + theta_lang[4]*c + theta_lang[5]*c)\n",
        "        return q_star\n",
        "\n",
        "    # 2.3 Coupled Langmuir\n",
        "    def iso_cup_langmuir(theta_cuplang, c, IDX, comp_idx): # already for specific comp\n",
        "        H = theta_cuplang[:2] # [HA, HB]\n",
        "        K = theta_cuplang[2:] # [KA, KB]\n",
        "        cA = c[IDX[0] + 0: IDX[0] + nx ]\n",
        "        cB = c[IDX[1] + 0: IDX[1] + nx ]\n",
        "        c_i = [cA, cB]\n",
        "        q_star = H[comp_idx]*c_i[comp_idx]/(1 + K[0]*cA + K[1]*cB)\n",
        "        return q_star\n",
        "\n",
        "    # 2.3 Bi-Langmuir\n",
        "    def iso_bi_langmuir(theta_bl, c, IDX, comp_idx): # already for specific comp\n",
        "        cA = c[IDX[0] + 0: IDX[0] + nx ]\n",
        "        cB = c[IDX[1] + 0: IDX[1] + nx ]\n",
        "        c_i = [cA, cB]\n",
        "\n",
        "        q_star = theta_bl[0]*c_i[comp_idx]/(1 + theta_bl[1]*cA + theta_bl[2]*cB) +\\\n",
        "                theta_bl[3]*c_i[comp_idx]/(1 + theta_bl[4]*cA + theta_bl[5]*cB)\n",
        "\n",
        "        return q_star\n",
        "\n",
        "\n",
        "    # 3. FREUDLICH:\n",
        "    def iso_freundlich(theta_fre, c): # already for specific comp\n",
        "        q_star = theta_fre[0]*c**(1/theta_fre[1])\n",
        "        return q_star\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "    # Mass Transfer (MT) Models:\n",
        "\n",
        "    def mass_transfer(kav_params, q_star, q): # already for specific comp\n",
        "        # kav_params: [kA, kB]\n",
        "        kav =  kav_params\n",
        "        MT = kav * Bm/(5 + Bm) * (q_star - q)\n",
        "        # MT = kav * (q_star - q)\n",
        "        return MT\n",
        "\n",
        "    # MT PARAMETERS\n",
        "    ###########################################################################################\n",
        "    # print('np.shape(parameter_sets[:][\"kh\"]):', np.shape(parameter_sets[3]))\n",
        "    kav_params = [parameter_sets[i][\"kh\"] for i in range(num_comp)]  # [kA, kB, kC, kD, kE, kF]\n",
        "    # print('kav_params:', kav_params)\n",
        "    # print('----------------------------------------------------------------')\n",
        "    ###########################################################################################\n",
        "\n",
        "    # # FORMING THE ODES\n",
        "\n",
        "\n",
        "    # Form the remaining schedule matrices that are to be searched by the funcs\n",
        "\n",
        "    # Column velocity schedule:\n",
        "    u_col_all = -Q_col_all/A_col/e\n",
        "\n",
        "    # Column Dispersion schedule:\n",
        "    # Different matrices for each comp because diff Pe's for each comp\n",
        "    D_col_all = []\n",
        "    for i in range(num_comp): # for each comp\n",
        "        D_col = -(u_col_all*L)/Pe_all[i] # constant dispersion coeff\n",
        "        D_col_all.append(D_col)\n",
        "\n",
        "    # Storage Spaces:\n",
        "    coef_0 = np.zeros_like(u_col_all)\n",
        "    coef_1 = np.zeros_like(u_col_all)\n",
        "    coef_2 = np.zeros_like(u_col_all)\n",
        "\n",
        "    # coef_0, coef_1, & coef_2 correspond to the coefficents of ci-1, ci & ci+1 respectively\n",
        "    # These depend on u and so change with time, thus have a schedule\n",
        "\n",
        "    # From descritization:\n",
        "    coef_0_all = []\n",
        "    coef_1_all = []\n",
        "    coef_2_all = []\n",
        "    for j in range(num_comp): # for each comp\n",
        "        for i  in range(Ncol_num): # coefficients for each col\n",
        "            coef_0[i,:] = ( D_col_all[j][i,:]/dx**2 ) - ( u_col_all[i,:]/dx ) # coefficeint of i-1\n",
        "            coef_1[i,:] = ( u_col_all[i,:]/dx ) - (2*D_col_all[j][i,:]/(dx**2))# coefficeint of i\n",
        "            coef_2[i,:] = (D_col_all[j][i,:]/(dx**2))    # coefficeint of i+1\n",
        "        coef_0_all.append(coef_0)\n",
        "        coef_1_all.append(coef_1)\n",
        "        coef_2_all.append(coef_2)\n",
        "\n",
        "    # All shedules:\n",
        "    # For each shceudle, rows => col idx, columns => Time idx\n",
        "    # :\n",
        "    # - Q_pulse_all: Injection flowrates\n",
        "    # - C_pulse_all: Injection concentrations for each component\n",
        "    # - Q_col_all:  Flowrates WITHIN each col\n",
        "    # - u_col_all: Linear velocities WITHIN each col\n",
        "    # - D_col_all: Dispersion coefficeints WITHIN each col\n",
        "    # - coef_0, 1 and 2: ci, ci-1 & ci+1 ceofficients\n",
        "\n",
        "    # print('coef_0:\\n',coef_0)\n",
        "    # print('coef_1:\\n',coef_1)\n",
        "    # print('coef_2:\\n',coef_2)\n",
        "    # print('\\nD_col_all:\\n',D_col_all)\n",
        "    # print('Q_col_all:\\n',Q_col_all)\n",
        "    # print('A_col:\\n',A_col)\n",
        "    # print('u_col_all:\\n',u_col_all)\n",
        "\n",
        "\n",
        "    def coeff_matrix_builder_UNC(t, Q_col_all, Q_pulse_all, dx, start, alpha, c, nx_col, comp_idx): # note that c_length must include nx_BC\n",
        "\n",
        "        # Define the functions that call the appropriate schedule matrices:\n",
        "        # Because all scheudels are of the same from, only one function is required\n",
        "        # Calling volumetric flows:\n",
        "        get_X = lambda t, X_schedule, col_idx: next((X_schedule[col_idx][j] for j in range(len(X_schedule[col_idx])) if t_start_inject_all[col_idx][j] <= t < t_start_inject_all[col_idx][j] + t_index), 1/100000000)\n",
        "        get_C = lambda t, C_schedule, col_idx, comp_idx: next((C_schedule[comp_idx][col_idx][j] for j in range(len(C_schedule[comp_idx][col_idx])) if t_start_inject_all[col_idx][j] <= t < t_start_inject_all[col_idx][j] + t_index), 1/100000000)\n",
        "\n",
        "        def small_col_matix(nx_col, col_idx):\n",
        "        # Initialize small_col_coeff ('small' = for 1 col)\n",
        "\n",
        "            small_col_coeff = np.zeros((int(nx_col),int(nx_col))) #(5,5)\n",
        "\n",
        "            # Where the 1st (0th) row and col are for c1\n",
        "            # get_C(t, coef_0_all, k, comp_idx)\n",
        "            # small_col_coeff[0,0], small_col_coeff[0,1] = get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "            small_col_coeff[0,0], small_col_coeff[0,1] = get_C(t,coef_1_all,col_idx, comp_idx), get_C(t,coef_2_all,col_idx, comp_idx)\n",
        "            # for c2:\n",
        "            # small_col_coeff[1,0], small_col_coeff[1,1], small_col_coeff[1,2] = get_X(t,coef_0,col_idx), get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "            small_col_coeff[1,0], small_col_coeff[1,1], small_col_coeff[1,2] = get_C(t,coef_0_all,col_idx, comp_idx), get_C(t, coef_1_all, col_idx, comp_idx), get_C(t, coef_2_all,col_idx, comp_idx)\n",
        "\n",
        "            for i in range(2,nx_col): # from row i=2 onwards\n",
        "                # np.roll the row entries from the previous row, for all the next rows\n",
        "                new_row = np.roll(small_col_coeff[i-1,:],1)\n",
        "                small_col_coeff[i:] = new_row\n",
        "\n",
        "            small_col_coeff[-1,0] = 0\n",
        "            small_col_coeff[-1,-1] = small_col_coeff[-1,-1] +  get_C(t,coef_2_all,col_idx, comp_idx) # coef_1 + coef_2 account for rolling boundary\n",
        "\n",
        "\n",
        "            return small_col_coeff\n",
        "\n",
        "\n",
        "        larger_coeff_matrix = np.zeros((nx,nx)) # ''large'' = for all cols # (20, 20)\n",
        "\n",
        "        # Add the cols\n",
        "        for col_idx in range(Ncol_num):\n",
        "            larger_coeff_matrix[col_idx*nx_col:(col_idx+1)*nx_col, col_idx*nx_col:(col_idx+1)*nx_col] = small_col_matix(nx_col,col_idx)\n",
        "        # print('np.shape(larger_coeff_matrix)\\n',np.shape(larger_coeff_matrix))\n",
        "\n",
        "        # vector_add: vector that applies the boundary conditions to each boundary node\n",
        "        def vector_add(nx, c, start, comp_idx):\n",
        "            vec_add = np.zeros(nx)\n",
        "            c_BC = np.zeros(Ncol_num)\n",
        "            # Indeceis for the boundary nodes are stored in \"start\"\n",
        "            # Each boundary node is affected by the form:\n",
        "            # c_BC = V1 * C_IN - V2 * c[i] + V3 * c[i+1]\n",
        "\n",
        "            # R1 = ((beta * alpha) / gamma)\n",
        "            # R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "            # R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "\n",
        "            # Where:\n",
        "            # C_IN is the weighted conc exiting the port facing the column entrance.\n",
        "            # alpha , bata and gamma depend on the column vecolity and are thus time dependant\n",
        "            # Instead of forming schedules for alpha , bata and gamma, we calculate them in-line\n",
        "\n",
        "            for i in range(len(start)):\n",
        "                #  start[i] => the node at the entrance to the ith col\n",
        "                # So start[3] is the node representing the 1st node in col 3\n",
        "\n",
        "                Q_1 = get_X(t, Q_col_all, i-1) # Vol_flow from previous column (which for column 0, is the last column in the chain)\n",
        "                Q_2 = get_X(t, Q_pulse_all, i) # Vol_flow injected IN port i\n",
        "\n",
        "                Q_out_port = get_X(t, Q_col_all, i) # Vol_flow OUT of port 0 (Also could have used Q_1 + Q_2)\n",
        "\n",
        "\n",
        "                W1 = Q_1/Q_out_port # Weighted flowrate to column i\n",
        "                W2 = Q_2/Q_out_port # Weighted flowrate to column i\n",
        "\n",
        "                # Calcualte Weighted Concentration:\n",
        "\n",
        "                c_injection = get_C(t, Cj_pulse_all, i, comp_idx)\n",
        "\n",
        "                if Q_2 > 0: # Concentration in the next column is only affected for injection flows IN\n",
        "                    C_IN = W1 * c[i*nx_col-1] + W2 * c_injection\n",
        "                else:\n",
        "                    # C_IN = c[i*nx_col-1] # no change in conc during product collection\n",
        "                    C_IN = c[start[i]-1] # no change in conc during product collection\n",
        "\n",
        "                # Calcualte alpha, bata and gamma:\n",
        "                # Da = get_X(t, D_col_all, i)\n",
        "                Da = get_C(t, D_col_all, i, comp_idx)\n",
        "                u =  get_X(t, u_col_all, i)\n",
        "                beta = 1 / alpha\n",
        "                gamma = 1 - 3 * Da / (2 * u * dx)\n",
        "\n",
        "                ##\n",
        "                R1 = ((beta * alpha) / gamma)\n",
        "                R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "                R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "                ##\n",
        "\n",
        "                # Calcualte the BC effects:\n",
        "                j = start[i]\n",
        "                c_BC[i] = R1 * C_IN - R2 * c[j] + R3 * c[j+1] # the boundary concentration for that node\n",
        "\n",
        "            # print('c_BC:\\n', c_BC)\n",
        "\n",
        "            for k in range(len(c_BC)):\n",
        "                # vec_add[start[k]]  = get_X(t,coef_0,k)*c_BC[k]\n",
        "                vec_add[start[k]]  = get_C(t, coef_0_all, k, comp_idx)*c_BC[k]\n",
        "\n",
        "            return vec_add\n",
        "            # print('np.shape(vect_add)\\n',np.shape(vec_add(nx, c, start)))\n",
        "        return larger_coeff_matrix, vector_add(nx, c, start, comp_idx)\n",
        "\n",
        "    def coeff_matrix_builder_CUP(t, Q_col_all, Q_pulse_all, dx, start_CUP, alpha, c, nx_col,IDX): # note that c_length must include nx_BC\n",
        "\n",
        "        # Define the functions that call the appropriate schedule matrices:\n",
        "        # Because all scheudels are of the same from, only one function is required\n",
        "        # Calling volumetric flows:\n",
        "        get_X = lambda t, X_schedule, col_idx: next((X_schedule[col_idx][j] for j in range(len(X_schedule[col_idx])) if t_start_inject_all[col_idx][j] <= t < t_start_inject_all[col_idx][j] + t_index), 1/100000000)\n",
        "\n",
        "\n",
        "        # 1. From coefficent \"small\" matrix for movement of single comp through single col\n",
        "        # 2. Form  \"large\" coefficent matrix for movement through one all cols\n",
        "        # 3. The large  coefficent matrix for each comp will then be combined into Final Matrix\n",
        "\n",
        "        # 1.\n",
        "        def small_col_matrix(nx_col, col_idx):\n",
        "\n",
        "        # Initialize small_col_coeff ('small' = for 1 col)\n",
        "\n",
        "            small_col_coeff = np.zeros((int(nx_col),int(nx_col))) #(5,5)\n",
        "\n",
        "            # Where the 1st (0th) row and col are for c1\n",
        "            #\n",
        "            small_col_coeff[0,0], small_col_coeff[0,1] = get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "            # for c2:\n",
        "            small_col_coeff[1,0], small_col_coeff[1,1], small_col_coeff[1,2] = get_X(t,coef_0,col_idx), get_X(t,coef_1,col_idx), get_X(t,coef_2,col_idx)\n",
        "\n",
        "            for i in range(2,nx_col): # from row i=2 onwards\n",
        "                # np.roll the row entries from the previous row, for all the next rows\n",
        "                new_row = np.roll(small_col_coeff[i-1,:],1)\n",
        "                small_col_coeff[i:] = new_row\n",
        "\n",
        "            small_col_coeff[-1,0] = 0\n",
        "            small_col_coeff[-1,-1] = small_col_coeff[-1,-1] +  get_X(t,coef_2,col_idx) # coef_1 + coef_2 account for rolling boundary\n",
        "\n",
        "            return small_col_coeff\n",
        "\n",
        "        # 2. Func to Build Large Matrix\n",
        "\n",
        "        def matrix_builder(M, M0):\n",
        "            # M = Matrix to add (small)\n",
        "            # M0 = Initial state of the larger matrix to be added to\n",
        "            nx_col = M.shape[0]\n",
        "            repeat = int(np.round(M0.shape[0]/M.shape[0]))# numbner of times the small is added to the larger matrix\n",
        "            for col_idx in range(repeat):\n",
        "                        M0[col_idx*nx_col:(col_idx+1)*nx_col, col_idx*nx_col:(col_idx+1)*nx_col] = M\n",
        "            return M0\n",
        "\n",
        "\n",
        "        # 3. Generate and Store the Large Matrices\n",
        "        # Storage Space:\n",
        "        # NOTE: Assuming all components have the same Dispersion coefficients,\n",
        "        # all components will have the same large_col_matrix\n",
        "        # Add the cols\n",
        "        larger_coeff_matrix = np.zeros((nx,nx)) # ''large'' = for all cols # (20, 20)\n",
        "\n",
        "        for col_idx in range(Ncol_num):\n",
        "            larger_coeff_matrix[col_idx*nx_col:(col_idx+1)*nx_col, col_idx*nx_col:(col_idx+1)*nx_col] = small_col_matrix(nx_col,col_idx)\n",
        "\n",
        "        # print('np.shape(larger_coeff_matrix)\\n',np.shape(larger_coeff_matrix))\n",
        "\n",
        "        # Inital final matrix:\n",
        "        n = nx*num_comp\n",
        "        final_matrix0 = np.zeros((n,n))\n",
        "\n",
        "\n",
        "        final_matrix = matrix_builder(larger_coeff_matrix, final_matrix0)\n",
        "\n",
        "            # vector_add: vector that applies the boundary conditions to each boundary node\n",
        "        def vector_add(nx, c, start):\n",
        "            vec_add = np.zeros(nx*num_comp)\n",
        "            c_BC = np.zeros(nx*num_comp)\n",
        "            # Indeceis for the boundary nodes are stored in \"start\"\n",
        "            # Each boundary node is affected by the form:\n",
        "            # c_BC = V1 * C_IN - V2 * c[i] + V3 * c[i+1]\n",
        "\n",
        "            # R1 = ((beta * alpha) / gamma)\n",
        "            # R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "            # R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "\n",
        "            # Where:\n",
        "            # C_IN is the weighted conc exiting the port facing the column entrance.\n",
        "            # alpha , bata and gamma depend on the column vecolity and are thus time dependant\n",
        "            # Instead of forming schedules for alpha , bata and gamma, we calculate them in-line\n",
        "\n",
        "            for i in range(len(start)):\n",
        "                #k = i%len(start) # Recounts columns for B\n",
        "                Q_1 = get_X(t, Q_col_all, i-1) # Vol_flow from previous column (which for column 0, is the last column in the chain)\n",
        "                Q_2 = get_X(t, Q_pulse_all, i) # Vol_flow injected IN port i\n",
        "\n",
        "                Q_out_port = get_X(t, Q_col_all, i) # Vol_flow OUT of port 0 (Also could have used Q_1 + Q_2)\n",
        "\n",
        "\n",
        "                W1 = Q_1/Q_out_port # Weighted flowrate to column i\n",
        "                W2 = Q_2/Q_out_port # Weighted flowrate to column i\n",
        "\n",
        "                # Calcualte Weighted Concentration:\n",
        "                # Identifiers:\n",
        "                A = IDX[0]\n",
        "                B = IDX[1]\n",
        "\n",
        "                # C_IN_A = W1 * c[A + i*nx_col-1] + W2 * get_X(t, C_pulse_all_A, i) # c[-1] conc out the last col\n",
        "                # C_IN_B = W1 * c[B + i*nx_col-1] + W2 * get_X(t, C_pulse_all_B, i) # c[-1] conc out the last col\n",
        "\n",
        "                C_IN_A = W1 * c[A + i*nx_col-1] + W2 * get_X(t, Cj_pulse_all[0], i) # c[-1] conc out the last col\n",
        "                C_IN_B = W1 * c[B + i*nx_col-1] + W2 * get_X(t, Cj_pulse_all[1], i) # c[-1] conc out the last col\n",
        "\n",
        "\n",
        "                # Calcualte alpha, bata and gamma:\n",
        "                Da = get_X(t, D_col_all, i)\n",
        "                u =  get_X(t, u_col_all, i)\n",
        "                beta = 1 / alpha\n",
        "                gamma = 1 - 3 * Da / (2 * u * dx)\n",
        "\n",
        "                ##\n",
        "                R1 = ((beta * alpha) / gamma)\n",
        "                R2 = ((2 * Da / (u * dx)) / gamma)\n",
        "                R3 = ((Da / (2 * u * dx)) / gamma)\n",
        "                ##\n",
        "\n",
        "                # Calcualte the BC effects:\n",
        "                j = start[i]\n",
        "                # print('j:', j)\n",
        "                c_BC[i] = R1 * C_IN_A - R2 * c[j] + R3 * c[j+1] # the boundary concentration for that node\n",
        "                c_BC[B + i] = R1 * C_IN_B - R2 * c[B+j] + R3 * c[B+j+1]\n",
        "            # print('c_BC:\\n', c_BC)\n",
        "            # print('c_BC.shape:\\n', c_BC.shape)\n",
        "\n",
        "            for k in range(len(start)):\n",
        "                vec_add[start[k]]  = get_X(t,coef_0,k)*c_BC[k]\n",
        "                vec_add[B + start[k]]  = get_X(t,coef_0,k)*c_BC[B+ k]\n",
        "\n",
        "            return vec_add\n",
        "            # print('np.shape(vect_add)\\n',np.shape(vec_add(nx, c, start)))\n",
        "        return final_matrix, vector_add(nx, c, start_CUP)\n",
        "\n",
        "    # ###########################################################################################\n",
        "\n",
        "    # # mod1: UNCOUPLED ISOTHERM:\n",
        "    # # Profiles for each component can be solved independently\n",
        "\n",
        "    # ###########################################################################################\n",
        "    def mod1(t, v, comp_idx, Q_pulse_all):\n",
        "        # call.append(\"call\")\n",
        "        # print(len(call))\n",
        "        c = v[:nx]\n",
        "        q = v[nx:]\n",
        "\n",
        "        # Initialize the derivatives\n",
        "        dc_dt = np.zeros_like(c)\n",
        "        dq_dt = np.zeros_like(q)\n",
        "        # print('v size\\n',np.shape(v))\n",
        "\n",
        "        # Isotherm:\n",
        "        #########################################################################\n",
        "        isotherm = iso_lin(theta_lin[comp_idx], c)\n",
        "        #isotherm = iso_langmuir(theta_lang[comp_idx], c, comp_idx)\n",
        "        #isotherm = iso_freundlich(theta_fre, c)\n",
        "\n",
        "\n",
        "        # Mass Transfer:\n",
        "        #########################################################################\n",
        "        # print('isotherm size\\n',np.shape(isotherm))\n",
        "        MT = mass_transfer(kav_params[comp_idx], isotherm, q)\n",
        "        #print('MT:\\n', MT)\n",
        "\n",
        "        coeff_matrix, vec_add = coeff_matrix_builder_UNC(t, Q_col_all, Q_pulse_all, dx, start, alpha, c, nx_col, comp_idx)\n",
        "        # print('coeff_matrix:\\n',coeff_matrix)\n",
        "        # print('vec_add:\\n',vec_add)\n",
        "        dc_dt = coeff_matrix @ c + vec_add - F * MT\n",
        "        dq_dt = MT\n",
        "\n",
        "        return np.concatenate([dc_dt, dq_dt])\n",
        "\n",
        "    # ##################################################################################\n",
        "\n",
        "    def mod2(t, v):\n",
        "\n",
        "        # where, v = [c, q]\n",
        "        c = v[:num_comp*nx] # c = [cA, cB] | cA = c[:nx], cB = c[nx:]\n",
        "        q = v[num_comp*nx:] # q = [qA, qB]| qA = q[:nx], qB = q[nx:]\n",
        "\n",
        "        # Craate Lables so that we know the component assignement in the c vecotor:\n",
        "        A, B = 0*nx, 1*nx # Assume Binary 2*nx, 3*nx, 4*nx, 5*nx\n",
        "        IDX = [A, B]\n",
        "\n",
        "        # Thus to refer to the liquid concentration of the i = nth row of component B: c[C + n]\n",
        "        # Or the the solid concentration 10th row of component B: q[B + 10]\n",
        "        # Or to refer to all A's OR B's liquid concentrations: c[A + 0: A + nx] OR c[B + 0: B + nx]\n",
        "\n",
        "\n",
        "        # Initialize the derivatives\n",
        "        dc_dt = np.zeros_like(c)\n",
        "        dq_dt = np.zeros_like(q)\n",
        "\n",
        "\n",
        "        coeff_matrix, vec_add = coeff_matrix_builder_CUP(t, Q_col_all, Q_pulse_all, dx, start, alpha, c, nx_col, IDX)\n",
        "        # print('coeff_matrix:\\n',coeff_matrix)\n",
        "        # print('vec_add:\\n',vec_add)\n",
        "\n",
        "\n",
        "\n",
        "        ####################### Building MT Terms ####################################################################\n",
        "\n",
        "        # Initialize\n",
        "\n",
        "        MT = np.zeros(len(c)) # column vector: MT kinetcis for each comp: MT = [MT_A MT_B]\n",
        "\n",
        "        for comp_idx in range(num_comp): # for each component\n",
        "\n",
        "            ######################(i) Isotherm ####################################################################\n",
        "\n",
        "            # Comment as necessary for required isotherm:\n",
        "            # isotherm = iso_bi_langmuir(theta_blang[comp_idx], c, IDX, comp_idx)\n",
        "            isotherm = iso_cup_langmuir(theta_cup_lang, c, IDX, comp_idx)\n",
        "            # print('qstar:\\n', isotherm.shape)\n",
        "            ################### (ii) MT ##########################################################\n",
        "            MT_comp = mass_transfer(kav_params[comp_idx], isotherm, q[IDX[comp_idx] + 0: IDX[comp_idx] + nx ])\n",
        "            MT[IDX[comp_idx] + 0: IDX[comp_idx] + nx ] = MT_comp\n",
        "            # [MT_A, MT_B, . . . ] KINETICS FOR EACH COMP\n",
        "\n",
        "\n",
        "\n",
        "        dc_dt = coeff_matrix @ c + vec_add - F * MT\n",
        "        dq_dt = MT\n",
        "\n",
        "        return np.concatenate([dc_dt, dq_dt])\n",
        "\n",
        "    # ##################################################################################\n",
        "\n",
        "    # SOLVING THE ODES\n",
        "    # creat storage spaces:\n",
        "    y_matrices = []\n",
        "\n",
        "    t_sets = []\n",
        "    t_lengths = []\n",
        "\n",
        "    c_IN_values_all = []\n",
        "    F_in_values_all = []\n",
        "    call = []\n",
        "\n",
        "    # print('----------------------------------------------------------------')\n",
        "    # print(\"\\n\\nSolving the ODEs. . . .\")\n",
        "\n",
        "\n",
        "\n",
        "    if iso_type == \"UNC\": # UNCOUPLED - solve 1 comp at a time\n",
        "        for comp_idx in range(num_comp): # for each component\n",
        "            # print(f'Solving comp {comp_idx}. . . .')\n",
        "            # print('\\nSolution Size:')\n",
        "            v0 = np.zeros(Ncol_num* (nx_col + nx_col)) #  for both c and q\n",
        "            solution = solve_ivp(mod1, t_span, v0, args=(comp_idx , Q_pulse_all))\n",
        "            y_solution, t = solution.y, solution.t\n",
        "            y_matrices.append(y_solution)\n",
        "            t_sets.append(t)\n",
        "            t_lengths.append(len(t))\n",
        "            # print(f'y_matrices[{i}]', y_matrices[i].shape)\n",
        "\n",
        "\n",
        "    # Assuming only a binary coupled system\n",
        "    if iso_type == \"CUP\": # COUPLED - solve\n",
        "            # nx = nx_col*num_comp\n",
        "            v0 = np.zeros(num_comp*(nx)*2) # for c and 2, for each comp\n",
        "            solution = solve_ivp(mod2, t_span, v0)\n",
        "            y_solution, t = solution.y, solution.t\n",
        "            # Convert y_solution from: [cA, cB, qA, qB] ,  TO: [[cA, qA ], [cB, qB]]\n",
        "            # Write a function to do that\n",
        "\n",
        "            def reshape_ysol(x, nx, num_comp):\n",
        "                # Initialize a list to store the reshaped components\n",
        "                reshaped_list = []\n",
        "\n",
        "                # Iterate over the number of components\n",
        "                for i in range(num_comp):\n",
        "                    # Extract cX and qX submatrices for the i-th component\n",
        "                    cX = x[i*nx:(i+1)*nx, :]      # Extract cX submatrix\n",
        "                    qX = x[i*nx + num_comp*nx : (i+1)*nx + num_comp*nx, :]       # Extract qX submatrix\n",
        "                    concat = np.concatenate([cX, qX])\n",
        "                    # print('i:', i)\n",
        "                    # print('cX:\\n',cX)\n",
        "                    # print('qX:\\n',qX)\n",
        "                    # Append the reshaped pair [cX, qX] to the list\n",
        "                    reshaped_list.append(concat)\n",
        "\n",
        "                # Convert the list to a NumPy array\n",
        "                result = np.array(reshaped_list)\n",
        "\n",
        "                return result\n",
        "\n",
        "            y_matrices = reshape_ysol(y_solution, nx, num_comp)\n",
        "            # print('len(t_sets) = ', len(t_sets[0]))\n",
        "            # print('len(t) = ', len(t))\n",
        "\n",
        "    # print('----------------------------------------------------------------')\n",
        "    # print('\\nSolution Size:')\n",
        "    # for i in range(num_comp):\n",
        "    #     print(f'y_matrices[{i}]', y_matrices[i].shape)\n",
        "    # print('----------------------------------------------------------------')\n",
        "    # print('----------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ###########################################################################################\n",
        "\n",
        "    # VISUALIZATION\n",
        "\n",
        "    ###########################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # MASS BALANCE AND PURITY CURVES\n",
        "    ###########################################################################################\n",
        "\n",
        "    def find_indices(t_ode_times, t_schedule):\n",
        "        \"\"\"\n",
        "        t_schedule -> vector of times when (events) port switches happen e.g. at [0,5,10] seconds\n",
        "        t_ode_times -> vector of times from ODE\n",
        "\n",
        "        We want to know where in t_ode_times, t_schedule occures\n",
        "        These iwll be stored as indecies in t_idx\n",
        "        Returns:np.ndarray: An array of indices in t_ode_times corresponding to each value in t_schedule.\n",
        "        \"\"\"\n",
        "        t_idx = np.searchsorted(t_ode_times, t_schedule)\n",
        "        t_idx = np.append(t_idx, len(t_ode_times))\n",
        "\n",
        "        return t_idx\n",
        "\n",
        "    # Fucntion to find the values of scheduled quantities\n",
        "    # at all t_ode_times points\n",
        "\n",
        "    def get_all_values(X, t_ode_times, t_schedule_times, Name):\n",
        "\n",
        "        \"\"\"\n",
        "        X -> Matrix of Quantity at each schedule time. e.g:\n",
        "        At t_schedule_times = [0,5,10] seconds feed:\n",
        "        a concentraction of, X = [1,2,3] g/m^3\n",
        "\n",
        "        \"\"\"\n",
        "        # Get index times\n",
        "        t_idx = find_indices(t_ode_times, t_schedule_times)\n",
        "        # print('t_idx:\\n', t_idx)\n",
        "\n",
        "        # Initialize:\n",
        "        nrows = np.shape(X)[0]\n",
        "        # print('nrows', nrows)\n",
        "\n",
        "        values = np.zeros((nrows, len(t_ode_times))) # same num of rows, we just extend the times\n",
        "        # print('np.shape(values):\\n',np.shape(values))\n",
        "\n",
        "        # Modify:\n",
        "        k = 0\n",
        "\n",
        "        for i in range(len(t_idx)-1): # during each schedule interval\n",
        "            j = i%nrows\n",
        "\n",
        "            # # k is a counter that pushes the row index to the RHS every time it loops back up\n",
        "            # if j == 0 and i == 0:\n",
        "            #     pass\n",
        "            # elif j == 0:\n",
        "            #     k += 1\n",
        "\n",
        "            # print('j',j)\n",
        "\n",
        "            X_new = np.tile(X[:,j], (len(t_ode_times[t_idx[i]:t_idx[i+1]]), 1))\n",
        "\n",
        "            values[:, t_idx[i]:t_idx[i+1]] = X_new.T # apply appropriate quantity value at approprite time intrval\n",
        "\n",
        "        # Visualize:\n",
        "        # # Table\n",
        "        # print(Name,\" Values.shape:\\n\", np.shape(values))\n",
        "        # print(Name,\" Values:\\n\", values)\n",
        "        # # Plot\n",
        "        # plt.plot(t_ode_times, values)\n",
        "        # plt.xlabel('Time (s)')\n",
        "        # plt.ylabel('X')\n",
        "        # plt.show()\n",
        "\n",
        "        return values, t_idx\n",
        "\n",
        "\n",
        "    # Function that adds row slices from a matrix M into one vector\n",
        "    def get_X_row(M, row_start, jump, width):\n",
        "\n",
        "        \"\"\"\n",
        "        M  => Matrix whos rows are to be searched and sliced\n",
        "        row_start => Starting row - the row that the 1st slice comes from\n",
        "        jump => How far the row index jumps to caputre the next slice\n",
        "        width => the widths of each slice e.g. slice 1 is M[row, width[0]:width[1]]\n",
        "\n",
        "        \"\"\"\n",
        "        # Quick look at the inpiuts\n",
        "        # print('M.shape:\\n', M.shape)\n",
        "        # print('width:', width)\n",
        "\n",
        "        # Initialize\n",
        "        values = []\n",
        "        nrows = M.shape[0]\n",
        "\n",
        "        for i in range(len(width)-1):\n",
        "            j = i%nrows\n",
        "            # print('i', i)\n",
        "            # print('j', j)\n",
        "            t_start = int(width[i])\n",
        "            tend = int(width[i+1])\n",
        "\n",
        "            kk = (row_start+j*jump)%nrows\n",
        "\n",
        "            MM = M[kk, t_start:tend]\n",
        "\n",
        "            values.extend(MM)\n",
        "\n",
        "        return values\n",
        "\n",
        "\n",
        "\n",
        "    #  MASS INTO SYSMEM\n",
        "\n",
        "    # - Only the feed port allows material to FLOW IN\n",
        "    ###########################################################################################\n",
        "\n",
        "    # Convert the Feed concentration schedule to show feed conc for all time\n",
        "    # Do this for each component\n",
        "    # C_feed_all = [[] for _ in range(num_comp)]\n",
        "\n",
        "    row_start = 0 # iniital feed port row in schedule matrix\n",
        "\n",
        "    row_start_matrix_raff = nx_col*Z3\n",
        "    row_start_matrix_ext = (nx_col*(Z3 + Z4 + Z1))\n",
        "\n",
        "    row_start_schedule_raff = row_start+Z3\n",
        "    row_start_schedule_ext = row_start+Z3+Z4+Z1\n",
        "\n",
        "    jump_schedule = 1\n",
        "    jump_matrix = nx_col\n",
        "\n",
        "\n",
        "    def feed_profile(t_odes, Cj_pulse_all, t_schedule, row_start, jump):\n",
        "\n",
        "        \"\"\"\"\n",
        "        Function that returns :\n",
        "        (i) The total mass fed of each component\n",
        "        (ii) Vector of feed conc profiles of each component\n",
        "        \"\"\"\n",
        "\n",
        "        # Storage Locations:\n",
        "        C_feed_all = []\n",
        "        t_idx_all = []\n",
        "        m_feed = []\n",
        "\n",
        "        C_feed = [[] for _ in range(num_comp)]\n",
        "\n",
        "        for i in range(num_comp):\n",
        "\n",
        "            if iso_type == 'UNC':\n",
        "\n",
        "                C, t_idx = get_all_values(Cj_pulse_all[i], t_odes[i], t_schedule, 'Concentration')\n",
        "                t_idx_all.append(t_idx)\n",
        "\n",
        "            elif iso_type == 'CUP':\n",
        "                C, t_idx_all = get_all_values(Cj_pulse_all[i], t_odes, t_schedule, 'Concentration')\n",
        "\n",
        "            C_feed_all.append(C)\n",
        "\n",
        "            # print('t_idx_all:\\n', t_idx_all )\n",
        "\n",
        "        for i in range(num_comp):\n",
        "            if iso_type == 'UNC':\n",
        "                C_feed[i] = get_X_row( C_feed_all[i], row_start, jump, t_idx_all[i]) # g/cm^3\n",
        "            elif iso_type == 'CUP':\n",
        "                C_feed[i] = get_X_row( C_feed_all[i], row_start, jump, t_idx_all) # g/cm^3\n",
        "        # print('C_feed[0]:',C_feed[0])\n",
        "\n",
        "        for i in range(num_comp):\n",
        "            F_feed = np.array([C_feed[i]]) * QF # (g/cm^3 * cm^3/s)  =>  g/s | mass flow into col (for comp, i)\n",
        "            F_feed = np.array([F_feed]) # g/s\n",
        "\n",
        "            if iso_type == 'UNC':\n",
        "                m_feed_add = integrate.simpson(F_feed, x=t_odes[i]) # g\n",
        "            if iso_type == 'CUP':\n",
        "                m_feed_add = integrate.simpson(F_feed, x=t_odes) # g\n",
        "\n",
        "            m_feed.append(m_feed_add)\n",
        "\n",
        "        m_feed = np.concatenate(m_feed) # g\n",
        "        # print(f'm_feed: {m_feed} g')\n",
        "\n",
        "        return C_feed, m_feed, t_idx_all\n",
        "\n",
        "    if iso_type == 'UNC':\n",
        "        C_feed, m_feed, t_idx_all = feed_profile(t_sets, Cj_pulse_all, t_schedule, row_start, jump_schedule)\n",
        "    elif iso_type == 'CUP':\n",
        "        C_feed, m_feed, t_idx_all = feed_profile(t, Cj_pulse_all, t_schedule, row_start, jump_schedule)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def prod_profile(t_odes, y_odes, t_schedule, row_start_matrix, jump_matrix, t_idx_all, row_start_schedule):\n",
        "\n",
        "        \"\"\"\"\n",
        "        Function that can be used to return:\n",
        "\n",
        "        (i) The total mass exited at the Raffinate or Extract ports of each component\n",
        "        (ii) Vector of Raffinate or Extract mass flow profiles of each component\n",
        "        (iii) Vector of Raffinate or Extract vol flow profiles of each component\n",
        "\n",
        "        P = Product either raff or ext\n",
        "        \"\"\"\n",
        "        ######## Storages for the Raffinate #########\n",
        "        C_P1 = []\n",
        "        C_P2 = []\n",
        "\n",
        "        Q_all_flows = [] # Flowrates expirenced by each component\n",
        "        m_out_P = np.zeros(num_comp)\n",
        "\n",
        "        P_vflows_1 = []\n",
        "        P_mflows_1 = []\n",
        "        m_P_1 = []\n",
        "\n",
        "        P_vflows_2 = []\n",
        "        P_mflows_2 = []\n",
        "        m_P_2 = []\n",
        "        t_idx_all_Q = []\n",
        "\n",
        "        P_mprofile = []\n",
        "        P_cprofile = []\n",
        "        P_vflow = [[] for _ in range(num_comp)]\n",
        "\n",
        "\n",
        "        if iso_type == 'UNC':\n",
        "            for i in range(num_comp): # for each component\n",
        "                Q_all_flows_add, b = get_all_values(Q_col_all, t_odes[i], t_schedule, 'Column Flowrates')\n",
        "                # print('Q_all_flows_add:\\n', Q_all_flows_add)\n",
        "                Q_all_flows.append(Q_all_flows_add) # cm^3/s\n",
        "                t_idx_all_Q.append(b)\n",
        "\n",
        "        elif iso_type == 'CUP':\n",
        "            Q_all_flows, t_idx_all_Q = get_all_values(Q_col_all, t_odes, t_schedule, 'Column Flowrates')\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(num_comp):# for each component\n",
        "\n",
        "            # Search the ODE matrix\n",
        "            C_R1_add = np.array(get_X_row( y_odes[i][:nx,:], row_start_matrix-1, jump_matrix, t_idx_all[i])) # exclude q\n",
        "            C_R2_add = np.array(get_X_row( y_odes[i][:nx,:], row_start_matrix, jump_matrix, t_idx_all[i]))\n",
        "            # Search the Flowrate Schedule\n",
        "            P_vflows_1_add = np.array(get_X_row(Q_all_flows[i], row_start_schedule-1, jump_schedule, t_idx_all_Q[i]))\n",
        "            P_vflows_2_add = np.array(get_X_row(Q_all_flows[i], row_start_schedule, jump_schedule, t_idx_all_Q[i]))\n",
        "\n",
        "            # Raffinate Massflow Curves\n",
        "            # print('C_R1_add.type():\\n',type(C_R1_add))\n",
        "            # print('np.shape(C_R1_add):\\n', np.shape(C_R1_add))\n",
        "\n",
        "            # print('P_vflows_1_add.type():\\n',type(P_vflows_1_add))\n",
        "            # print('np.shape(P_vflows_1_add):\\n', np.shape(P_vflows_1_add))\n",
        "\n",
        "            # Assuming only conc change accross port when (i) adding feed or (ii) desorbent\n",
        "            C_R2_add = C_R1_add\n",
        "            # P_mflows_1_add = C_R1_add * P_vflows_1_add  # (g/cm^3 * cm^3/s)  =>  g/s\n",
        "            # P_mflows_2_add = C_R2_add * P_vflows_2_add  # g/s\n",
        "\n",
        "            if row_start_matrix == row_start_matrix_raff:\n",
        "                P_vflows_1_add = -QR*np.ones_like(C_R1_add)\n",
        "                P_mflows_1_add = C_R1_add * P_vflows_1_add  # (g/cm^3 * cm^3/s)  =>  g/s\n",
        "\n",
        "            elif row_start_matrix == row_start_matrix_ext:\n",
        "                P_vflows_1_add = -QX*np.ones_like(C_R1_add)\n",
        "                P_mflows_1_add = C_R1_add * P_vflows_1_add  # (g/cm^3 * cm^3/s)  =>  g/s\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Flow profiles:\n",
        "            # Concentration\n",
        "            P_cprofile.append(C_R1_add) # g/s\n",
        "            # Mass g/s\n",
        "            P_mprofile.append(P_mflows_1_add ) #- P_mflows_2_add) # g/s\n",
        "            # Volumetric cm^3/s\n",
        "            P_vflow[i] = P_vflows_1_add #- P_vflows_2_add # cm^3\n",
        "\n",
        "            # Integrate\n",
        "            if iso_type == 'UNC':\n",
        "                m_P_add_1 = integrate.simpson(P_mflows_1_add, x=t_odes[i]) # g\n",
        "                # m_P_add_2 = integrate.simpson(P_mflows_2_add, x=t_odes[i]) # g\n",
        "\n",
        "            if iso_type == 'CUP':\n",
        "                m_P_add_1 = integrate.simpson(P_mflows_1_add, x=t_odes) # g\n",
        "                # m_P_add_2 = integrate.simpson(P_mflows_2_add, x=t_odes) # g\n",
        "\n",
        "\n",
        "\n",
        "            # Storage\n",
        "            C_P1.append(C_R1_add)  # Concentration Profiles\n",
        "            C_P2.append(C_R2_add)\n",
        "\n",
        "            P_vflows_1.append(P_vflows_1_add)\n",
        "            P_vflows_2.append(P_vflows_2_add)\n",
        "\n",
        "            P_mflows_1.append(P_mflows_1_add)\n",
        "            # P_mflows_2.append(P_mflows_2_add)\n",
        "\n",
        "            m_P_1.append(m_P_add_1) # masses of each component\n",
        "            # m_P_2.append(m_P_add_2) # masses of each component\n",
        "\n",
        "        # Final Mass Exited\n",
        "        # Mass out from P and ext\n",
        "        for i in range(num_comp):\n",
        "            m_out_P_add = m_P_1[i] #- m_P_2[i]\n",
        "            # print(f'i:{i}')\n",
        "            # print(f'm_out_P_add = m_P_1[i] - m_P_2[i]: { m_P_1[i]} - {m_P_2[i]}')\n",
        "            m_out_P[i] = m_out_P_add # [A, B] g\n",
        "\n",
        "        return P_cprofile, P_mprofile, m_out_P, P_vflow\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluating the product flowrates\n",
        "    #######################################################\n",
        "    # raff_mprofile, m_out_raff, raff_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_R1, row_start_R2, jump_matrix, t_idx_all, row_start+Z3)\n",
        "    # ext_mprofile, m_out_ext, ext_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_X1, row_start_X2, jump_matrix, t_idx_all, row_start+Z3+Z4+Z1)\n",
        "    if iso_type == 'UNC':\n",
        "        raff_cprofile, raff_mprofile, m_out_raff, raff_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_matrix_raff, jump_matrix, t_idx_all, row_start_schedule_raff)\n",
        "        ext_cprofile, ext_mprofile, m_out_ext, ext_vflow = prod_profile(t_sets, y_matrices, t_schedule, row_start_matrix_ext, jump_matrix, t_idx_all, row_start_schedule_ext)\n",
        "    elif iso_type == 'CUP':\n",
        "        raff_cprofile, raff_mprofile, m_out_raff, raff_vflow = prod_profile(t, y_matrices, t_schedule, row_start_matrix_raff, jump_matrix, t_idx_all, row_start_schedule_raff)\n",
        "        ext_cprofile, ext_mprofile, m_out_ext, ext_vflow = prod_profile(t, y_matrices, t_schedule, row_start_matrix_ext, jump_matrix, t_idx_all, row_start_schedule_ext)\n",
        "    #######################################################\n",
        "    # print(f'raff_vflow: {raff_vflow}')\n",
        "    # print(f'np.shape(raff_vflow): {np.shape(raff_vflow[0])}')\n",
        "    # print(f'ext_vflow: {ext_vflow}')\n",
        "    # print(f'np.shape(ext_vflow): {np.shape(ext_vflow[0])}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # MASS BALANCE:\n",
        "    #######################################################\n",
        "\n",
        "    # Error = Expected Accumulation - Model Accumulation\n",
        "\n",
        "    #######################################################\n",
        "\n",
        "    # Expected Accumulation = Mass In - Mass Out\n",
        "    # Model Accumulation = Integral in all col at tend (how much is left in col at end of sim)\n",
        "\n",
        "\n",
        "    # Calculate Expected Accumulation\n",
        "    #######################################################\n",
        "    m_out = np.array([m_out_raff]) + np.array([m_out_ext]) # g\n",
        "    m_out = np.concatenate(m_out)\n",
        "    m_in = np.concatenate(m_feed) # g\n",
        "    # ------------------------------------------\n",
        "    Expected_Acc = m_in - m_out # g\n",
        "    # ------------------------------------------\n",
        "\n",
        "\n",
        "    # Calculate Model Accumulation\n",
        "    #######################################################\n",
        "    def model_acc(y_ode, V_col_total, e, num_comp):\n",
        "        \"\"\"\n",
        "        Func to integrate the concentration profiles at tend and estimate the amount\n",
        "        of solute left on the solid and liquid phases\n",
        "        \"\"\"\n",
        "        mass_l = np.zeros(num_comp)\n",
        "        mass_r = np.zeros(num_comp)\n",
        "\n",
        "        for i in range(num_comp): # for each component\n",
        "\n",
        "            V_l = e * V_col_total # Liquid Volume cm^3\n",
        "            V_r = (1-e)* V_col_total # resin Volume cm^3\n",
        "\n",
        "            # conc => g/cm^3\n",
        "            # V => cm^3\n",
        "            # integrate to get => g\n",
        "\n",
        "            # # METHOD 1:\n",
        "            # V_l = np.linspace(0,V_l,nx) # cm^3\n",
        "            # V_r = np.linspace(0,V_r,nx) # cm^3\n",
        "            # mass_l[i] = integrate.simpson(y_ode[i][:nx,-1], x=x)*A_col*e # mass in liq at t=tend\n",
        "            # mass_r[i] = integrate.simpson(y_ode[i][nx:,-1], x=x)*A_col*(1-e) # mass in resin at t=tend\n",
        "\n",
        "            # METHOD 2:\n",
        "            V_l = np.linspace(0,V_l,nx) # cm^3\n",
        "            V_r = np.linspace(0,V_r,nx) # cm^3\n",
        "\n",
        "            mass_l[i] = integrate.simpson(y_ode[i][:nx,-1], x=V_l) # mass in liq at t=tend\n",
        "            mass_r[i] = integrate.simpson(y_ode[i][nx:,-1], x=V_r) # mass in resin at t=tend\n",
        "\n",
        "            # METHOD 3:\n",
        "            # c_avg[i] = np.average(y_ode[i][:nx,-1]) # Average conc at t=tend\n",
        "            # q_avg[i] = np.average(y_ode[i][:nx,-1])\n",
        "\n",
        "            # mass_l = c_avg * V_l\n",
        "            # mass_r = q_avg * V_r\n",
        "\n",
        "\n",
        "        Model_Acc = mass_l + mass_r # g\n",
        "\n",
        "        return Model_Acc\n",
        "\n",
        "    Model_Acc = model_acc(y_matrices, V_col_total, e, num_comp)\n",
        "\n",
        "    # ------------------------------------------\n",
        "    Error = Model_Acc - Expected_Acc\n",
        "\n",
        "    Error_percent = (sum(Error)/sum(Expected_Acc))*100\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # Calculate KEY PERORMANCE PARAMETERS:\n",
        "    #######################################################\n",
        "    # 1. Purity\n",
        "    # 2. Recovery\n",
        "    # 3. Productivity\n",
        "\n",
        "\n",
        "    # 1. Purity\n",
        "    #######################################################\n",
        "    # 1.1 Instantanoues:\n",
        "    # raff_in_purity = raff_mprofile/sum(raff_mprofile)\n",
        "    # ext_insant_purity = ext_mprofile/sum(ext_mprofile)\n",
        "\n",
        "    # 1.2 Integral:\n",
        "    raff_intgral_purity = m_out_raff/sum(m_out_raff)*100\n",
        "    ext_intgral_purity = m_out_ext/sum(m_out_ext)*100\n",
        "\n",
        "    # Final Attained Purity in the Stream\n",
        "    raff_stream_final_purity = np.zeros(num_comp)\n",
        "    ext_stream_final_purity = np.zeros(num_comp)\n",
        "\n",
        "    for i in range(num_comp):\n",
        "        raff_stream_final_purity[i] = raff_cprofile[i][-1]\n",
        "        ext_stream_final_purity[i] = ext_cprofile[i][-1]\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Recovery\n",
        "    #######################################################\n",
        "    # 2.1 Instantanoues:\n",
        "\n",
        "    # raff_in_recovery = raff_mprofile/sum(C_feed*QF)\n",
        "    # ext_insant_recovery = ext_mprofile/sum(C_feed*QF)\n",
        "\n",
        "    # 2.2 Integral:\n",
        "    raff_recov = m_out_raff/m_in*100\n",
        "    ext_recov = m_out_ext/m_in*100\n",
        "\n",
        "    # 3. Productivity\n",
        "    #######################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Visuliization of PERORMANCE PARAMETERS:\n",
        "    #######################################################\n",
        "\n",
        "    ############## TABLES ##################\n",
        "\n",
        "\n",
        "\n",
        "    # Define the data for the table\n",
        "    # data = {\n",
        "    #     'Metric': [\n",
        "    #         'Total Mass IN',\n",
        "    #         'Total Mass OUT',\n",
        "    #         'Total Expected Acc (IN-OUT)',\n",
        "    #         'Total Model Acc (r+l)',\n",
        "    #         'Total Error (Mod-Exp)',\n",
        "    #         'Total Error Percent (relative to Exp_Acc)',\n",
        "    #         'Final Raffinate Collected Purity [A, B,. . ]',\n",
        "    #         'Final Extract Collected Purity [A, B,. . ]',\n",
        "    #         'Final Raffinate Dimensionless Stream Concentration  [A, B,. . ]',\n",
        "    #         'Final Extract Dimensionless Stream Concentration  [A, B,. . ]',\n",
        "    #         'Final Raffinate Recovery[A, B,. . ]',\n",
        "    #         'Final Extract Recovery[A, B,. . ]'\n",
        "    #     ],\n",
        "    #     'Value': [\n",
        "    #         f\"{m_in} g\",\n",
        "    #         f\"{m_out} g\",\n",
        "    #         f'{sum(Expected_Acc)} g',\n",
        "    #         f'{sum(Model_Acc)} g',\n",
        "    #         f'{sum(Error)} g',\n",
        "    #         f'{Error_percent} %',\n",
        "\n",
        "    #         f'{raff_intgral_purity} %',\n",
        "    #         f'{ext_intgral_purity} %',\n",
        "    #         f'{raff_stream_final_purity} g/cm^3',\n",
        "    #         f'{ext_stream_final_purity}',\n",
        "    #         f'{raff_recov} %',\n",
        "    #         f'{ext_recov} %'\n",
        "    #     ]\n",
        "    # }\n",
        "\n",
        "    # # Create a DataFrame\n",
        "    # df = pd.DataFrame(data)\n",
        "\n",
        "    # # Display the DataFrame\n",
        "    # print(df)\n",
        "\n",
        "    return y_matrices, nx, t, t_sets, t_schedule, C_feed, m_in, m_out, raff_cprofile, ext_cprofile, raff_intgral_purity, raff_recov, ext_intgral_purity, ext_recov, raff_vflow, ext_vflow, Model_Acc, Expected_Acc, Error_percent\n",
        "\n"
      ],
      "metadata": {
        "id": "IUcyZ2DE0t85"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "# What tpye of isoherm is required?\n",
        "# Coupled: \"CUP\"\n",
        "# Uncoupled: \"UNC\"\n",
        "iso_type = \"UNC\"\n",
        "\n",
        "###################### PRIMARY INPUTS #########################\n",
        "# Define the names, colors, and parameter sets for 6 components\n",
        "Names = [\"Glucose\", \"Fructose\"]#, 'C', 'D']#, \"C\"]#, \"D\", \"E\", \"F\"]\n",
        "color = [\"g\", \"orange\"]#, \"purple\", \"brown\"]#, \"b\"]#, \"r\", \"purple\", \"brown\"]\n",
        "num_comp = len(Names) # Number of components\n",
        "e = 0.40         # bed voidage\n",
        "Pe_all = [500, 500] #, 200, 200]\n",
        "Bm = 300\n",
        "\n",
        "# Column Dimensions\n",
        "\n",
        "# How many columns in each Zone?\n",
        "\n",
        "Z1, Z2, Z3, Z4 = 1,3,3,1 # *3 for smb config\n",
        "zone_config = np.array([Z1, Z2, Z3, Z4])\n",
        "nnn = Z1 + Z2 + Z3 + Z4\n",
        "\n",
        "L = 70 # cm # Length of one column\n",
        "d_col = 5 # cm # column internal diameter\n",
        "# Calculate the radius\n",
        "r_col = d_col / 2\n",
        "# Calculate the area of the base\n",
        "A_col = np.pi * (r_col ** 2) # cm^2\n",
        "V_col = A_col*L # cm^3\n",
        "# Dimensions of the tubing and from each column:\n",
        "# Assuming the pipe diameter is 20% of the column diameter:\n",
        "d_in = 0.2 * d_col # cm\n",
        "nx_per_col = 15\n",
        "\n",
        "\n",
        "################ Time Specs #################################################################################\n",
        "t_index_min = 3.30 # min # Index time # How long the pulse holds before swtiching\n",
        "n_num_cycles = 15    # Number of Cycles you want the SMB to run for\n",
        "###############  FLOWRATES   #################################################################################\n",
        "\n",
        "# Jochen et al:\n",
        "Q_P, Q_Q, Q_R, Q_S = 5.21, 4, 5.67, 4.65 # x10-7 m^3/s\n",
        "conv_fac = 0.1 # x10-7 m^3/s => cm^3/s\n",
        "Q_P, Q_Q, Q_R, Q_S  = Q_P*conv_fac, Q_Q*conv_fac, Q_R*conv_fac, Q_S*conv_fac\n",
        "\n",
        "Q_I, Q_II, Q_III, Q_IV = Q_R,  Q_S, Q_P, Q_Q\n",
        "\n",
        "\n",
        "Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV])\n",
        "\n",
        "\n",
        "\n",
        "# # Parameter Sets for different components\n",
        "################################################################\n",
        "\n",
        "# Units:\n",
        "# - Concentrations: g/cm^3\n",
        "# - kh: 1/s\n",
        "# - Da: cm^2/s\n",
        "\n",
        "# A must have a less affinity to resin that B - FOUND IN EXtract purity\n",
        "parameter_sets = [\n",
        "    {\"kh\": 3.15/100, \"H\": 0.27, \"C_feed\": 0.02},  # Component A\n",
        "    {\"kh\": 2.217/100, \"H\": 0.53, \"C_feed\": 0.02}] #, # Component B\n",
        "\n",
        "# ISOTHERM PARAMETERS\n",
        "###########################################################################################\n",
        "theta_lin = [parameter_sets[i]['H'] for i in range(num_comp)] # [HA, HB]\n",
        "print('theta_lin:', theta_lin)\n",
        "# theta_lang = [1, 2, 3, 4 ,5, 6] # [HA, HB]\n",
        "theta_cup_lang = [5.29, 3.24, 2.02, 0.03] # [HA, HB, KA, KB]\n",
        "# theta_fre = [1.2, 0.5]\n",
        "# theta_blang = [[2.69, 0.0336, 0.0466, 0.1, 1, 3],\\\n",
        "#                 [3.73, 0.0336, 0.0466, 0.3, 1, 3]] # [HA, HB]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0VTadDf01Dd",
        "outputId": "4ebd58b9-b7e2-4089-f65a-b9b4253f0980"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta_lin: [0.27, 0.53]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimization Settings"
      ],
      "metadata": {
        "id": "k9VGA_Q5h1eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - - - - -\n",
        "Q_fixed_feed = 2 # L/h\n",
        "Q_fixed_D = 2 # L/h\n",
        "# - - - - -\n",
        "t_index_min = 5 # min\n",
        "# - - - - -\n",
        "Q_max = 7 # L/h\n",
        "Q_min = 1 # L/h\n",
        "# - - - - -\n",
        "m_max = 0.53\n",
        "m_min = 0.27\n",
        "# - - - - -\n",
        "m1_fixed = 0.8\n",
        "m4_fixed = 0.2\n",
        "# - - - - -\n",
        "# L/h --> cm^3/s:\n",
        "Q_max = Q_max/3.6 # L/h --> cm^3/s\n",
        "Q_max = Q_min/3.6 # L/h --> cm^3/s\n",
        "Q_fixed_feed = Q_fixed_feed/3.6 # L/h --> cm^3/s\n",
        "Q_fixed_D = Q_fixed_D/3.6 # L/h --> cm^3/s\n",
        "\n",
        "# - - - - -"
      ],
      "metadata": {
        "id": "CpNHd2DGh0aK"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SMB_inputs = [iso_type, Names, color, num_comp, nx_per_col, e, Pe_all, Bm, zone_config, L, d_col, d_in, t_index_min, n_num_cycles, Q_internal, parameter_sets]\n"
      ],
      "metadata": {
        "id": "J-t9zQRu03se"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Setup\n",
        "\n",
        "\n",
        "First, we define the constraint used in the example in outcome_constraint. The second function weighted_obj is a \"feasibility-weighted objective,\" which returns zero when not feasible. Not that both the constraint and the objective function come from the same experiment."
      ],
      "metadata": {
        "id": "CCHRLIYT-2Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.test_functions import Hartmann\n",
        "\n",
        "\n",
        "neg_hartmann6 = Hartmann(negate=True)\n",
        "# Where theres ned_hartmann6, replace with the weighted recovery and the\n",
        "# constraint function should be replaced with the weighted purity\n",
        "\n",
        "\n",
        "def weighted_obj(X, constraint_threshold, t_index_min):\n",
        "  \"\"\"Feasibility weighted objective; zero if not feasible.\n",
        "\n",
        "    X = [m1, m2, m3, m4]; type= torche_tensor\n",
        "    Objective: WAR = Weighted Average Recovery\n",
        "    Constraint: WAP = Weighted Average Purity\n",
        "\n",
        "    Use WAP to calculate the feasibility weights. Which\n",
        "    will scale teh EI output.\n",
        "\n",
        "  \"\"\"\n",
        "  WAR = np.zeros((len(X[:,0])))\n",
        "  WAP = np.zeros((len(X[:,0])))\n",
        "\n",
        "  # print(f'np.shape(X)[-1]: {np.shape(X)[-1]}')\n",
        "\n",
        "  def mj_to_Qj(mj):\n",
        "    Qj = (mj*V_col*(1-e) + V_col*e)/(t_index_min*60) # cm^3/s\n",
        "    return Qj\n",
        "\n",
        "\n",
        "  for i in range(len(X[:,0])):\n",
        "\n",
        "      # print(f't_index: {t_index}')\n",
        "      # print(f't_index type: {type(t_index)}')\n",
        "\n",
        "      if np.shape(X)[-1] == 4: # when we are generating the initial smaples (we have all the flowrates already)\n",
        "          # Unpack and convert to float and np.arrays from torch.tensors:\n",
        "          m1, m2, m3, m4 = float(X[i,0]), float(X[i,1]), float(X[i,2]), float(X[i,3])\n",
        "          print(f'[m1, m2, m3, m4]: [{m1}, {m2}, {m3}, {m4}]')\n",
        "          Q_I, Q_II, Q_III, Q_IV = mj_to_Qj(m1), mj_to_Qj(m2), mj_to_Qj(m3), mj_to_Qj(m4)\n",
        "          Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV]) # cm^3/s\n",
        "          print(f'Q_internal: {Q_internal} cm^s/s')\n",
        "          print(f'Q_internal: {Q_internal*3.6} L/h')\n",
        "          print(f'Q_internal type: {type(Q_internal)}')\n",
        "\n",
        "      # elif np.shape(X)[-1] < 4: # During optimization - (we only have QX, Q_rec and t_index)\n",
        "      #     # Unpack and convert to float and np.arrays from torch.tensors:\n",
        "      #     print(f'-----------------')\n",
        "      #     print(f't_index: {t_index}')\n",
        "      #     # print(f't_index type: {type(t_index)}')\n",
        "      #     QX= float(X[i,0]) # cm^3/s\n",
        "      #     Q_rec = -float(X[i,1])# cm^3/s ENSURE this is negative value\n",
        "\n",
        "      #     # ----- Caclulate QR, using vol balance\n",
        "      #     vol_in = Q_fixed_feed + Q_fixed_D # cm^3/s\n",
        "      #     QR = -(vol_in + QX) # SINCE abs(vol_in) > abs(QX)\n",
        "\n",
        "      #     Q_external = np.array([Q_fixed_feed, QR, Q_fixed_D, QX])\n",
        "      # -------------------------------------------------------------\n",
        "      # -------------------------------------------------------------\n",
        "\n",
        "      # print(f'Q_internal type: {type(Q_internal)}')\n",
        "      # Update SMB_inputs:\n",
        "      SMB_inputs[12] = t_index_min  # Update t_index\n",
        "      SMB_inputs[14] = Q_internal # Update Q_internal\n",
        "\n",
        "      results = SMB(SMB_inputs)\n",
        "\n",
        "      # print(f'done solving sample {i+1}')\n",
        "\n",
        "      raff_purity = results[10]  # [Glu, Fru]\n",
        "      ext_purity = results[12]  # [Glu, Fru]\n",
        "\n",
        "      raff_recovery = results[11]  # [Glu, Fru]\n",
        "      ext_recovery = results[13]  # [Glu, Fru]\n",
        "\n",
        "      pur1 = raff_purity[0] / 100\n",
        "      pur2 = ext_purity[1] / 100\n",
        "\n",
        "      rec1 = raff_recovery[0] / 100\n",
        "      rec2 = ext_recovery[1] / 100\n",
        "\n",
        "      # Constraint\n",
        "      WAP_add = 0.5*pur1 + 0.5*pur2\n",
        "      print(f'WAP_add: {WAP_add}')\n",
        "      # Objective\n",
        "      WAR_add = 0.5*rec1 + 0.5*rec2\n",
        "      print(f'WAR_add: {WAR_add}')\n",
        "      print(f'---------------------------------')\n",
        "\n",
        "      # Pack\n",
        "      WAP[i] = WAP_add\n",
        "      WAR[i] = WAR_add\n",
        "\n",
        "  # Convert to torch.tensors:\n",
        "  WAP = torch.tensor(WAP, device=device, dtype=dtype)\n",
        "  WAR = torch.tensor(WAR, device=device, dtype=dtype)\n",
        "  feasibility_weights = (WAP - constraint_threshold >= 0).type_as(X)\n",
        "\n",
        "  return WAP, WAP, feasibility_weights"
      ],
      "metadata": {
        "id": "Hc1_C2ukpT07"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.models.transforms.input import Normalize\n",
        "from botorch.models import SingleTaskGP, ModelListGP\n",
        "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
        "\n",
        "NOISE_SE = 0.25\n",
        "train_yvar = torch.tensor(NOISE_SE**2, device=device, dtype=dtype)\n",
        "\n",
        "def Q_external_to_internal(Q_external, Q_rec):\n",
        "    \"\"\"\n",
        "    Q_rec = 0.567 # cm^3/s (Subramani et al)\n",
        "\n",
        "    \"\"\"\n",
        "      # Unpack and calculate internal flowrates\n",
        "    QF = Q_external[0]\n",
        "    QR = Q_external[1]\n",
        "    QD = Q_external[2]\n",
        "    QX = Q_external[3]\n",
        "    Q_rec = Q_rec\n",
        "\n",
        "    # Default for BENCH scale:\n",
        "    # Q_rec = 0.28 # cm^3/s (approx 1 L/h)\n",
        "    # Set as necessary:\n",
        "\n",
        "    #- --------------------\n",
        "    # Written Relative to Q_I\n",
        "    Q_I = Q_rec + QD  # cm^3/s\n",
        "    Q_II = QX + Q_I\n",
        "    Q_III = QX + QF + Q_I\n",
        "    Q_IV = QR + QX + QF + Q_I  # Fixed Q_II to Q_I as the variable was not defined yet\n",
        "\n",
        "    #- --------------------\n",
        "    Q_internal = np.array([Q_I, Q_II, Q_III, Q_IV])\n",
        "    #- --------------------\n",
        "    return Q_internal\n",
        "\n",
        "def flowrate_random_samples(Q_fixed_feed, n, t_index,):\n",
        "    \"\"\"\n",
        "    Generate random: [m1, m2, m3, m4] combibations\n",
        "    n = number of samples\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    sample  = np.zeros((n,4)) # [m1, m2, m3, m4]\n",
        "    sample[:,0] = np.ones(n)*Q_fixed_feed\n",
        "\n",
        "    for num in range(n):\n",
        "\n",
        "      # x = [QR, QD, QX, t_index]\n",
        "\n",
        "\n",
        "\n",
        "      # Select a QD, xD\n",
        "      # Set to not be more than 2 times the fixed feed\n",
        "      QD = Q_fixed_D\n",
        "\n",
        "      # Volume Balance: (Q_fixed_feed + xD) = (xR + xX)\n",
        "      vol_in = Q_fixed_feed + QD\n",
        "\n",
        "      # Select a QX\n",
        "      alpha = np.random.uniform(0.1, 0.75) # between fraction of vol_in\n",
        "      QX = -alpha*vol_in\n",
        "\n",
        "      # Calculate QR, xR as the balance:\n",
        "      QR = -(1-alpha)*vol_in\n",
        "\n",
        "      # Randomly sample a Q_rec:\n",
        "      Q_rec =  np.random.uniform(Q_min, Q_max) # cm^3/s\n",
        "\n",
        "      # Fill the current row:\n",
        "      sample[num,1] = QR # first value\n",
        "      sample[num,2] = QD\n",
        "      sample[num,3] = QX\n",
        "      sample[num,4] = Q_rec\n",
        "      sample[num,5] = np.round(t_index_min,3)\n",
        "\n",
        "    sample = np.array(sample)\n",
        "    # print(f'sampled flowrates:\\n {sample} cm^3/s \\n {sample*3.6} L/h \\n internal: {Q_external_to_internal(sample, 0.567)*3.6} L/h')\n",
        "    if (Q_fixed_feed + QD) - abs(QR + QX) < 0.001:\n",
        "      print(f'MB closed: {(Q_fixed_feed + QD) - abs(QR + QX)}')\n",
        "    else:\n",
        "      print(f'MB NOT closed, error: {(Q_fixed_feed + QD) - abs(QR + QX)}')\n",
        "\n",
        "    return torch.from_numpy(sample[:,3:]), torch.from_numpy(sample)\n",
        "\n",
        "def lhq_sample_mj(m_min, m_max, n_samples, diff=0.1):\n",
        "    \"\"\"\n",
        "    Function that performs Latin Hypercube (LHS) sampling for [m1, m2, m3, m4]\n",
        "    Note that for all mjs: (m_min < m_j < m_max)\n",
        "    And that:\n",
        "          (i)   m4 < m1 - (diff*m1) and m2 < m1 - (diff*m1)\n",
        "          (ii)  m2 < m3 - (diff*m3)\n",
        "          (iii) m3 > m4 + (diff*m4)\n",
        "    Final result is an np.array of size: (n_samples, 4)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the array to store the samples\n",
        "    samples = np.zeros((n_samples, 4))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Sample m1, m2, m3, m4 within bounds and respecting constraints\n",
        "        m1 = np.random.uniform(m_min, m_max)\n",
        "        m4 = np.random.uniform(m_min, m1-diff*m1)\n",
        "\n",
        "        # Sample m2 such that it respects the constraint: m2 < m1 - (diff*m1)\n",
        "        m2 = np.random.uniform(m_min, m_max)\n",
        "        while m2 >= m1 - (diff * m1):  # Ensuring m2 < m1 - (diff*m1)\n",
        "            m2 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Sample m3 such that it respects the constraint: m3 > m2 + (diff*m2)\n",
        "        m3 = np.random.uniform(m_min, m_max)\n",
        "        while m3 <= m2 + (diff * m2):  # Ensuring m3 > m2 + (diff*m2)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Ensure the constraint: m3 > m4 + (diff * m4)\n",
        "        while m3 <= m4 + (diff * m4):  # Ensuring m3 > m4 + (diff*m4)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Store the sample in the array\n",
        "        samples[i] = [m1, m2, m3, m4]\n",
        "\n",
        "\n",
        "    return torch.from_numpy(samples)\n",
        "\n",
        "def fixed_feed_lhq_sample_mj(t_index_min, Q_fixed_feed, m_min, m_max, n_samples, diff=0.1):\n",
        "    \"\"\"\n",
        "    Since the feed is fixed, m3 is caluclated\n",
        "\n",
        "    Function that performs Latin Hypercube (LHS) sampling for [m1, m2, m3, m4]\n",
        "    Note that for all mjs: (m_min < m_j < m_max)\n",
        "    And that:\n",
        "          (i)   m4 < m1 - (diff*m1) and m2 < m1 - (diff*m1)\n",
        "          (ii)  m2 < m3 - (diff*m3)\n",
        "          (iii) m3 > m4 + (diff*m4)\n",
        "    Final result is an np.array of size: (n_samples, 4)\n",
        "    \"\"\"\n",
        "    # Initialize the array to store the samples\n",
        "    samples = np.zeros((n_samples, 4))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Sample m1, m2, m3, m4 within bounds and respecting constraints\n",
        "        m1 = np.random.uniform(m_min, m_max)\n",
        "        m4 = np.random.uniform(m_min, m1-diff*m1)\n",
        "\n",
        "        # Sample m2 such that it respects the constraint: m2 < m1 - (diff*m1)\n",
        "        m2 = np.random.uniform(m_min, m_max)\n",
        "        while m2 >= m1 - (diff * m1):  # Ensuring m2 < m1 - (diff*m1)\n",
        "            m2 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Sample m3 such that it respects the constraint: m3 > m2 + (diff*m2)\n",
        "        m3 = np.random.uniform(m_min, m_max)\n",
        "        while m3 <= m2 + (diff * m2):  # Ensuring m3 > m2 + (diff*m2)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Ensure the constraint: m3 > m4 + (diff * m4)\n",
        "        while m3 <= m4 + (diff * m4):  # Ensuring m3 > m4 + (diff*m4)\n",
        "            m3 = np.random.uniform(m_min, m_max)\n",
        "\n",
        "        # Store the sample in the array\n",
        "        samples[i] = [m1, m2, m3, m4]\n",
        "\n",
        "    return torch.from_numpy(samples)\n",
        "\n",
        "def fixed_m1_and_m4_lhq_sample_mj(t_index_min, m1, m4, m_min, m_max, n_samples, diff=0.1):\n",
        "    \"\"\"\n",
        "    - Since the feed is fixed, m3 is caluclated AND\n",
        "    - Since the desorbant is fixed, m1 is caluclated\n",
        "\n",
        "    Function that performs Latin Hypercube (LHS) sampling for [m1, m2, m3, m4]\n",
        "    Note that for all mjs: (m_min < m_j < m_max)\n",
        "    And that:\n",
        "          (i)   m4 < m1 - (diff*m1) and m2 < m1 - (diff*m1)\n",
        "          (ii)  m2 < m3 - (diff*m3)\n",
        "          (iii) m3 > m4 + (diff*m4)\n",
        "    Final result is an np.array of size: (n_samples, 4)\n",
        "    \"\"\"\n",
        "    # Initialize the array to store the samples\n",
        "    samples = np.zeros((n_samples, 4))\n",
        "    samples[:,0] = np.ones(n_samples)*m1\n",
        "    samples[:,-1] = np.ones(n_samples)*m4\n",
        "    m2_set = np.linspace(m_min, m_max, n_samples)\n",
        "    num_of_m3_per_m2 = 3\n",
        "\n",
        "    #Sample from the separation triangle:\n",
        "    for i in range(n_samples): # for each vertical line\n",
        "        k = i%num_of_m3_per_m2\n",
        "        m2 = m2_set[k]\n",
        "        samples[i, 1] = m2\n",
        "        m3 = np.random.uniform(m2, m_max)\n",
        "        samples[i, 2] = m3\n",
        "\n",
        "    return torch.from_numpy(samples[1:3]), torch.from_numpy(samples)\n",
        "\n",
        "def generate_initial_data(constraint_threshold, n, t_index_min):\n",
        "\n",
        "    # generate training data\n",
        "    # Generate Random Sample of [QF, QR, QD, QX, t]\n",
        "    # train_x, train_all = flowrate_random_samples(Q_fixed_feed, n , t_index_min)\n",
        "\n",
        "    # train_x = lhq_sample_mj(0.2, 1.7, n, diff=0.1)\n",
        "    # train_x = fixed_feed_lhq_sample_mj(t_index_min, Q_fixed_feed, 0.2, 1.7, n, diff=0.1)\n",
        "    train_x, train_all = fixed_m1_and_m4_lhq_sample_mj(t_index_min = t_index_min, m1=m1_fixed, m4=m4_fixed, m_min=m_min, m_max=m_max, n_samples=n, diff=0.1)\n",
        "    print(f'done sampling flowrate')\n",
        "\n",
        "    # exact_obj, exact_con = weighted_obj(train_x) # add output dimension\n",
        "    exact_WAR, exact_WAP, feasibility_weights = weighted_obj(train_all, constraint_threshold, t_index_min)\n",
        "\n",
        "\n",
        "    exact_WAP = exact_WAP.unsqueeze(-1)  # add output dimension\n",
        "    exact_WAR = exact_WAR.unsqueeze(-1)\n",
        "    # print(f'exact_WAP: {exact_WAP}')\n",
        "    # print(f'exact_WAR: {exact_WAR}')\n",
        "\n",
        "    # =Zero, the recoverys with purities < constraint_threshold\n",
        "    feas_weighted_WAR = exact_WAR # * feasibility_weights\n",
        "    # print(f'feas_weighted_WAR: {feas_weighted_WAR}')\n",
        "    # print(f'exact_WAR: {exact_WAR}')\n",
        "\n",
        "    train_obj = feas_weighted_WAR # + NOISE_SE  * torch.randn_like(exact_WAR)\n",
        "    train_con = exact_WAP # + NOISE_SE * torch.randn_like(exact_WAP)\n",
        "\n",
        "    best_observed_value = feas_weighted_WAR.max().item()\n",
        "\n",
        "    # train_obj = torch.tensor(train_obj, device=device, dtype=dtype)\n",
        "    # train_con = torch.tensor(train_con, device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "    return train_x, train_obj, train_con, exact_WAR, exact_WAP, feasibility_weights, best_observed_value\n",
        "\n",
        "\n",
        "def initialize_model(train_x, train_obj, train_con, state_dict=None):\n",
        "    # define models for objective and constraint\n",
        "    model_obj = SingleTaskGP(\n",
        "        train_x,\n",
        "        train_obj,\n",
        "        train_yvar.expand_as(train_obj),\n",
        "        input_transform=Normalize(d=train_x.shape[-1]),\n",
        "    ).to(train_x)\n",
        "\n",
        "    model_con = SingleTaskGP(\n",
        "        train_x,\n",
        "        train_con,\n",
        "        train_yvar.expand_as(train_con),\n",
        "        input_transform=Normalize(d=train_x.shape[-1]),\n",
        "    ).to(train_x)\n",
        "\n",
        "    # combine into a multi-output GP model\n",
        "    model = ModelListGP(model_obj, model_con)\n",
        "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
        "    # load state dict if it is passed\n",
        "    if state_dict is not None:\n",
        "        model.load_state_dict(state_dict)\n",
        "    return mll, model"
      ],
      "metadata": {
        "id": "t9qOlbF5tbqC"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_obj, train_con, WAR, WAP, feasibility_weights, best_observed_value = generate_initial_data(constraint_threshold=0.4, n=1, t_index_min=5)\n",
        "\n",
        "print(f'train_obj:{train_obj}')\n",
        "print(f'train_con:{train_con}')\n",
        "print(f'feasibility_weights:{feasibility_weights}')\n",
        "print(f'WAR:{WAR}')\n",
        "print(f'WAP:{WAP}')\n",
        "\n",
        "# print(f'feasibility_weights:{feasibility_weights}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJeALr3h7_5W",
        "outputId": "354eca34-3359-4f68-b608-672e8269fcaf"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done sampling flowrate\n",
            "[m1, m2, m3, m4]: [0.8, 0.27, 0.3983532803966522, 0.2]\n",
            "Q_internal: [4.03171057 2.57479698 2.92762649 2.38237443] cm^s/s\n",
            "Q_internal: [14.51415806  9.26926912 10.53945535  8.57654794] L/h\n",
            "Q_internal type: <class 'numpy.ndarray'>\n",
            "WAP_add: 0.862063829835642\n",
            "WAR_add: 0.6781794377246688\n",
            "---------------------------------\n",
            "train_obj:tensor([[0.8621]], dtype=torch.float64)\n",
            "train_con:tensor([[0.8621]], dtype=torch.float64)\n",
            "feasibility_weights:tensor([1.], dtype=torch.float64)\n",
            "WAR:tensor([[0.8621]], dtype=torch.float64)\n",
            "WAP:tensor([[0.8621]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a construct to extract the objective and constraint from the GP**\n",
        "\n",
        "The methods below take the outputs of the GP and return the objective and the constraint. In general, these can be any Callable, but here we simply need to index the correct output."
      ],
      "metadata": {
        "id": "9fN6JwoOGKVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.acquisition.objective import GenericMCObjective\n",
        "\n",
        "def obj_callable(Z: torch.Tensor, X: Optional[torch.Tensor] = None):\n",
        "    '''\n",
        "    Z = Torch tensor that is the output of ????\n",
        "    '''\n",
        "    return Z[..., 0]\n",
        "\n",
        "\n",
        "def constraint_callable(Z):\n",
        "    return Z[..., 1]\n",
        "\n",
        "\n",
        "objective = GenericMCObjective(objective=obj_callable)\n"
      ],
      "metadata": {
        "id": "_ZZ_pR1Ktt_V"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define a helper function that performs the essential BO step**\n",
        "\n",
        "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch [x1,x2,..xq], where x E R^6 - along with the observed function values.\n",
        "\n",
        "For this example, we'll use a small batch of q=3.\n",
        "\n",
        "The function optimize_acqf optimizes the\n",
        " points jointly. A simple initialization heuristic is used to select the 10 restart initial locations from a set of 50 random points."
      ],
      "metadata": {
        "id": "2hfFSh8aKMd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.optim import optimize_acqf\n",
        "# bounds = [m2, m3]\n",
        "bounds = torch.tensor([\n",
        "    [m_min, m_min],  # Lower bounds\n",
        "    [m_max, m_max]   # Upper bounds\n",
        "], device=device, dtype=dtype)\n",
        "\n",
        "BATCH_SIZE = 3 if not SMOKE_TEST else 2\n",
        "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
        "RAW_SAMPLES = 512 if not SMOKE_TEST else 32\n",
        "\n",
        "\n",
        "def optimize_acqf_and_get_observation(acq_func):\n",
        "    \"\"\"Optimizes the acquisition function, and returns a new candidate and a noisy observation.\"\"\"\n",
        "    # optimize\n",
        "    # ensure the\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=bounds,\n",
        "        q=BATCH_SIZE, # how many candidate points to recommennd\n",
        "        num_restarts=NUM_RESTARTS,\n",
        "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
        "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
        "    )\n",
        "    # observe new values\n",
        "    # new_x = [QD, QX, t_index]\n",
        "    new_x = candidates.detach()\n",
        "\n",
        "    print(f'Getting WAP and WAR Performance:')\n",
        "    exact_WAR, exact_WAP, feasibility_weights = weighted_obj(new_x, constraint_threshold)\n",
        "\n",
        "    exact_obj = exact_WAR * feasibility_weights\n",
        "    exact_con = exact_WAP\n",
        "\n",
        "\n",
        "    exact_obj = torch.tensor(np.array([exact_obj]), dtype=torch.float32, device=device)\n",
        "    exact_con = torch.tensor(np.array([exact_con]), dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "    new_obj = exact_obj + NOISE_SE * torch.randn_like(exact_obj)\n",
        "    new_con = exact_con + NOISE_SE * torch.randn_like(exact_con)\n",
        "\n",
        "    return new_x, new_obj, new_con\n",
        "\n",
        "\n",
        "def update_random_observations(best_random):\n",
        "    \"\"\"Simulates a random policy by taking a the current list of best values observed randomly,\n",
        "    drawing a new random point, observing its value, and updating the list.\n",
        "    \"\"\"\n",
        "    rand_x = torch.rand(BATCH_SIZE, 5)\n",
        "    next_random_best = weighted_obj(rand_x).max().item()\n",
        "    best_random.append(max(best_random[-1], next_random_best))\n",
        "    return best_random"
      ],
      "metadata": {
        "id": "x_PT9RG0t6MI"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform Bayesian Optimization loop with qLogNEI**\n",
        "\n",
        "The Bayesian optimization \"loop\" for a batch size of, q, simply iterates the following steps:\n",
        "\n",
        "1. given a surrogate model, choose a batch of points [x1,x2,..,xq]\n",
        "2. observe f(x) for each, x, in the batch\n",
        "3. update the surrogate model.\n",
        "\n",
        "Just for illustration purposes, we run three trials each of which do N_BATCH=20 rounds of optimization. The acquisition function is approximated using MC_SAMPLES=256 samples."
      ],
      "metadata": {
        "id": "R-Vyaqh9aGZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import warnings\n",
        "\n",
        "from botorch import fit_gpytorch_mll\n",
        "from botorch.acquisition import (\n",
        "    qLogExpectedImprovement,\n",
        "    qLogNoisyExpectedImprovement,\n",
        ")\n",
        "from botorch.exceptions import BadInitialCandidatesWarning\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=BadInitialCandidatesWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "n_initial = [10,10,10] # initial points per trial\n",
        "N_TRIALS = len(n_initial) if not SMOKE_TEST else 2\n",
        "N_BATCH = 20 if not SMOKE_TEST else 2\n",
        "MC_SAMPLES = 256 if not SMOKE_TEST else 32\n",
        "constraint_threshold = 0.85\n",
        "verbose = False\n",
        "\n",
        "best_observed_all_ei, best_observed_all_nei, best_random_all = [], [], []\n",
        "\n",
        "# average over multiple trials\n",
        "for trial in range(1, N_TRIALS + 1):\n",
        "    print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n",
        "    best_observed_ei, best_observed_nei, best_random = [], [], []\n",
        "\n",
        "    # call helper functions to generate initial training data and initialize model\n",
        "    print(f'\\nGenerating {n_initial[trial-1]} random initial points for trial {trial}:')\n",
        "    print(f'----------------------------------------')\n",
        "    (train_x_ei,\n",
        "    train_obj_ei,\n",
        "    train_con_ei,\n",
        "    WAR,\n",
        "    WAP,\n",
        "    feasibility_weights,\n",
        "    best_observed_value_ei\n",
        "\n",
        "    ) = generate_initial_data(constraint_threshold=0.75, n=n_initial[trial-1], t_index_min = t_index_min)\n",
        "    print(f'train_obj_ei: {train_obj_ei}')\n",
        "    print(f'train_obj_ei-shape: {np.shape(train_obj_ei)}')\n",
        "    print(f'train_con_ei: {train_con_ei}')\n",
        "    print(f'train_con_ei-shape: {np.shape(train_con_ei)}')\n",
        "    # Fit GPs to inital data\n",
        "    print(f'Fitting GPs to inital data. . .')\n",
        "    print(f'-------------------------------')\n",
        "    mll_ei, model_ei = initialize_model(train_x_ei, train_obj_ei, train_con_ei)\n",
        "\n",
        "    train_x_nei, train_obj_nei, train_con_nei = train_x_ei, train_obj_ei, train_con_ei\n",
        "    best_observed_value_nei = best_observed_value_ei\n",
        "    mll_nei, model_nei = initialize_model(train_x_nei, train_obj_nei, train_con_nei)\n",
        "\n",
        "    best_observed_ei.append(best_observed_value_ei)\n",
        "    best_observed_nei.append(best_observed_value_nei)\n",
        "    best_random.append(best_observed_value_ei)\n",
        "\n",
        "    # run N_BATCH rounds of BayesOpt after the initial random batch\n",
        "\n",
        "    for iteration in range(1, N_BATCH + 1):\n",
        "\n",
        "        t0 = time.monotonic()\n",
        "\n",
        "        # fit the models\n",
        "        fit_gpytorch_mll(mll_ei)\n",
        "        fit_gpytorch_mll(mll_nei)\n",
        "\n",
        "        # define the qEI and qNEI acquisition modules using a QMC sampler\n",
        "        qmc_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([MC_SAMPLES]))\n",
        "\n",
        "        # for best_f, we use the best observed noisy values as an approximation\n",
        "        qLogEI = qLogExpectedImprovement(\n",
        "            model=model_ei,\n",
        "            best_f=(train_obj_ei * (train_con_ei <= 0).to(train_obj_ei)).max(),\n",
        "            sampler=qmc_sampler,\n",
        "            objective=objective,\n",
        "            constraints=[constraint_callable],\n",
        "        )\n",
        "\n",
        "        qLogNEI = qLogNoisyExpectedImprovement(\n",
        "            model=model_nei,\n",
        "            X_baseline=train_x_nei,\n",
        "            sampler=qmc_sampler,\n",
        "            objective=objective,\n",
        "            constraints=[constraint_callable],\n",
        "        )\n",
        "\n",
        "        # optimize and get new observation\n",
        "        print(f'maximizing EI:')\n",
        "        print(f'-----------------')\n",
        "        #as advised by ei:\n",
        "        new_x_ei, new_obj_ei, new_con_ei = optimize_acqf_and_get_observation(qLogEI)\n",
        "        print(f'maximizing NEI:')\n",
        "        print(f'-----------------')\n",
        "        #as advised by nei:\n",
        "        # new_x_nei, new_obj_nei, new_con_nei = optimize_acqf_and_get_observation(qLogNEI)\n",
        "\n",
        "        # update training points\n",
        "        # for ei:\n",
        "        new_obj_ei = new_obj_ei.T  # Convert new from (1,n) to (n,1)\n",
        "        new_con_ei = new_con_ei.T\n",
        "\n",
        "        train_x_ei = torch.cat([train_x_ei, new_x_ei])\n",
        "        train_obj_ei = torch.cat([train_obj_ei, new_obj_ei], dim=0)\n",
        "        train_con_ei = torch.cat([train_con_ei, new_con_ei], dim=0)\n",
        "\n",
        "        # # for nei:\n",
        "        # new_obj_nei = new_obj_nei.T  # Convert new from (1,n) to (n,1)\n",
        "        # new_con_nei = new_con_nei.T\n",
        "\n",
        "        # train_x_nei = torch.cat([train_x_nei, new_x_nei])\n",
        "        # train_obj_nei = torch.cat([train_obj_nei, new_obj_nei], dim=0)\n",
        "        # train_con_nei = torch.cat([train_con_nei, new_con_nei], dim=0)\n",
        "\n",
        "\n",
        "        # update progress\n",
        "        # best_random = update_random_observations(best_random)\n",
        "        best_value_ei = train_obj_ei.max().item()\n",
        "        best_value_nei = train_obj_nei.max().item()\n",
        "        best_observed_ei.append(best_value_ei)\n",
        "        best_observed_nei.append(best_value_nei)\n",
        "\n",
        "        # reinitialize the models so they are ready for fitting on next iteration\n",
        "        # use the current state dict to speed up fitting\n",
        "        mll_ei, model_ei = initialize_model(\n",
        "            train_x_ei,\n",
        "            train_obj_ei,\n",
        "            train_con_ei,\n",
        "            model_ei.state_dict(),\n",
        "        )\n",
        "        # mll_nei, model_nei = initialize_model(\n",
        "        #     train_x_nei,\n",
        "        #     train_obj_nei,\n",
        "        #     train_con_nei,\n",
        "        #     model_nei.state_dict(),\n",
        "        # )\n",
        "\n",
        "        t1 = time.monotonic()\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"\\nBatch {iteration:>2}: best_value (random, qEI, qNEI) = \"\n",
        "                f\"({max(best_random):>4.2f}, {best_value_ei:>4.2f}, {best_value_nei:>4.2f}), \"\n",
        "                f\"time = {t1-t0:>4.2f}.\",\n",
        "                end=\"\",\n",
        "            )\n",
        "        else:\n",
        "            print(\".\", end=\"\")\n",
        "\n",
        "    best_observed_all_ei.append(best_observed_ei)\n",
        "    best_observed_all_nei.append(best_observed_nei)\n",
        "    best_random_all.append(best_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "D1E3XlZluO8I",
        "outputId": "7087c27c-0ba0-4ee6-87e1-c84c7677a319"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial  1 of 3 \n",
            "Generating 10 random initial points for trial 1:\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-f1c2ef841b22>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mbest_observed_value_ei\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     ) = generate_initial_data(constraint_threshold=0.75, n=n_initial[trial-1], t_index_min = t_index_min)\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'train_obj_ei: {train_obj_ei}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'train_obj_ei-shape: {np.shape(train_obj_ei)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-131-1b43f323bcc3>\u001b[0m in \u001b[0;36mgenerate_initial_data\u001b[0;34m(constraint_threshold, n, t_index_min)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;31m# train_x = lhq_sample_mj(0.2, 1.7, n, diff=0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# train_x = fixed_feed_lhq_sample_mj(t_index_min, Q_fixed_feed, 0.2, 1.7, n, diff=0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_m1_and_m4_lhq_sample_mj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_index_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_index_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm1_fixed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm4_fixed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'done sampling flowrate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-131-1b43f323bcc3>\u001b[0m in \u001b[0;36mfixed_m1_and_m4_lhq_sample_mj\u001b[0;34m(t_index_min, m1, m4, m_min, m_max, n_samples, diff)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Ensure the constraint: m3 > m4 + (diff * m4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mm3\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mm4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Ensuring m3 > m4 + (diff*m4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mm3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Store the sample in the array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the results¶\n",
        "\n",
        "\n",
        "\n",
        "The plot below shows the best objective value observed at each step of the optimization for each of the algorithms. The confidence intervals represent the variance at that step in the optimization across the trial runs. The variance across optimization runs is quite high, so in order to get a better estimate of the average performance one would have to run a much larger number of trials N_TRIALS (we avoid this here to limit the runtime of this tutorial)."
      ],
      "metadata": {
        "id": "86lyHnituWMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def ci(y):\n",
        "    return 1.96 * y.std(axis=0) / np.sqrt(N_TRIALS)\n",
        "\n",
        "\n",
        "GLOBAL_MAXIMUM = neg_hartmann6.optimal_value\n",
        "\n",
        "\n",
        "iters = np.arange(N_BATCH + 1) * BATCH_SIZE\n",
        "y_ei = np.asarray(best_observed_all_ei)\n",
        "y_nei = np.asarray(best_observed_all_nei)\n",
        "y_rnd = np.asarray(best_random_all)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "# ax.errorbar(iters, y_rnd.mean(axis=0), yerr=ci(y_rnd), label=\"random\", linewidth=1.5)\n",
        "ax.errorbar(iters, y_ei.mean(axis=0), yerr=ci(y_ei), label=\"qLogEI\", linewidth=1.5)\n",
        "ax.errorbar(iters, y_nei.mean(axis=0), yerr=ci(y_nei), label=\"qLogNEI\", linewidth=1.5)\n",
        "# plt.plot(\n",
        "#     [0, N_BATCH * BATCH_SIZE],\n",
        "#     [GLOBAL_MAXIMUM] * 2,\n",
        "#     \"k\",\n",
        "#     label=\"true best feasible objective\",\n",
        "#     linewidth=2,\n",
        "# )\n",
        "ax.set_ylim(top=1,bottom=0)\n",
        "ax.set(\n",
        "    xlabel=\"number of observations (beyond initial points)\",\n",
        "    ylabel=\"best objective value\",\n",
        ")\n",
        "ax.legend(loc=\"lower right\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "serzwQkbuRKV",
        "outputId": "7a6ea9f1-4273-40ad-98ac-38e147822e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f96c26fb3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}